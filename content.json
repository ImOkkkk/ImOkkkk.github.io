{"meta":{"title":"ImOkkkk BLOG","subtitle":"When they go low,we go high.","description":"Java开发, 后端开发","author":"ImOkkkk","url":"https://imokkkk.github.io","root":"/"},"pages":[{"title":"about","date":"2020-04-30T00:02:54.000Z","updated":"2020-04-30T00:03:10.835Z","comments":true,"path":"about/index.html","permalink":"https://imokkkk.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-04-30T00:00:45.000Z","updated":"2020-04-30T00:01:46.812Z","comments":true,"path":"categories/index.html","permalink":"https://imokkkk.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-04-30T00:00:30.000Z","updated":"2020-04-30T00:01:25.532Z","comments":true,"path":"tags/index.html","permalink":"https://imokkkk.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Prometheus+Grafana 入门","slug":"Prometheus+Grafana入门","date":"2023-06-26T07:57:22.385Z","updated":"2023-06-26T09:17:46.206Z","comments":true,"path":"PrometheusAndGrafana/","link":"","permalink":"https://imokkkk.github.io/PrometheusAndGrafana/","excerpt":"Prometheus安装 下载：https://github.com/prometheus/prometheus/releases/download/v2.29.1/prometheus-2.29.1.linux-amd64.tar.gz 上传到服务器，解压","text":"Prometheus安装 下载：https://github.com/prometheus/prometheus/releases/download/v2.29.1/prometheus-2.29.1.linux-amd64.tar.gz 上传到服务器，解压 12tar -xvzf prometheus-2.29.1.linux-amd64.tar.gzmv prometheus-2.29.1.linux-amd64 prometheus 安装node_exporter，下载：https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz 上传到服务器，解压并启动 12345tar -zvxf node_exporter-1.3.1.linux-amd64.tar.gzmv node_exporter-1.3.1.linux-amd64/ node_exporternohup /usr/local/node_exporter/node_exporter &amp;#检查是否启动netstat -nltp |grep 9100 修改Prometheus配置文件prometheus.yml 启动 123./prometheus --config.file=\"prometheus.yml\" &amp;#检查是否启动ps -ef |grep prometheus 使用访问地址：http://192.168.0.153:9090/ Targets 点开 http://192.168.0.153:9100/metrics，展示各种指标 Graph上面的指标可以在这里以表格/折线图的形式查询 Grafana安装 下载：https://dl.grafana.com/oss/release/grafana-9.2.3.linux-amd64.tar.gz 上传到服务器，解压 12tar -zvxf grafana-9.2.3.linux-amd64.tar.gzmv grafana-9.2.3/ grafana/ 创建Grafana服务 1234567891011cat &gt; /usr/lib/systemd/system/grafana-server.service &lt;&lt;EOF[Unit]Description=GrafanaAfter=network.target[Service]Type=notifyExecStart=/usr/local/grafana/bin/grafana-server -homepath /usr/local/grafanaRestart=on-failure[Install]WantedBy=multi-user.targetEOF 启动 12systemctl enable grafana-server.servicesystemctl start grafana-server.service 使用http://192.168.0.153:3000/ 默认账户/密码：admin/admin 配置数据源 导入Dashboard 如导入id为9276的Dashboard：https://grafana.com/grafana/dashboards/9276-1-cpu/ 整合SpringBoot应用 引入依赖 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt; &lt;version&gt;1.7.2&lt;/version&gt;&lt;/dependency&gt; 增加配置 1management.endpoints.web.exposure.include=* 开放actuator endpoint 重启，检查actuator endpoint是否生效 修改Prometheus配置 1234567891011scrape_configs: - job_name: \"prometheus\" # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: [\"localhost:9090\",\"192.168.0.153:9100\"] - job_name: \"data-fill\" scrape_interval: 5s #每5s抓取一次 metrics_path: '/api/actuator/prometheus' #抓取的数据url static_configs: - targets: [\"192.168.0.153:6868\"] 导入Dashboard id：4701 https://grafana.com/grafana/dashboards/4701-jvm-micrometer/","categories":[{"name":"云原生","slug":"云原生","permalink":"https://imokkkk.github.io/categories/云原生/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://imokkkk.github.io/tags/Prometheus/"},{"name":"Grafana","slug":"Grafana","permalink":"https://imokkkk.github.io/tags/Grafana/"},{"name":"监控","slug":"监控","permalink":"https://imokkkk.github.io/tags/监控/"}]},{"title":"零拷贝","slug":"零拷贝","date":"2023-03-31T09:22:58.136Z","updated":"2023-03-31T09:30:18.415Z","comments":true,"path":"zerocopy/","link":"","permalink":"https://imokkkk.github.io/zerocopy/","excerpt":"什么是零拷贝？计算机操作时，CPU不需要先将数据从某处内存复制到另一个特定的区域，这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。 零拷贝技术可以减少数据拷贝和共享总线操作的次数，从而提高数据传输的效率； 零拷贝技术减少了用户进程地址空间和内核地址空间之间因为上下文切换而带来的开销。 并不是不需要拷贝，而是减少不必要的拷贝。 应用：Kafka、Netty、RocketMQ等。","text":"什么是零拷贝？计算机操作时，CPU不需要先将数据从某处内存复制到另一个特定的区域，这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。 零拷贝技术可以减少数据拷贝和共享总线操作的次数，从而提高数据传输的效率； 零拷贝技术减少了用户进程地址空间和内核地址空间之间因为上下文切换而带来的开销。 并不是不需要拷贝，而是减少不必要的拷贝。 应用：Kafka、Netty、RocketMQ等。 Linux的I/O机制与DMA操作系统内存空间分为用户态和内核态，用户的应用程序无法直接操作硬件，需要通过内核空间进行操作转换，才能真正操作硬件。因此，应用程序需要与网卡、磁盘等硬件进行数据交互时，就需要在用户态和内核态之间来回复制数据。早期，这些操作由CPU负责，压力很大。 DMA(直接内存存取)控制器，接管了数据读写请求，减少CPU的负担。 此时，IO读取，涉及两个过程： DMA等待数据准备好，把磁盘数据读取到操作系统内核缓冲区； 用户进程，将内核缓冲区的数据copy到用户空间。 传统数据传送机制如：读取文件，再用socket发送出去，共2次DMA copy，2次CPU copy，4次上下文切换。 用户进程调用read()，上下文从用户态转向内核态； 将磁盘文件，DMA copy到操作系统内核缓冲区； 将内核缓冲区的数据，CPU copy到应用程序的buffer，上下文从内核态转向用户态； 用户进程调用write()，上下文从用户态转向内核态； 将应用程序buffer中的数据，CPU copy到socket网络发送缓冲区(也属于操作系统内核缓冲区)； 将socket buffer中的数据，DMA copy到网卡，进行网络传输，上下文从内核态切换回用户态。 read()、write()换属于系统调用，每次调用涉及2次上下文切换。 Linux支持的零拷贝mmap内存映射直接将磁盘文件数据映射到内核缓冲区，这个映射的过程是基于 DMA 拷贝的，同时用户缓冲区是跟内核缓冲区共享一块映射数据的，建立共享映射之后，就不需要从内核缓冲区拷贝到用户缓冲区了。使用mmap代替read()，减少一次CPU拷贝，共2次DMA copy，1次CPU copy，4次上下文切换； 用户进程调用mmap()，用户态转向内核态； 磁盘中的数据与应用程序buffer映射，基于DMA copy，内核态转向用户态； 用户进程调用write()，上下文从用户态转向内核态； 将应用程序buffer中的数据，CPU copy到socket网络发送缓冲区； 将socket buffer中的数据，DMA copy到网卡，进行网络传输，上下文从内核态转向用户态。 sendfileLinux从2.1支持sendfile 调用sendfile()时，DMA将磁盘数据复制到操作系统内核缓冲区，然后直接将内核buffer中的数据长度和描述符直接拷贝到socket buffer。如果设备支持，无需CPU拷贝。共1(或0)次CPU拷贝，2次DMA拷贝，2次上下文切换。 用户进程调用sendfile()，上下文从用户态转向内核态； 将磁盘文件中的数据，DMA copy到操作系统内核缓冲区； 将内核buffer中数据的长度和描述符，CPUcopy到socket buffer；(设备支持的话，此次CPU copy可以省略) 将socket buffer中的数据，DMA copy到网卡，进行网络传输，从内核态转向用户态。 spliceLinux从2.6.17支持splice 数据从磁盘读取到操作系统内核缓冲区后，直接将其转成内核空间其他数据buffer，而不需要拷贝到用户空间。 从磁盘读取到内核 buffer 后，在内核空间直接与 socket buffer 建立 pipe管道。 与sendfile()的区别： send file 0 CPU copy需要硬件支持，splice()不需要 Java支持的零拷贝Java支持内存映射mmap、sendfile。 NIO提供的内存映射MappedByteBufferNIO中的FileChanel.map()采用了内存映射方式，底层就是调用Linux mmap()实现的。 适合读取大文件，同时也能对文件内容进行更改。 NIO 提供的 sendfileJava NIO 中提供的 FileChannel 拥有 transferTo 和 transferFrom 两个方法，可直接把FileChannel 中的数据拷贝到另外一个 Channel，或者直接把另外一个 Channel 中的数据拷贝到 FileChannel。该接口常被用于高效的网络 /文件的数据传输和大文件拷贝。 Kafka中的零拷贝 Producer生产的数据持久化到broker，broker采用mmap文件映射，实现顺序的快速写入； Consumer从broker读取数据，broker采用sendfile，将磁盘文件读到OS内核缓冲区后，直接转到socket buffer进行网络发送。 Netty中的零拷贝 网络通信上，Netty接收和发送ByteBuffer采用DIRECT BUFFERS，使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝。如果采用传统的堆内存进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才写入Socket中，相对于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝； 缓存操作上，Netty提供了CompositeByteBuf类，可以将多个ByteBuffer合并为一个逻辑上的ByteBuf，避免了各个ByteBuf之间的拷贝； 通过 wrap 操作，我们可以将 byte[]数组、ByteBuf、 ByteBuffer 等包装成一个 NettyByteBuf 对象，进而避免了拷贝操作。ByteBuf支持slice 操作，因此可以将ByteBuf分解为多个共享同一个存储区域的ByteBuf，避免了内存的拷贝。 文件传输上，Netty通过FileRegion包装的FileChannel.tranferTo实现文件传输，它可以直接将文件缓冲区的数据发送到目标Channel，避免了传统通过循环write方式导致的内存拷贝问题。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://imokkkk.github.io/categories/操作系统/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://imokkkk.github.io/tags/Netty/"},{"name":"Kafka","slug":"Kafka","permalink":"https://imokkkk.github.io/tags/Kafka/"},{"name":"NIO","slug":"NIO","permalink":"https://imokkkk.github.io/tags/NIO/"}]},{"title":"Spring：自定义Converter实现参数转换","slug":"Spring：自定义Converter实现参数转换","date":"2023-01-18T05:51:18.160Z","updated":"2023-01-18T06:09:20.658Z","comments":true,"path":"springcustomconverter/","link":"","permalink":"https://imokkkk.github.io/springcustomconverter/","excerpt":"背景多个后端、前端开发时约定日期类型参数时未能统一，即存在2022-12-21 12:45:00，也存在2022/12/21 12:45:00。导致可以复用的接口，前端传参格式却不一样，前端改动的话，工作量比较大，所以在后端做格式兼容处理。","text":"背景多个后端、前端开发时约定日期类型参数时未能统一，即存在2022-12-21 12:45:00，也存在2022/12/21 12:45:00。导致可以复用的接口，前端传参格式却不一样，前端改动的话，工作量比较大，所以在后端做格式兼容处理。 实现 项目中只有某些字段需要做兼容处理，新建一个注解，用于标识哪些字段需要做兼容处理。 123456@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.FIELD, ElementType.PARAMETER&#125;)@Documentedpublic @interface BizDateAdapter &#123;&#125; 实现ConditionalGenericConverter接口，自定义匹配、转换逻辑。 123456789101112131415161718192021222324@Componentpublic class BizDateConverter implements ConditionalGenericConverter &#123; //匹配逻辑 @Override public boolean matches(TypeDescriptor sourceType, TypeDescriptor targetType) &#123; return targetType.hasAnnotation(BizDateAdapter.class); &#125; @Override public Set&lt;ConvertiblePair&gt; getConvertibleTypes() &#123; return null; &#125; //转换逻辑 @Override public Object convert(Object source, TypeDescriptor sourceType, TypeDescriptor targetType) &#123; if (source != null &amp;&amp; source instanceof String) &#123; Date date = DateUtil.parseDate(String.valueOf(source), \"yyyy-MM-dd\"); if (date == null)&#123; date = DateUtil.parseDate(String.valueOf(source), \"yyyy/MM/dd\"); &#125; return date; &#125; return source; &#125;&#125; 添加自定义的转换器到容器 12345678@Configurationpublic class IWebMvcConfigurer implements WebMvcConfigurer &#123; //...... @Override public void addFormatters(FormatterRegistry registry) &#123; registry.addConverter(new BizDateConverter()); &#125;&#125; 使用 123456789101112public ApiResponse xxxxx( @RequestParam(value = \"userId\", required = false) String userId, @RequestParam(value = \"bizDate\") @BizDateAdapter Date bizDate) &#123; //......&#125;@Datapublic class xxxxx &#123; @BizDateAdapter private Date bizDate; //......&#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/tags/Spring/"}]},{"title":"Docker部署主从MySQL","slug":"Docker部署主从MySQL","date":"2022-12-01T02:38:14.947Z","updated":"2022-12-01T02:38:26.080Z","comments":true,"path":"dockermysqlmasterslave/","link":"","permalink":"https://imokkkk.github.io/dockermysqlmasterslave/","excerpt":"Docker部署主从MySQL 拉取镜像 1docker pull mysql:5.7.37 准备文件夹及配置文件 123mkidir /mysql/data/mkidir /mysql/conf.d/touch /mysql/my.cnf","text":"Docker部署主从MySQL 拉取镜像 1docker pull mysql:5.7.37 准备文件夹及配置文件 123mkidir /mysql/data/mkidir /mysql/conf.d/touch /mysql/my.cnf my.conf 1234567891011121314151617[mysqld]lower_case_table_names=1user=mysqllog-bin=mysql-binserver-id=9character-set-server=utf8default_authentication_plugin=mysql_native_passwordsecure_file_priv=/var/lib/mysqlexpire_logs_days=7sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTIONmax_connections=1000[client]default-character-set=utf8[mysql]default-character-set=utf8 从库操作同上，my.conf略有不同： 123mkidir /mysql-slave/data/mkidir /mysql-slave/conf.d/touch /mysql-slave/my.cnf my.conf 12345678910111213141516171819[mysqld]lower_case_table_names=1user=mysqllog-bin=mysql-binserver-id=1slave-skip-errors=1032,1062character-set-server=utf8default_authentication_plugin=mysql_native_passwordsecure_file_priv=/var/lib/mysqlexpire_logs_days=7sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTIONmax_connections=1000[client]default-character-set=utf8[mysql]default-character-set=utf8 启动容器 12345#主库docker run -di -v /root/mysql/data:/var/lib/mysql -v /root/mysql/conf.d:/etc/mysql/conf.d -v /root/mysql/my.cnf:/etc/mysql/my.cnf -p 3306:3306 --name mysql-master -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7.37#从库 docker run -di -v /root/mysql-slave/data:/var/lib/mysql -v /root/mysql-slave/conf.d:/etc/mysql/conf.d -v /root/mysql-slave/my.cnf:/etc/mysql/my.cnf -p 3307:3306 --name mysql-master -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7.37 配置主从 主库 12345678910111213141516#连接主库mysql -h 127.0.0.1 -P 3306 -u root -p123456##创建sync用户create user 'sync'@'%' identified by '123';##授权用户grant all privileges on *.* to 'sync'@'%' ;###刷新权限flush privileges;#查看主服务器状态(显示如下)show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000002 | 2735529 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.01 sec) 从库 1234567#连接从库mysql -h 127.0.0.1 -P 3306 -u root -p123456change master to master_host='49.234.xx.xx',master_port=3306,master_user='sync',master_password='123456',master_log_file='mysql-bin.000001',master_log_pos=0;#启用从库start slave;#查看从库状态，看到Slave_IO_Running: Yes Slave_SQL_Running: Yes即可show slave status\\G; 参考： https://zhuanlan.zhihu.com/p/90486568 https://blog.csdn.net/weixin_29664819/article/details/113137387","categories":[{"name":"数据库","slug":"数据库","permalink":"https://imokkkk.github.io/categories/数据库/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://imokkkk.github.io/tags/Docker/"},{"name":"MySQL","slug":"MySQL","permalink":"https://imokkkk.github.io/tags/MySQL/"}]},{"title":"分库分表","slug":"分库分表","date":"2022-11-28T09:04:28.239Z","updated":"2022-12-14T08:54:26.277Z","comments":true,"path":"fenkufenbiaon/","link":"","permalink":"https://imokkkk.github.io/fenkufenbiaon/","excerpt":"分库分表理论什么情况下需要分库分表？ MySQL单表容量大于1000万(BTREE索引树在3-5层之间)。 垂直切分垂直分库根据业务耦合性，将业务关联度低的不同表存储在不同的数据库。类似于”微服务”。","text":"分库分表理论什么情况下需要分库分表？ MySQL单表容量大于1000万(BTREE索引树在3-5层之间)。 垂直切分垂直分库根据业务耦合性，将业务关联度低的不同表存储在不同的数据库。类似于”微服务”。 垂直分表把一个表的多个字段拆分成多个表，一般按字段的冷热拆分，热字段一个表，冷字段一个表。 优点 同时解决了业务层面的耦合； 一定程度提升IO、数据库连接数、单机硬件的资源瓶颈。 缺点 无法使用sql join，需要编码进行聚合操作，开发复杂度增加； 分布式事务处理复杂； 依然存在单表数据量大的问题。 水平切分水平分库水平分表优点 解决单表数据量过大的问题； 业务编码改造相对较小。 缺点 跨分片的事务一致性难以保证； 跨库的join关联查询性能差。 数据分片规则Hash取模按照数据的某一特征（key）来计算哈希值，并将哈希值与系统中的节点建立映射关系，从而将哈希值不同的数据分布到不同的节点上。 如选择id作为数据分片的key，n台实例，则取id的hash值 % n得到的结果就是数据所在实例。 优点 实现简单 缺点 加入或者删除一个节点的时候，需要迁移大量的数据； 很难解决数据不均衡的问题； 如果查询条件中不带用于分片的key，那么需要查询所有分库，再在内存中合并数据，效率低，复杂度高/ 范围分片按照关键值划分成不同的区间，每个物理节点负责一个或者多个区间。 如id 0-10000数据位于实例1，10000-20000位于实例2…。 优点 单表/库大小可控； 易于水平扩展，假如/删除实例时，无需对其它实例的数据迁移； 易于范围查询。 缺点 热点数据(如按时间字段分片)； 元数据(每个实例存储哪些数据区间)相对复杂一些。 一致性Hash一致性 hash 是将数据按照特征值映射到一个首尾相接的 hash 环上，同时也将节点（按照 IP 地址或者机器名 hash）映射到这个环上。对于数据，从数据在环上的位置开始，顺时针找到的第一个节点即为数据的存储节点。 特点 一致性 hash 方式在增删的时候只会影响到 hash 环上相邻的节点，不会发生大规模的数据迁移； 一致性 hash 方式在增加节点时，只能分摊一个节点的压力；删除节点时，改节点的压力只能转移到下一个节点，所以实际工程中一般引入虚拟节点，即节点个数远大于物理节点个数，一个物理节点负责多个虚拟节点的真实存储。操作数据时，先通过hash环找到对应的虚拟节点，再通过虚拟节点与物理节点的映射关系找到对应的物理节点。 优点 一致性 hash 方式在节点增删的时候只会影响到 hash 环上相邻的节点，不会发生大规模的数据迁移； 可以根据物理节点的性能来调整每一个物理节点对于虚拟节点的数量，充分、合理利用资源。 缺点 需要维护的元数据增加(虚拟节点与物理节点的映射关系) 分库分表引入的问题 分布式事务 跨节点join关联查询 字段冗余：空间换时间，避免join查询。如订单表保存userId时候，也将userName冗余保存一份，这样查询订单详情时就不需要再去查询”user表”； 数据组装：分为多次子查询请求，最后在内存中组装结果。 跨节点分页、排序、函数问题 当排序字段就是分片字段时，通过分片规则就比较容易定位到指定的分片；如果排序字段非分片字段，需要在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序。 全局主键 雪花算法(时钟回拨问题)，百度uid-generator，美团Leaf、 ShardingShereShardingSphere 是一款分布式的数据库生态系统， 可以将任意数据库转换为分布式数据库，并通过数据分片、弹性伸缩、加密等能力对原有数据库进行增强。 常用功能 数据分片 分库 &amp; 分表 读写分离 分片策略定制 无中心化分布式主键 分布式事务 LOCAL 事务 XA 强一致性事务 BASE柔性事务 数据库治理 数据脱敏、加密 流量治理 数据迁移 ShardingSphere-JDBCShardingSphere-Proxy结果归并将各个数据节点获取的多数据结果集，组合、处理成一个结果集并返回给请求。 遍历、排序、分组、分页、聚合。 遍历归并 将多个数据结果集合并为一个单向链表； 排序归并 每个数据结果集自身有序，所以需要对多个有序的数组进行排序；ShardingSphere在对排序的查询进行归并时，将每个结果集的当前数据值进行比较（通过实现Java的Comparable接口完成），并将其放入优先级队列。 分组归并 流式分组归并：要求SQL的排序项与分组项的字段及排序类型(ASC或DESC)必须保持一致； 内存归并 聚合归并 装饰者模式 分页归并 ShardingSphere通过装饰者模式来增加对数据结果集进行分页的能力。 实践ShardingSphere读写分离Docker部署主从MySQL参考：https://imokkkk.github.io/dockermysqlmasterslave/ 代码实现 依赖 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;4.1.1&lt;/version&gt; &lt;/dependency&gt;&lt;!-- 必须搭配druid，不能使用druid-spring-boot-starter--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.2.8&lt;/version&gt; &lt;/dependency&gt; 配置 12345678910111213141516171819202122232425262728spring: main: allow-bean-definition-overriding: true shardingsphere: datasource: names: master0,slave0 master0: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://49.234.xx.xxx:3306/url_gen?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8 username: root password: ENC(xPowsGGk7qtWdvCvCtChLOVLXPAyMPOrrZWKSmN5mwyKIzgAwBxNx4uxcJ+9Ksbn) slave0: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://49.234.xx.xxx:3307/url_gen?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8 username: root password: 123456 sharding: master-slave-rules: ms: masterDataSourceName: master0 slaveDataSourceNames: - slave0 load-balance-algorithm-type: round_robin props: sql: show: true 效果 增删改操作主库，查询操作从库： 详细的实现代码：https://github.com/ImOkkkk/short-link-generator/tree/shardingsphere-read-write-splitting ShardingSphere读写分离+单库分表代码实现 依赖 同上 配置 12345678910111213141516171819202122232425262728293031323334spring: main: allow-bean-definition-overriding: true shardingsphere: datasource: master: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://49.234.xx.xxx:3306/url_gen?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8 username: root password: ENC(xPowsGGk7qtWdvCvCtChLOVLXPAyMPOrrZWKSmN5mwyKIzgAwBxNx4uxcJ+9Ksbn) slave0: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://49.234.xx.xxx:3307/url_gen?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8 username: root password: 123456 names: master, slave0 sharding: tables: url: actual-data-nodes: ds0.url$-&gt;&#123;0..1&#125; table-strategy: inline: sharding-column: surl #对url表surl字段hash取模(分表的个数) algorithm-expression: url$-&gt;&#123;Math.abs(surl.hashCode())%2&#125; master-slave-rules: ds0: master-data-source-name: master slave-data-source-names: slave0 props: sql: show: true 效果 批量插入3条数据：2条数据新增到主库的url0表，1条数据新增到url1表。 详细的实现代码：https://github.com/ImOkkkk/short-link-generator/tree/shardingsphere-read-write-splitting-table ShardingSphere分库+分表 依赖 同上 配置 1234567891011121314151617181920212223242526272829303132333435spring: main: allow-bean-definition-overriding: true shardingsphere: datasource: ds0: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://49.234.xx.xxx:3306/url_gen?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8 username: root password: ENC(xPowsGGk7qtWdvCvCtChLOVLXPAyMPOrrZWKSmN5mwyKIzgAwBxNx4uxcJ+9Ksbn) ds1: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://49.234.xx.xxx:3306/url_gen1?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8 username: root password: ENC(xPowsGGk7qtWdvCvCtChLOVLXPAyMPOrrZWKSmN5mwyKIzgAwBxNx4uxcJ+9Ksbn) names: ds0, ds1 sharding: #分库策略 default-database-strategy: inline: sharding-column: sid algorithm-expression: ds$-&gt;&#123;Math.abs(sid.hashCode())%2&#125; #分表策略 tables: url: actual-data-nodes: ds$-&gt;&#123;0..1&#125;.url$-&gt;&#123;0..1&#125; table-strategy: inline: sharding-column: surl algorithm-expression: url$-&gt;&#123;Math.abs(surl.hashCode())%2&#125; props: sql: show: true 效果 批量插入3条数据： 1条数据新增到ds1的url1表； 1条数据新增到ds1的url0表； 1条数据新增到ds0的url0表。 详细的实现代码：https://github.com/ImOkkkk/short-link-generator/tree/shardingsphere-splitting-database-table","categories":[{"name":"分布式","slug":"分布式","permalink":"https://imokkkk.github.io/categories/分布式/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://imokkkk.github.io/tags/MySQL/"},{"name":"分库分表","slug":"分库分表","permalink":"https://imokkkk.github.io/tags/分库分表/"},{"name":"ShardingSphere","slug":"ShardingSphere","permalink":"https://imokkkk.github.io/tags/ShardingSphere/"}]},{"title":"CAP理论&常用的分布式事务概览","slug":"CAP理论&常用的分布式事务概览","date":"2022-11-25T02:27:49.050Z","updated":"2022-11-25T02:27:56.979Z","comments":true,"path":"cpaandtransaction/","link":"","permalink":"https://imokkkk.github.io/cpaandtransaction/","excerpt":"CAP理论&amp;常用的分布式事务概览CAPCAP理论是分布式系统的重要理论，即一个分布式系统最多只能同时满足一致性(C: Consistency)、可用性(A: Availability)、分区容错性(Partition tolerance)这三项中的两项。 一致性 数据能一起变化，能让数据整齐划一。","text":"CAP理论&amp;常用的分布式事务概览CAPCAP理论是分布式系统的重要理论，即一个分布式系统最多只能同时满足一致性(C: Consistency)、可用性(A: Availability)、分区容错性(Partition tolerance)这三项中的两项。 一致性 数据能一起变化，能让数据整齐划一。 可用性 每一个非故障节点接收的任何请求，都能处理并返回响应结果。 分区容错性 分区：分布式系统中，节点之间通信出现了问题。 如果出现了分区问题，系统仍然可以运行。 权衡在分布式系统中，网络异常不可避免，所以P往往无法忽略，所以需要考虑在发生分区故障时，如何选择C和A。 CP 系统一旦发生分区故障后，允许系统停机或者长时间无响应，但系统每个节点总是会返回一致的数据。如：分布式协调系统Zookeeper、分布式存储系统Redis等，数据一致性是最基本的要求。 AP 如果系统发生分区故障后，依然可以访问系统，但是无法保证全局数据的一致性(舍弃数据的强一致性，退而求其次保证最终一致性)。如：Eureka CAP如何权衡和取舍没有好坏之分，需要根据不同的业务场景进行选择，适合的才是最好的。对于涉及到钱这种不能有任何差错的场景，数据强一致性是必须要保证的。对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。 BASE既然无法保证强一致性(Strong Consistency)，应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性(Eventual consistency)。 BASE指基本可用(Basically Available)、软状态(Soft State)、最终一致性(Eventual Consistency)。 基本可用 分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。 电商大促，流量激增，部分用户可能会被引导到降级页面，服务层也可能提供降级服务； 网络异常，接口响应从0.5s增加到2s。 软状态 允许系统存在中间状态，而该中间状态不会影响系统整体可用性。即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。如MySQL主从节点间的数据同步。 最终一致性 统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 常见分布式事务分布式事务可以简单的理解为协调多个资源，达到共同提交或共同失败的效果，及分布式ACID。 两阶段提交(2PC)分布式系统中，所有的节点虽然都可以知道自己执行后的状态，但无法知道其它节点执行后的状态，一个事务跨越多个系统时，需要引入一个协调者的组件来统一各个节点的执行结果是否提交。 准备阶段 协调者询问事务的参与者是否可以执行操作，并等待参与者响应，参与者会执行相应的事务操作并记录重做和回滚日志，所有执行成功的参与者向协调整发送AGREEMENT 或者 ABORT 表示执行操作的结果。 提交阶段 当事务的所有参与者都决定提交事务时，协调者会向参与者发送 COMMIT 请求，参与者在完成操作并释放资源之后向协调者返回完成消息，协调者在收到所有参与者的完成消息时会结束整个事务；与之相反，当有参与者决定 ABORT 当前事务时，协调者会向事务的参与者发送回滚请求，参与者会根据之前执行操作时的回滚日志对操作进行回滚并向协调者发送完成的消息，在提交阶段，无论当前事务被提交还是回滚，所有的资源都会被释放并且事务也一定会结束。 问题 阻塞； 两阶段提交的执行过程中，如果协调者永久宕机，部分参与者将永远无法完成事务；整个事务不一致。 三阶段提交(3PC)三阶段提交相较于二阶段提交引入了超时机制和询问阶段，如果协调者或者参与者在规定之间内没有接受到来自其它节点的响应，就会根据当前的状态选择提交或终止整个事务。 3PC 2PC 询问 准备 准备 提交 提交 3PC的询问阶段，对应的才是2PC的准备阶段，都是ASK参与者是否准备好了，但2PC的执行过程是阻塞的，一个资源在进入准备阶段之后，必须等所有的资源准备完毕后才能进入下一步，对全局一无所知。而3PC拆分出询问阶段，在确保所有参与者建康良好的情况下，才会发起真正的事务处理，在效率和容错性上更胜一筹。而且3PC在准备阶段，如果超时，就认为失败；在提交阶段如果超时还会继续执行下去。整个事务不会一直等待下去。 问题 准备与提交阶段不是原子的，与2PC一样，依然存在一致性问题。 TCC补偿事务，为每一个操作，都准备一个确认动作和相应的补偿动作。 try 尝试阶段 尝试锁定资源； confirm 确认阶段 尝试将锁定的资源提交； cancel 取消阶段 其中某个环节执行失败，发起事务取消动作。 如：资金转账：try就是冻结金额；confirm就是完成扣减；cancel就是解冻，只要对应的订单号是一直的，多次执行也不会有任何问题。 问题 需要编码实现，额外精力设计TCC逻辑。 框架 tcc-transaction、seata等。 SAGA很多业务场景下，只需要保证业务的最终一致性。SAGA通过消息来协调一系列的本地子事务，来保证最终一致性。 与TCC相比，SAGA少了try，需要提供执行逻辑和补偿逻辑。在每一个本地事务中都会向集群中的其他服务发送一条的新的消息来触发下一个本地的事务；一旦本地的事务因为违反了业务逻辑而失败，那么就会立刻触发补偿逻辑来撤回之前本地事务造成的副作用。 问题 嵌套问题，SAGA只允许两层嵌套，层次过深的话，消息流转会太复杂； 补偿操作无法保证执行成功，可以记录日志或人工参与； 本地小事务不是同时提交的，执行过程中，会产生脏数据。 框架 seata 本地消息表解决数据库事务和MQ之间的事务问题。 有一个分布式事务，在正常落库之后，需要通过MQ来协调后续业务的执行。 正常写入数据库；写入本地消息表(记录MQ消息处理状态，发送中和已完成)，此处使用本地事务； 写入本地消息表成功后，异步发送MQ消息； 后续业务订阅MQ消息，消费成功后，把执行成功的状态再通过MQ来发送。本地业务订阅这个执行状态，并把消息表中对应的记录状态，改为已完成；如果消费失败，则不做处理； 存在一个定时任务，持续扫描本地消息表中，状态为发送中的消息(注意延时)，并再次把这些消息发送到MQ，重复2的过程。 问题 需要额外编码，与业务耦合； 本地消息表需要写数据库，增加数据库压力。 总结实际中，普遍选用软事务，TCC、SAGA、本地消息表。SAGA应对长事务特别拿手，但隔离性稍差；TCC需要较多编码，适合少量的分布式事务流程；本地消息表应用场景有限，耦合业务不能复用。各有利弊，需要结合使用场景进行选择。","categories":[{"name":"分布式","slug":"分布式","permalink":"https://imokkkk.github.io/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://imokkkk.github.io/tags/分布式/"},{"name":"事务","slug":"事务","permalink":"https://imokkkk.github.io/tags/事务/"}]},{"title":"扩展EnvironmentPostProcessor从数据库加载配置项&spring.factories","slug":"扩展EnvironmentPostProcessor从数据库加载配置项&spring.factories","date":"2022-11-25T02:26:05.047Z","updated":"2023-01-18T05:46:15.105Z","comments":true,"path":"EnvironmentPostProcessor/","link":"","permalink":"https://imokkkk.github.io/EnvironmentPostProcessor/","excerpt":"应用未接入配置中心时，一些配置项(如oss存储配置信息、邮件服务配置信息等)需要从其它数据源获取，下面以从数据库获取配置信息为例。 @Value既然需要通过从数据库中读取配置信息，那么先了解一下@Value的工作原理：","text":"应用未接入配置中心时，一些配置项(如oss存储配置信息、邮件服务配置信息等)需要从其它数据源获取，下面以从数据库获取配置信息为例。 @Value既然需要通过从数据库中读取配置信息，那么先了解一下@Value的工作原理： SpringBoot应用启动 org.springframework.boot.SpringApplication#run(java.lang.Class&lt;?&gt;, java.lang.String...) =&gt; org.springframework.boot.SpringApplication#run(java.lang.String...) =&gt; 刷新Spring容器 org.springframework.boot.SpringApplication#refreshContext =&gt; org.springframework.context.support.AbstractApplicationContext#refresh 实例化Bean org.springframework.context.support.AbstractApplicationContext#finishBeanFactoryInitialization =&gt; org.springframework.beans.factory.config.ConfigurableListableBeanFactory#preInstantiateSingletons（实例化非懒加载的单例Bean) =&gt; org.springframework.beans.factory.support.AbstractBeanFactory#getBean(java.lang.String) =&gt; org.springframework.beans.factory.support.AbstractBeanFactory#doGetBean =&gt; org.springframework.beans.factory.support.AbstractBeanFactory#createBean =&gt; org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#doCreateBean 属性填充 org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#populateBean =&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374protected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) &#123; if (bw == null) &#123; if (mbd.hasPropertyValues()) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Cannot apply property values to null instance\"); &#125; else &#123; // Skip property population phase for null instance. return; &#125; &#125; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123; //实例化后 if (!bp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; return; &#125; &#125; &#125; PropertyValues pvs = (mbd.hasPropertyValues() ? mbd.getPropertyValues() : null); //@Bean(autowire = Autowire.BY_NAME) int resolvedAutowireMode = mbd.getResolvedAutowireMode(); if (resolvedAutowireMode == AUTOWIRE_BY_NAME || resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // Add property values based on autowire by name if applicable. if (resolvedAutowireMode == AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // Add property values based on autowire by type if applicable. if (resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; pvs = newPvs; &#125; boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); boolean needsDepCheck = (mbd.getDependencyCheck() != AbstractBeanDefinition.DEPENDENCY_CHECK_NONE); PropertyDescriptor[] filteredPds = null; if (hasInstAwareBpps) &#123; if (pvs == null) &#123; pvs = mbd.getPropertyValues(); &#125; for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123; //依赖注入入口 //@Autowired @Value：AutowiredAnnotationBeanPostProcessor @Resource：CommonAnnotationBeanPostProcessor PropertyValues pvsToUse = bp.postProcessProperties(pvs, bw.getWrappedInstance(), beanName); if (pvsToUse == null) &#123; if (filteredPds == null) &#123; filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); &#125; pvsToUse = bp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvsToUse == null) &#123; return; &#125; &#125; pvs = pvsToUse; &#125; &#125; if (needsDepCheck) &#123; if (filteredPds == null) &#123; filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); &#125; checkDependencies(beanName, mbd, filteredPds, pvs); &#125; if (pvs != null) &#123; // MergedBeanDefinitionPostProcessor.postProcessMergedBeanDefinition()会覆盖@Autowired applyPropertyValues(beanName, mbd, bw, pvs); &#125;&#125; org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor#postProcessProperties =&gt; org.springframework.beans.factory.annotation.InjectionMetadata#inject =&gt; 1234567891011121314public void inject(Object target, @Nullable String beanName, @Nullable PropertyValues pvs) throws Throwable &#123; Collection&lt;InjectedElement&gt; checkedElements = this.checkedElements; Collection&lt;InjectedElement&gt; elementsToIterate = (checkedElements != null ? checkedElements : this.injectedElements); if (!elementsToIterate.isEmpty()) &#123; for (InjectedElement element : elementsToIterate) &#123; //遍历每个注入点进行依赖注入 //Autowired：AutowiredAnnotationBeanPostProcessor.AutowiredFieldElement.inject //AutowiredAnnotationBeanPostProcessor.AutowiredMethodElement.inject //Resource：InjectionMetadata.InjectedElement.inject element.inject(target, beanName, pvs); &#125; &#125; &#125; org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.AutowiredFieldElement#resolveFieldValue =&gt; org.springframework.beans.factory.support.DefaultListableBeanFactory#resolveDependency =&gt; org.springframework.beans.factory.support.DefaultListableBeanFactory#doResolveDependency 处理@Value org.springframework.beans.factory.annotation.QualifierAnnotationAutowireCandidateResolver#findValue org.springframework.beans.factory.support.AbstractBeanFactory#resolveEmbeddedValue =&gt; org.springframework.context.support.PropertySourcesPlaceholderConfigurer#processProperties(org.springframework.beans.factory.config.ConfigurableListableBeanFactory, org.springframework.core.env.ConfigurablePropertyResolver) (把所有的配置文件都变成了一个个propertysource对象，同时把environment对象也包装成了一个propertysource对象，并且一个个propertysource对象存储在了MutablePropertySources中。) 大致上来看，@Value解析，分为以下几步： org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor#postProcessMergedBeanDefinition =&gt; org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor#findAutowiringMetadata =&gt; org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor#buildAutowiringMetadata 收集标注了@Value等注解的字段； org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor#postProcessProperties开始注入值； 注入过程中，从DefaultListableBeanFactory中遍历所有的embeddedValueResolver，这些embeddedValueResolvers是StringValueResolver，调用其resolveStringValue方法； 其中有一个PropertySourcesPlaceholderConfigurer类构造的StringValueResolver，调用它的resolveStringValue，最终从一个PropertySourcesPropertyResolver的propertySources中遍历所有的propertySource，其中就有Environment的propertySources，匹配到值后返回。 EnvironmentPostProcessor从上面可以得出结论，在容器refresh之前，从数据库读取信息封装为MapPropertySource塞入Environment#Property即可。 主要实现类： 123456789101112131415161718192021222324252627282930313233343536public class DatabaseEnvironmentPostProcessor implements EnvironmentPostProcessor &#123; @Override public void postProcessEnvironment( ConfigurableEnvironment environment, SpringApplication application) &#123; if (environment.getPropertySources().contains(\"databasePropertySources\")) &#123; return; &#125; // 命令行参数 boolean commandLineArgs = environment.getPropertySources().contains(\"commandLineArgs\"); if (commandLineArgs) &#123; environment .getPropertySources() .addBefore(\"commandLineArgs\", loadConfigurationFromDatabase(environment)); &#125; else &#123; if (environment.getProperty(\"spring.datasource.url\") != null) &#123; environment.getPropertySources().addFirst(loadConfigurationFromDatabase(environment)); // 设置激活的Profile String activeProfile = environment.getProperty(\"spring.profiles.active\", \"prd\"); environment.addActiveProfile(activeProfile); &#125; &#125; &#125; private PropertySource loadConfigurationFromDatabase(ConfigurableEnvironment environment) &#123; String url = environment.getProperty(\"spring.datasource.url\"); String username = environment.getProperty(\"spring.datasource.username\"); String password = environment.getProperty(\"spring.datasource.password\"); String driverClassName = environment.getProperty(\"spring.datasource.druid.driver-class-name\"); Map&lt;String, ?&gt; configs = new DatabasePropertySourceLoader(url, username, password, driverClassName).load(); PropertySource propertySource = new MapPropertySource(\"databasePropertySources\", (Map&lt;String, Object&gt;) configs); return propertySource; &#125;&#125; spring.factories 12org.springframework.boot.env.EnvironmentPostProcessor=\\cn.imokkkk.env.DatabaseEnvironmentPostProcessor spring.factoriesSpring Factories是一种类似于Java SPI的机制，在resources/META-INF/spring.factories文件中配置接口的实现类名称(接口名称=实现类)，然后在程序中读取该配置文件并实例化，是spring-boot-starter-xxx的实现基础。 为了实现从数据库读取配置信息的需求，显然需要在容器refresh之前完成从数据库读取并添加到环境变量中。 调用链路 从spring.factories加载ApplicationListener对应的监听器，并启动 org.springframework.boot.SpringApplication#run(java.lang.String...) =&gt; org.springframework.boot.SpringApplication#getRunListenersorg.springframework.boot.SpringApplication#run(java.lang.String...) =&gt; org.springframework.boot.SpringApplication#getSpringFactoriesInstances(java.lang.Class&lt;T&gt;, java.lang.Class&lt;?&gt;[], java.lang.Object...) =&gt; org.springframework.core.io.support.SpringFactoriesLoader#loadFactoryNames 发布ApplicationEnvironmentPreparedEvent org.springframework.boot.SpringApplication#prepareEnvironment =&gt; org.springframework.boot.context.event.EventPublishingRunListener#environmentPrepared 监听到事件并执行 org.springframework.context.event.SimpleApplicationEventMulticaster#invokeListener =&gt; org.springframework.boot.context.config.ConfigFileApplicationListener#onApplicationEvent =&gt; org.springframework.boot.context.config.ConfigFileApplicationListener#onApplicationEnvironmentPreparedEvent =&gt; cn.imokkkk.env.DatabaseEnvironmentPostProcessor#postProcessEnvironment sql语句： 123456789&gt;CREATE TABLE `app_config` (&gt; `id` bigint(20) NOT NULL AUTO_INCREMENT,&gt; `config_key` varchar(50) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL,&gt; `config_value` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL,&gt; `remark` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL,&gt; `is_halt` char(1) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL,&gt; PRIMARY KEY (`id`) USING BTREE&gt;) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci;&gt;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/categories/Spring/"}],"tags":[{"name":"源码","slug":"源码","permalink":"https://imokkkk.github.io/tags/源码/"},{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/tags/Spring/"}]},{"title":"Spring源码","slug":"Spring源码","date":"2022-09-06T08:35:59.549Z","updated":"2022-11-10T09:19:05.658Z","comments":true,"path":"SpringSourceCode/","link":"","permalink":"https://imokkkk.github.io/SpringSourceCode/","excerpt":"Spring源码核心注解","text":"Spring源码核心注解 注解 功能 @Bean 容器中注册组件 @Primary 同类组件如果有多个，标注主组件 @DependsOn 组件之间声明依赖关系 @Lazy 组件懒加载(使用的时候才会创建) @Scope 声明组件的作用域(SCOPE_PROTOTYPE，SCOPE_SINGLETON) @Configuration 声明这是一个配置类，替换配置文件 @Component @Controller、@Service、@Repository @Indexed 加速注解，标注了@Indexed的组件，直接会启动快速加载 @Order 数字越小优先级越高，越先工作 @ComponentScan 包扫描 @Conditional 条件注入 @Import 导入第三方Jar包中的组件，或定制批量导入组件逻辑 @ImportResource 导入以前的xml配置文件，让其生效 @Profile 基于多环境激活 @PropertySource 外部properties配置文件和JavaBean进行绑定，结合ConfigurationProperties @PropertySource @PropertySource组合注解 @Autowired 自动装配，默认按类型装配 @Qualifier 精确指定，默认按名称装配 @Resource 来自于JSR-250，非Spring提供，默认按名称装配 @Value 取值、计算机环境变量、JVM系统。@Value(“${XX}”) @Lookup 单例组件依赖非单例组件，非单例组件获取需要使用方法 Resource&amp;ResourceLoaderResource需要处理不同类型的外部资源(URL、File、ClassPath等等)，而且处理这些资源步骤都是类似的(打开资源，读取资源，关闭资源)。Spring提供Resource接口来统一这些底层资源一致的访问，作为所有资源的统一抽象。 Resource体系 Resource 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public interface Resource extends InputStreamSource &#123; /** * 当前Resource资源是否存在 */ boolean exists(); /** * 当前Resource是否可读 */ default boolean isReadable() &#123; return exists(); &#125; /** * 当前Resource是否已经打开，如果返回true，则只能被读取一次然后关闭以避免资源泄露； * 常见的Resource实现一般返回false */ default boolean isOpen() &#123; return false; &#125; /** * 是否为文件资源 */ default boolean isFile() &#123; return false; &#125; /** * 如果当前Resource代表的资源能由java.util.URL代表，则返回该URL，否则抛出IOException */ URL getURL() throws IOException; /** * 如果当前Resource代表的资源能由java.util.URI代表，则返回该URL，否则抛出IOException */ URI getURI() throws IOException; /** * 如果当前Resource代表的底层资源能由java.io.File代表，则返回该File，否则抛出IOException */ File getFile() throws IOException; default ReadableByteChannel readableChannel() throws IOException &#123; return Channels.newChannel(getInputStream()); &#125; /** * 返回当前Resource代表的底层文件资源的长度，一般是值代表的文件资源的长度 */ long contentLength() throws IOException; /** * 返回当前Resource代表的底层资源的最后修改时间 */ long lastModified() throws IOException; Resource createRelative(String relativePath) throws IOException; /** * 返回当前Resource代表的底层文件资源的文件路径，比如File资源“file://d:/test.txt”将返回“d:/test.txt”； * 而URL资源http://www.baidu.cn将返回null，因为只返回文件路径 */ @Nullable String getFilename(); /** * 返回当前Resource代表的底层资源的描述符，通常就是资源的全路径（实际文件名或实际URL地址） */ String getDescription();&#125; ByteArrayResource：对于字节数组的实现，为其构造一个ByteArrayInputStream； ClassPathResource：ClassPath类型资源的实现，使用指定的Class或ClassLoader加载资源。用来访问类加载路径下的资源，该种类型在Web应用中可以自动搜索位于WEB-INF/classes下的资源文件，而无需使用绝对路径访问； InputStreamResource：InputStream的实现，如果需要将资源描述符保存在某处，或者如果需要从流中多次读取，不要使用InputStreamResource； UrlResource：对于java.net.URL类型资源的实现，支持File和URL的形式，既可以访问网络资源，也可以访问本地资源，加不同的前缀即可； FileSystemResource：对java.io.File类型和java.nio.file.Path资源的封装，支持File和URL的形式。实现了WritableResource接口，支持对资源的写操作； EncodedResource：实现对文件编码类型的处理。 它们都继承抽象类AbstractResource，AbstractResource实现了Resource接口。自定义Resource只需继承AbstractResource抽象类，它已经实现Resource接口的大部分公共实现，再根据自定义的资源特性覆盖相应的方法。 ResourceLoaderResource定义统一的资源，ResourceLoader加载资源。 ResourceLoader体系 DefaultResourceLoader 1234567891011121314151617181920212223242526272829303132333435public class DefaultResourceLoader implements ResourceLoader &#123; ...... @Override public Resource getResource(String location) &#123; Assert.notNull(location, \"Location must not be null\"); // 是否有自定义协议策略 for (ProtocolResolver protocolResolver : getProtocolResolvers()) &#123; Resource resource = protocolResolver.resolve(location, this); if (resource != null) &#123; return resource; &#125; &#125; // 以“/”开头，构造返回ClassPathContextResource if (location.startsWith(\"/\")) &#123; return getResourceByPath(location); &#125; else if (location.startsWith(CLASSPATH_URL_PREFIX)) &#123; // 以\"classpath:\"开头，构造返回ClassPathResource return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); &#125; else &#123; // 否则构造URL地址 try &#123; URL url = new URL(location); // 如果是FileURL，构造返回FileUrlResource，否则构造返回UrlResource return (ResourceUtils.isFileURL(url) ? new FileUrlResource(url) : new UrlResource(url)); &#125; catch (MalformedURLException ex) &#123; //构造URL失败(不符合URL规范)，当作普通路径处理 return getResourceByPath(location); &#125; &#125; &#125; ......&#125; ProtocolResolver 用户自定义协议资源解决策略，作为DefaultResourceLoader的SPI，允许用户自定义资源加载协议，而不需要继承ResourceLoader的子类。实现ProtocolResolver接口。 1234567891011121314151617181920public class MyProtocolResolver implements ProtocolResolver &#123; @Override public Resource resolve(String location, ResourceLoader resourceLoader) &#123; System.out.println(\"自定义加载资源......\"); FileSystemResourceLoader fileSystemResourceLoader = new FileSystemResourceLoader(); return fileSystemResourceLoader.getResource(\"D:\\\\IDEAProbject\\\\openSources\\\\spring-framework\\\\gradle\\\\docs.gradle\"); &#125;&#125;public class Demo &#123; public static void main(String[] args) &#123; DefaultResourceLoader resourceLoader = new DefaultResourceLoader(); resourceLoader.addProtocolResolver(new MyProtocolResolver()); Resource resource = resourceLoader.getResource(\"/\"); System.out.println(resource.getFilename()); //docs.gradle System.out.println(resource.getDescription()); //file [D:\\IDEAProbject\\openSources\\spring-framework\\gradle\\docs.gradle] &#125;&#125; ResourcePatternResolver——ResourceLoader接口的增强 1234567891011public interface ResourcePatternResolver extends ResourceLoader &#123; /** * 支持classpath*:形式路径匹配，即Ant风格；允许使用通配符来对路径进行匹配。\"classpath*:\"可以返回路径下所有满足条件的资源实例 */ String CLASSPATH_ALL_URL_PREFIX = \"classpath*:\"; /** * 支持根据路径匹配模式返回多个 Resource 实例 */ Resource[] getResources(String locationPattern) throws IOException;&#125; PatchMatchingResourcePatternResolver ResourcePatternResolver最常用的子类 123456789101112public class PathMatchingResourcePatternResolver implements ResourcePatternResolver &#123; // 负责对基于字符串的路径和指定的模式符号进行匹配 private PathMatcher pathMatcher = new AntPathMatcher(); /** * 实例化一个DefaultResourceLoader，继承自ResourceLoader的方法委托给内部的DefaultResourceLoader实现 * PathMatchingResourcePatternResolver只负责处理实现ResourcePatternResolver中的方法 */ public PathMatchingResourcePatternResolver() &#123; this.resourceLoader = new DefaultResourceLoader(); &#125;&#125; ApplicationContext&amp;BeanFactoryBeanFactoryBeanFactory体系 BeanFactory BeanFactory是访问Spring中Bean容器的顶级接口，提供了IOC容器应遵守的最基本的接口。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public interface BeanFactory &#123; /** * FactoryBean实例前缀，如bean的名称为myJndiObject，并且该bean是FactoryBean，则获得&amp;myJndiObject的bean时会返回Factory而不是Factory返回的实例 */ String FACTORY_BEAN_PREFIX = \"&amp;\"; /** * 返回指定名称的bean * 如果name是别名，最终会转换回真实的名称； * 如果本Factory实例没找到，会询问父Factory查找； * 未找到抛出BeansException */ Object getBean(String name) throws BeansException; /** * 与&#123;@link #getBean(String)&#125;类似，增加类型校验，如果类型异常抛出BeanNotOfRequiredTypeException */ &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException; /** * 通过给定的参数匹配构造函数或者工厂方法创建给定名称的实例。 */ Object getBean(String name, Object... args) throws BeansException; /** * 按类型获取唯一Bean */ &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException; /** * 通过给定的参数匹配构造函数或者工厂方法创建给定类型的实例。 */ &lt;T&gt; T getBean(Class&lt;T&gt; requiredType, Object... args) throws BeansException; /** * 返回指定Bean的Provider，允许延迟按需检索实例 */ &lt;T&gt; ObjectProvider&lt;T&gt; getBeanProvider(Class&lt;T&gt; requiredType); &lt;T&gt; ObjectProvider&lt;T&gt; getBeanProvider(ResolvableType requiredType); /** * 是否存在指定名称的bean * 如果本Factory实例没找到，会询问父Factory查找； */ boolean containsBean(String name); /** * 判断name对应的bean是否为单例的 */ boolean isSingleton(String name) throws NoSuchBeanDefinitionException; /** * 判断name对应的bean是否为原型作用域 */ boolean isPrototype(String name) throws NoSuchBeanDefinitionException; /** * 判断name对应的bean与指定的类型是否匹配，更具体来说就是判断getBean方法返回的对象与目标类型是否匹配 * ResolvableType.forClass(XXXX.class))) */ boolean isTypeMatch(String name, ResolvableType typeToMatch) throws NoSuchBeanDefinitionException; /** * 判断name对应的bean与指定的类型是否匹配 */ boolean isTypeMatch(String name, Class&lt;?&gt; typeToMatch) throws NoSuchBeanDefinitionException; /** * 根据name获取其对应的bean的类型 */ @Nullable Class&lt;?&gt; getType(String name) throws NoSuchBeanDefinitionException; /** * 根据name获取其对应的bean的类型。指定是否允许FactoryBean初始化 */ @Nullable Class&lt;?&gt; getType(String name, boolean allowFactoryBeanInit) throws NoSuchBeanDefinitionException; /** * 返回指定名称的所有别名信息，如果name是别名，返回原始名和其他笔名，原始名在数组的第一个元素 */ String[] getAliases(String name);&#125; 有3个接口直接继承了BeanFactory：HierarchicalBeanFactory、ListableBeanFactory、AutowireCapableBeanFactory。 HierarchiaclBeanFactory 增加了层级结构管理的功能，通过ConfigurableBeanFactory接口中的setParentBeanFactory，允许配置父BeanFactory。 123456789101112public interface HierarchicalBeanFactory extends BeanFactory &#123; /** * 返回当前BeanFactory的父BeanFactory，如果没有返回null */ @Nullable BeanFactory getParentBeanFactory(); /** * 返回本BeanFactory是否包含指定namne的bean，不考虑父BeanFactory */ boolean containsLocalBean(String name);&#125; ListableBeanFactory 可以列表所有内部定义的bean。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public interface ListableBeanFactory extends BeanFactory &#123; /** * 判断BeanFactory是否含给定名称的Bean定义，并不考虑层次结构，并忽略通过Bean定义以外方式注册的单例Bean */ boolean containsBeanDefinition(String beanName); /** * 获取BeanFactory中已经定义的Bean数量，并不考虑层次结构，并忽略通过Bean定义以外方式注册的单例Bean */ int getBeanDefinitionCount(); /* * 返回本BeanFactory中所有Bean定义的名称 */ String[] getBeanDefinitionNames(); /** * 返回指定Bean的提供者，支持延迟初始化 */ &lt;T&gt; ObjectProvider&lt;T&gt; getBeanProvider(Class&lt;T&gt; requiredType, boolean allowEagerInit); &lt;T&gt; ObjectProvider&lt;T&gt; getBeanProvider(ResolvableType requiredType, boolean allowEagerInit); String[] getBeanNamesForType(ResolvableType type); /** * 返回与给定类型及子类匹配的bean的名称 * allowEagerInit 是否初始化FactoryBean。如果为false，会检查原始的FactoryBean是否与给定类型匹配，不考虑任何层次结构 */ String[] getBeanNamesForType(ResolvableType type, boolean includeNonSingletons, boolean allowEagerInit); String[] getBeanNamesForType(@Nullable Class&lt;?&gt; type); String[] getBeanNamesForType(@Nullable Class&lt;?&gt; type, boolean includeNonSingletons, boolean allowEagerInit); /** * 通过给定的类型获取其对应的bean实例，其中map的key是bean的名称，value是该bean对应的实例 */ &lt;T&gt; Map&lt;String, T&gt; getBeansOfType(@Nullable Class&lt;T&gt; type) throws BeansException; &lt;T&gt; Map&lt;String, T&gt; getBeansOfType(@Nullable Class&lt;T&gt; type, boolean includeNonSingletons, boolean allowEagerInit) throws BeansException; /** * 查找包含给定注解的bean的名称，而不创建对应的bean实例 */ String[] getBeanNamesForAnnotation(Class&lt;? extends Annotation&gt; annotationType); /* * 查找包含给定注解的bean名称，并创建对应的bean实例，其中返回的map中key为bean名称，value为bean的实例 */ Map&lt;String, Object&gt; getBeansWithAnnotation(Class&lt;? extends Annotation&gt; annotationType) throws BeansException; /** * 在给定name的bean上面查找是否包含给定的注解，如果包含则返回该注解，不包含则返回null * 通过此方法可以获得给定bean的注解，并获取其注解中对应的值 */ @Nullable &lt;A extends Annotation&gt; A findAnnotationOnBean(String beanName, Class&lt;A&gt; annotationType) throws NoSuchBeanDefinitionException; @Nullable &lt;A extends Annotation&gt; A findAnnotationOnBean( String beanName, Class&lt;A&gt; annotationType, boolean allowFactoryBeanInit) throws NoSuchBeanDefinitionException;&#125; AutowireCapableBeanFactory 增加了自动装配Bean属性依赖的能力，提供多种自动装配策略和细粒度控制装配过程的方法。 ConfigurableBeanFactory 扩展自HierarchicalBeanFactory和SingletonBeanRegistry，提供了对BeanFactory的配置能力。扩展的内容：类加载器、类型转换、属性编辑器、BeanPostProcessor、作用域、bean定义，处理bean依赖关系等。 ConfigurableListableBeanFactory 继承自上面3个接口，接口的集大成者。提供了分析和修改BeanDefinition以及预实例化单例的工具。、 抽象类 AbstractBeanFactory：实现了ConfigurableBeanFactory大部分功能； AbstractAutowireCapableBeanFactory：继承自AbatractBeanFactory，并额外实现了AutowireCapableBeanFactory。 实现类 DefaultListableBeanFactory继承自AbstractAutowireCapableBeanFactory，并实现了ConfigurableListableBeanFactory和BeanDefinitionRegistry。 ApplicationContextApplicationContext扩展自ResourcePatternResolver、ListableBeanFactory、HierarchicalBeanFactory、MessageSource、ApplicationEventPublisher、EnvironmentCapable。ApplicationContext接口作为BeanFactory的派生，因而提供BeanFactory所有的功能，而且ApplicationContext还在功能上做了扩展。有如下功能： 发现、定义、维护、管理Bean； 可以向外界暴露当前程序所运行的环境信息； 国际化； 事件发布/订阅； 解析资源； 层次化上下文； 一些context可能通过持有AutowireCapableBeanFactory来支持自动装载能力。 ApplicationContext体系 ConfigurableApplicationContext 提供了配置ApplicationContext的功能。配置和生命周期方法被封装在这里，以避免它们对使用ApplicationContext的代码可见。 AbstractApplicationContext 模板模式：模板方法定义为 final，可以避免被子类重写。需要子类重写的方法定义为 abstract，可以强迫子类去实现。 refresh：加载或刷新配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public abstract class AbstractApplicationContext extends DefaultResourceLoader implements ConfigurableApplicationContext &#123; @Override public void refresh() throws BeansException, IllegalStateException &#123; // 加锁：registerShutdownHook，close，refresh互斥 synchronized (this.startupShutdownMonitor) &#123; StartupStep contextRefresh = this.applicationStartup.start(\"spring.context.refresh\"); // Prepare this context for refreshing. // 设置Spring容器的启动时间，撤销关闭状态，开启活跃状态 prepareRefresh(); // Tell the subclass to refresh the internal bean factory. // 通知子类刷新内部factory并返回 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. //准备BeanFactory以在此上下文中使用 prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. // 后置处理BeanFactory postProcessBeanFactory(beanFactory); StartupStep beanPostProcess = this.applicationStartup.start(\"spring.context.beans.post-process\"); // Invoke factory processors registered as beans in the context. //调用所有注册在context中的BeanFactoryPostProcessor invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. //注册BeanPostProcessor后置处理器，这里的后置处理器在下方实例化Bean方法中会用到 registerBeanPostProcessors(beanFactory); beanPostProcess.end(); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. //初始化特定子类上下文中的其它特殊bean（模板模式） onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. //实例化BeanFactory中已经被注册但是未实例化的所有Bean(懒加载的不需要实例化)。即生成环境所需要的Bean。 finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); contextRefresh.end(); &#125; &#125; &#125;&#125; ClassPathXmlApplicationContext 通过读取类路径下的xml配置文件来构建ApplicationContext 12345678910111213public class ClassPathXmlApplicationContext extends AbstractXmlApplicationContext &#123; public ClassPathXmlApplicationContext( String[] configLocations, boolean refresh, @Nullable ApplicationContext parent) throws BeansException &#123; super(parent); setConfigLocations(configLocations); if (refresh) &#123; //refresh也会调用子类的相关方法以模板模式来完成整个刷新功能，刷新的最终结果就是beanfactory中被塞了一些context需要的bean和postprocessor，并且beandefinition被冻结，所有singleton被预实例化。 refresh(); &#125; &#125;&#125; XmlBeanDefinitionReder 自动扫描和自动装配的功能都是context命名空间提供的。 1234567891011121314public class XmlBeanDefinitionReader extends AbstractBeanDefinitionReader &#123;//获取一个命名空间处理器解析器public NamespaceHandlerResolver getNamespaceHandlerResolver() &#123; if (this.namespaceHandlerResolver == null) &#123; this.namespaceHandlerResolver = createDefaultNamespaceHandlerResolver(); &#125; return this.namespaceHandlerResolver; &#125; protected NamespaceHandlerResolver createDefaultNamespaceHandlerResolver() &#123; ClassLoader cl = (getResourceLoader() != null ? getResourceLoader().getClassLoader() : getBeanClassLoader()); return new DefaultNamespaceHandlerResolver(cl); //META-INF/spring.handlers &#125;&#125; ContextNamespaceHandler 12345678910111213public class ContextNamespaceHandler extends NamespaceHandlerSupport &#123; @Override public void init() &#123; registerBeanDefinitionParser(\"property-placeholder\", new PropertyPlaceholderBeanDefinitionParser()); registerBeanDefinitionParser(\"property-override\", new PropertyOverrideBeanDefinitionParser()); registerBeanDefinitionParser(\"annotation-config\", new AnnotationConfigBeanDefinitionParser()); registerBeanDefinitionParser(\"component-scan\", new ComponentScanBeanDefinitionParser()); //组件自动扫描 registerBeanDefinitionParser(\"load-time-weaver\", new LoadTimeWeaverBeanDefinitionParser()); registerBeanDefinitionParser(\"spring-configured\", new SpringConfiguredBeanDefinitionParser()); registerBeanDefinitionParser(\"mbean-export\", new MBeanExportBeanDefinitionParser()); registerBeanDefinitionParser(\"mbean-server\", new MBeanServerBeanDefinitionParser()); &#125;&#125; NamespaceHandlerSupport 策略模式+模板模式 NamespaceHolder继承了NamespaceHolderSupport，可以通过注册BeanDefinitionParser和BeanDefinitionDecorator来解析对应的XML节点，执行需要的功能，注册该功能返回的BeanDefinition。 ComponentScanBeanDefinitionParser 使用component-scan时的自动装配功能并不是BeanFactory实现的，它通过为BeanDefinition设置默认值，完全禁用了每一个Bean的自动装配功能，取而代之的是，它通过BeanPostProcessor拦截Bean的生命周期实现仅对具有@Autowired注解的属性注入。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class ClassPathBeanDefinitionScanner extends ClassPathScanningCandidateComponentProvider &#123; /** * 给basePackage包下所有@Component标注的类创建BeanDefinition，应用默认BeanDefinition设置， * 并且检查Bean类上的一些注解，对BeanDefinition进行对应设置，如@Lazy注解。 * 最后，它会将BeanDefinition包装成BeanDefinitionHolder，并返回Holder列表。 * 而且，该方法已经向BeanFactoryRegistry注册了BeanDefinition。 */ protected Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Assert.notEmpty(basePackages, \"At least one base package must be specified\"); //初始化BeanDefinition集合 Set&lt;BeanDefinitionHolder&gt; beanDefinitions = new LinkedHashSet&lt;&gt;(); //扫描所有basePackage for (String basePackage : basePackages) &#123; //候选组件 //使用自带的默认AnnotationTypeFilter来过滤basePackage指定的所有包下具有@Component注解的组件 Set&lt;BeanDefinition&gt; candidates = findCandidateComponents(basePackage); for (BeanDefinition candidate : candidates) &#123; //设置作用域 ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(candidate); candidate.setScope(scopeMetadata.getScopeName()); //bean名称 String beanName = this.beanNameGenerator.generateBeanName(candidate, this.registry); if (candidate instanceof AbstractBeanDefinition) &#123; postProcessBeanDefinition((AbstractBeanDefinition) candidate, beanName); &#125; if (candidate instanceof AnnotatedBeanDefinition) &#123; //检测是否具有@Lazy、@Primary、@DependsOn、@Role、@Description注解，如果有就向BeanDefinition中设置对应的配置 AnnotationConfigUtils.processCommonDefinitionAnnotations((AnnotatedBeanDefinition) candidate); &#125; if (checkCandidate(beanName, candidate)) &#123; //将BeanDefinition以及生成的beanName包装到BeanDefinitionHolder中 //BeanDefinitionHolder中封装了BeanDefinition和它的BeanName、别名等信息 BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(candidate, beanName); definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); beanDefinitions.add(definitionHolder); //向registry中注册BeanDefinition registerBeanDefinition(definitionHolder, this.registry); &#125; &#125; &#125; return beanDefinitions; &#125;&#125; ClassPathBeanDefinitionScanner 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 给basePackage包下所有@Component标注的类创建BeanDefinition，应用默认BeanDefinition设置， * 并且检查Bean类上的一些注解，对BeanDefinition进行对应设置，如@Lazy注解。 * 最后，它会将BeanDefinition包装成BeanDefinitionHolder，并返回Holder列表。 * 而且，该方法已经向BeanFactoryRegistry注册了BeanDefinition。 */protected Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Assert.notEmpty(basePackages, \"At least one base package must be specified\"); //初始化BeanDefinition集合 Set&lt;BeanDefinitionHolder&gt; beanDefinitions = new LinkedHashSet&lt;&gt;(); //扫描所有basePackage for (String basePackage : basePackages) &#123; //候选组件 //使用自带的默认AnnotationTypeFilter来过滤basePackage指定的所有包下具有@Component注解的组件 Set&lt;BeanDefinition&gt; candidates = findCandidateComponents(basePackage); for (BeanDefinition candidate : candidates) &#123; //设置作用域 ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(candidate); candidate.setScope(scopeMetadata.getScopeName()); //bean名称 String beanName = this.beanNameGenerator.generateBeanName(candidate, this.registry); if (candidate instanceof AbstractBeanDefinition) &#123; postProcessBeanDefinition((AbstractBeanDefinition) candidate, beanName); &#125; if (candidate instanceof AnnotatedBeanDefinition) &#123; //检测是否具有@Lazy、@Primary、@DependsOn、@Role、@Description注解，如果有就向BeanDefinition中设置对应的配置 AnnotationConfigUtils.processCommonDefinitionAnnotations((AnnotatedBeanDefinition) candidate); &#125; if (checkCandidate(beanName, candidate)) &#123; //将BeanDefinition以及生成的beanName包装到BeanDefinitionHolder中 //BeanDefinitionHolder中封装了BeanDefinition和它的BeanName、别名等信息 BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(candidate, beanName); definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); beanDefinitions.add(definitionHolder); //向registry中注册BeanDefinition registerBeanDefinition(definitionHolder, this.registry); &#125; &#125; &#125; return beanDefinitions;&#125; 总结 ClassPathXmlApplicationContext默认并不带自动装配功能，它只通过XML中定义的结构去解析并生成BeanDefinition； 自动装配功能由context命名空间提供； context命名空间的组件扫描功能为每一个扫描到的组件都定义了它BeanDefinition的默认值，并在关闭了它的自动装配功能； context命名空间通过向ApplicationContext中添加BeanPostProcessorBean来向底层BeanFactory中注册一批BeanPostProcessor，这其中包括用于实现自动装配的AutowiredAnnotationBeanPostProcessor； AutowiredAnnotationBeanPostProcessor是一个实例化感知BeanPostProcessor，它重写postProcessProperties方法和postProcessPropertyValues方法，拦截了Bean初始化阶段的属性设置post-processing。它对于每一个包含@Value、@Autowired、@Inject注解的属性，通过调用BeanFactory中的resolveDependency来获得需要被注入的Bean或值，并注入进被拦截的Bean中。 ApplicationContext与BeanFactory的区别 BeanFactory是IOC底层容器，面向Spring；ApplicationContext是具备应用特性的BeanFactory超集，面向使用 Spring框架的开发者。额外提供AOP、资源管理、国际化、事件、Environment抽象等功能； BeanFactroy采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化；ApplicationContext则在容器启动时，就实例化Bean，常驻在容器内，也可以为Bean配置Lazy-init=true来让Bean延迟实例化。 BeanDefinitionSpring容器启动的过程中，会将Bean解析成Spring内部的BeanDefinition结构。 不管是是通过xml配置文件的\\&lt;Bean&gt;标签，还是通过注解配置的@Bean，它最终都会被解析成一个Bean定义信息（对象），最后我们的Bean工厂就会根据这份Bean的定义信息，对bean进行实例化、初始化等等操作 BeanDefinitionSpring中Bean的定义，包含Bean的元数据信息：对应的bean的全限定类名，属性值，构造方法参数值和继承自它的类的更多信息。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement &#123; String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; int ROLE_APPLICATION = 0; int ROLE_SUPPORT = 1; int ROLE_INFRASTRUCTURE = 2; /** * 如果父类存在，设置这个bean定义的父定义的名称。 */ void setParentName(@Nullable String parentName); @Nullable String getParentName(); /** * 指定此bean定义的bean类名称。 */ void setBeanClassName(@Nullable String beanClassName); @Nullable String getBeanClassName(); /** * 覆盖此bean的作用域，指定一个新的作用域。 */ void setScope(@Nullable String scope); @Nullable String getScope(); /** *设置这个bean是否应该被延迟初始化。如果false，那么这个bean将在启动时由bean工厂实例化， * 懒加载 &lt;bean lazy-init=\"true/false\"&gt; */ void setLazyInit(boolean lazyInit); boolean isLazyInit(); /** * 设置这个bean依赖被初始化的bean的名字。 bean工厂将保证这些bean首先被初始化。 * &lt;bean depends-on=\"\"&gt; */ void setDependsOn(@Nullable String... dependsOn); @Nullable String[] getDependsOn(); /** * 配置Bean是否是自动装配的候选者 默认为 true XML中的 &lt;bean autowire-candidate=\"\"&gt; */ void setAutowireCandidate(boolean autowireCandidate); boolean isAutowireCandidate(); /** * 如果找到了多个可注入bean，那么则选择被Primary标记的bean/获取当前 Bean 是否为首选的 Bean XML中的 &lt;bean primary=\"\"&gt; */ void setPrimary(boolean primary); boolean isPrimary(); /** * 配置FactoryBean的名字 XML中的&lt;bean factory-bean=\"\"&gt; */ void setFactoryBeanName(@Nullable String factoryBeanName); @Nullable String getFactoryBeanName(); /** * 配置/获取 FactoryMethod 的名字，可以是某个实例的方法（和factoryBean配合使用） * 也可以是静态方法。 XML 中的&lt;bean factory-method=\"\"&gt; */ void setFactoryMethodName(@Nullable String factoryMethodName); @Nullable String getFactoryMethodName(); /** * 返回该 Bean 构造方法的参数值 */ ConstructorArgumentValues getConstructorArgumentValues(); default boolean hasConstructorArgumentValues() &#123; return !getConstructorArgumentValues().isEmpty(); &#125; /** * 获取普通属性的集合 */ MutablePropertyValues getPropertyValues(); default boolean hasPropertyValues() &#123; return !getPropertyValues().isEmpty(); &#125; /** * 配置 Bean 的初始化方法 XML中的&lt;bean init-method=\"\"&gt; */ void setInitMethodName(@Nullable String initMethodName); @Nullable String getInitMethodName(); /** * 配置 Bean 的销毁方法 XML中的&lt;bean destroy-method=\"\"&gt; */ void setDestroyMethodName(@Nullable String destroyMethodName); @Nullable String getDestroyMethodName(); void setRole(int role); int getRole(); void setDescription(@Nullable String description); /** * Return a human-readable description of this bean definition. */ @Nullable String getDescription(); // Read-only attributes /** * Return a resolvable type for this bean definition, * based on the bean class or other specific metadata. * &lt;p&gt;This is typically fully resolved on a runtime-merged bean definition * but not necessarily on a configuration-time definition instance. * @return the resolvable type (potentially &#123;@link ResolvableType#NONE&#125;) * @since 5.2 * @see ConfigurableBeanFactory#getMergedBeanDefinition * * 用来解析一个Bean对应的类型上的各种信息，比如泛型 */ ResolvableType getResolvableType(); /** * Return whether this a &lt;b&gt;Singleton&lt;/b&gt;, with a single, shared instance * returned on all calls. * @see #SCOPE_SINGLETON */ boolean isSingleton(); /** * Return whether this a &lt;b&gt;Prototype&lt;/b&gt;, with an independent instance * returned for each call. * @since 3.0 * @see #SCOPE_PROTOTYPE */ boolean isPrototype(); /** * Return whether this bean is \"abstract\", that is, not meant to be instantiated. */ boolean isAbstract(); /** * Return a description of the resource that this bean definition * came from (for the purpose of showing context in case of errors). */ @Nullable String getResourceDescription(); /** * Return the originating BeanDefinition, or &#123;@code null&#125; if none. * &lt;p&gt;Allows for retrieving the decorated bean definition, if any. * &lt;p&gt;Note that this method returns the immediate originator. Iterate through the * originator chain to find the original BeanDefinition as defined by the user. * 如果当前 BeanDefinition 是一个代理对象，那么该方法可以用来返回原始的 BeanDefinition */ @Nullable BeanDefinition getOriginatingBeanDefinition();&#125; BeanDefinition体系 AttributeAccessor 为其它任意类中获取或设置元数据提供了一个通用的规范。具体实现是AttributeAccessorSupport，采用LinkedHashMap进行存储。 BeanMetadataDefinition 提供了getSource()，返回Bean的来源。 抽象实现AbstractBeanDefinition 抽象类，BeanDefinition定义了一系列的get/set方法，并没有提供对应的属性。AbstractBeanDefinition中将所有的属性定义出来了。 RootBeanDefinition Spring去创建Bean就是基于RootBeanDefinition，在AbstractBeanDefinition的基础上扩展一些之外的功能。 ChildBeanDefinition 不可以单独存在，必须依赖一个父BeanDefinition。构造ChildBeanDefinition时，其构造方法传入父BeanDefinition的名称或通过setParentName设置父BeanDefinition名称。它可以从父类继承方法参数、属性值，并可以重写父类的方法，同时也可增加新的属性或者方法。 Spring2.5之后，逐渐被GenericBeanDefinition替代。 GenericBeanDefinition GenericBeanDefinition 可以动态设置父 Bean，同时兼具 RootBeanDefinition 和 ChildBeanDefinition 的功能。通过注解配置的bean以及XML配置的BeanDefinition类型都是GenericBeanDefinition。 子接口AnnotatedBeanDefinition 表示注解类型 BeanDefinition，拥有获取注解元数据和方法元数据的能力。 123456789101112public interface AnnotatedBeanDefinition extends BeanDefinition &#123; /** * 主要对 Bean 的注解信息进行操作，如：获取当前 Bean 标注的所有注解、判断是否包含指定注解。 */ AnnotationMetadata getMetadata(); /** * 方法的元数据类。提供获取方法名称、此方法所属类的全类名、是否是抽象方法、判断是否是静态方法、判断是否是final方法等。 */ @Nullable MethodMetadata getFactoryMethodMetadata();&#125; AnnotatedGenericBeanDefinition xxxBeanDefinitionReader加载的BeanDefinition 继承自GenericBeanDefinition并实现了AnnotatedBeanDefinition接口，使用了 @Configuration 注解标记配置类会解析为 AnnotatedGenericBeanDefinition。 ScannedGenericBeanDefinition ClassPathBeanDefinitionScanner加载的BeanDefinition 实现了 AnnotatedBeanDefinition也继承了 GenericBeanDefinition。使用了@Component、@Service、@Controller等注解类会解析为 ScannedGenericBeanDefinition。 ConfigurationClassBeanDefinition ConfigurationClassBeanDefinitionReader的一个使用静态内部类，负责将使用了@Bean的方法转换为ConfigurationClassBeanDefinition 类。 Spring初始化时，会用GenericBeanDefinition或是ConfigurationClassBeanDefinition（用@Bean注解注释的类）存储用户自定义的Bean，在初始化Bean时，又会将其转换为RootBeanDefinition。 扩展BeanDefinitionBuilder 快速创建一个BeanDefinition，可以进行链式的方法调用，建造者模式，默认是GenericBeanDefinition。 12345678910111213141516171819public class Demo &#123; public static void main(String[] args) &#123; AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder .genericBeanDefinition(User.class) //Spring5.0后提供的，可以自己书写函数，在里面做任意事情 .applyCustomizers( //bd是个AbstractBeanDefinition (bd -&gt; &#123; MutablePropertyValues propertyValues = bd.getPropertyValues(); propertyValues.add(\"name\", \"安卓\"); &#125;) ).getBeanDefinition(); System.out.println(beanDefinition); //Generic bean: class [cn.BeanDefinition.RootBeanDefinition.User]; scope=; abstract=false; lazyInit=null; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null DefaultListableBeanFactory beanFactory = new DefaultListableBeanFactory(); beanFactory.registerBeanDefinition(\"user\", beanDefinition); User user = (User) beanFactory.getBean(\"user\"); System.out.println(user); //User(name=安卓, age=null) &#125; BeanDefinitionReader 读取Spring配置文件中的内容，将其转换为IOC容器内的数据结构BeanDefinnition。 123456789101112131415161718192021222324252627282930313233public interface BeanDefinitionReader &#123; /** * 得到Bean定义的register */ BeanDefinitionRegistry getRegistry(); /** * 返回用于加载资源的 ResourceLoader（可以为null） */ @Nullable ResourceLoader getResourceLoader(); /** * 加载Bean的类加载器 */ @Nullable ClassLoader getBeanClassLoader(); /** * 生成Bean名称的名字生成器（若没有指定名称的话，会调用它生成） */ BeanNameGenerator getBeanNameGenerator(); /** * 核心方法，加载bean定义进来，然后注册到上面的register里面去 */ int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException; int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException; int loadBeanDefinitions(String location) throws BeanDefinitionStoreException; int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException;&#125; AbstractBeanDefinitionReader 实现了EnvironmentCapable，提供了获取/设置环境的方法。实现一些基本方法，通过策略模式核心方法loadBeanDefinitions交给子类实现。 XmlBeanDefinitionReader 继承了 AbstractBeanDefinitionReader 所有的方法，同时也扩展了很多新的方法，主要用于读取 XML 文件中定义的 bean。 1234567891011121314151617181920public class Demo &#123; public static void main(String[] args) &#123; // 1.获取资源 ClassPathResource resource = new ClassPathResource(\"spring-config.xml\"); // 2.获取BeanFactory DefaultListableBeanFactory beanFactory = new DefaultListableBeanFactory(); // 3.根据新建的BeanFactory创建一个BeanDefinitionReader对象，用于资源解析 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // 4.装载资源 // 4.1 资源定位 // 4.2 装载：BeanDefinitionReader读取、解析Resource资源，也就是将用户定义的Bean表示成IOC容器的内部数据结构：BeanDefinition。 // 4.3 注册：向IOC容器注册在第二步解析好的BeanDefinition，这个过程是通过BeanDefinitionRegistry接口来实现的。 // 本质上是将解析得到的BeanDefinition注入到一个HashMap容器中，IOC容器就是通过这个HashMap来维护这些BeanDefinition的。 // 注意：此过程并没有完成依赖注入，依赖注册是发生在应用第一次调用getBean()向容器索要Bean时。(可以通过设置预处理，即对某个Bean设置lazyInit属性) beanDefinitionReader.loadBeanDefinitions(resource); Person person = beanFactory.getBean(Person.class); System.out.println(person); &#125;&#125; PropertiesBeanDefinitionReader 从properties文件或者Map里加载Bean。 GroovyBeanDefinitionReader 可以读取 Groovy 语言定义的 Bean。 @Bean是@Configuration，统一由ConfigurationClassParser#parse()处理； @Component这种组件统一由解析@ComponentScan的处理器的ComponentScanAnnotationParser(借助ClassPathBeanDefinitionScanner)。 Aware有时需要感知到容器的存在，获取容器相关信息，Spring提供许多实现Aware接口的类，这些类主要是为了辅助Spring访问容器中的数据。 常见的Aware接口 类名 作用 BeanNameAware 获得容器中Bean名称 BeanClassLoaderAware 获得类加载器 BeanFactoryAware 获得Dean创建工厂 EnvironmentAware 获得环境变量 EmbeddedValueResolverAware 获得Spring容器加载的Properties文件属性值 ResourceLoaderAware 获得资源加载器 ApplicationEventPublishAware 获得应用事件发布器 MessageSourceAware 获得文本信息 ApplicationContextAware 获得当前应用上下文 Aware调用链AbstractAutowireCapableBeanFactory#createBean() ===&gt; AbstractAutowireCapableBeanFactory#doCreateBean() ===&gt;AbstractAutowireCapableBeanFactory#initializeBean() ===&gt; AbstractAutowireCapableBeanFactory#invokeAwareMethods() ===&gt; AbstractAutowireCapableBeanFactory#applyBeanPostProcessorsBeforeInitialization() ===&gt; ApplicationContextAwareProcessor#postProcessBeforeInitialization() ===&gt; ApplicationContextAwareProcessor#invokeAwareInterfaces() 自定义Aware 新建接口继承Aware 123public interface MyBeanNameAware extends Aware &#123; void setBeanName(String name);&#125; 新建一个BeanPostProcessor实现类，重新postProcessBeforeInitialization处理自定义的Aware 1234567891011@Componentpublic class MyBeanNameAwareProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; if (bean instanceof MyBeanNameAware)&#123; ((MyBeanNameAware) bean).setBeanName(beanName); &#125; return bean; &#125;&#125; 使用 1234567891011@Componentpublic class UserService implements MyBeanNameAware &#123; private String beanName; public void test()&#123; System.out.println(\"test\" + \"___\" + this.beanName); &#125; @Override public void setBeanName(String name) &#123; this.beanName = name; &#125;&#125; BeanFactoryPostProcessorBeanFactory的后置处理器，用来对BeanFactory进行加工。 修改已经注册的beanDefinition的元信息。发生在Spring启动时，实例化单例Bean之前。 12345678910@Componentpublic class MyBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; @Override public void postProcessBeanFactory( ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; BeanDefinition userService = beanFactory.getBeanDefinition(\"userService\"); userService.setScope(BeanDefinition.SCOPE_PROTOTYPE); //... &#125;&#125; 在ApplicationContext内部有一个核心的DefaultListableBeanFactory，实现了ConfigurableListableBeanFactory和BeanDefinitionRegistry接口，所以ApplicationContext和DefaultListableBeanFactory是可以注册BeanDefinition的，但是ConfigurableListableBeanFactory是不能注册BeanDefinition的，只能获取BeanDefinition，然后做修改。 BeanDefinitionRegistryPostProcessorBeanDefinitionRegistryPostProcessor继承自BeanFactoryPostProcessor，所以BeanDefinitionRegistryPostProcessor不仅提供postProcessBeanFactory()，方法还扩展了支持注册BeanDefinition的方法postProcessBeanDefinitionRegistry() 123456789101112131415@Componentpublic class MyBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanFactory( ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; &#125; @Override public void postProcessBeanDefinitionRegistry( BeanDefinitionRegistry registry) throws BeansException &#123; registry.registerBeanDefinition(\"1\", BeanDefinitionBuilder.genericBeanDefinition(OrderService.class) .getBeanDefinition()); &#125;&#125; BeanFactoryPostProcessor调用链AbstractApplicationContext#refresh() ===&gt; AbstractApplicationContext#invokeBeanFactoryPostProcessors() ===&gt; PostProcessorRegistrationDelegate#invokeBeanFactoryPostProcessors() ===&gt; PostProcessorRegistrationDelegate#invokeBeanFactoryPostProcessors() ===&gt; BeanFactoryPostProcessor#postProcessBeanFactory() InitializingBean、DisposableBeanSpring 开放了扩展接口，允许自定义 bean 的初始化和销毁方法。即当 Spring 容器在 bean 进行到相应的生命周期阶段时，会自动调用我们自定义的初始化和销毁方法。这两个扩展接口即是 InitializingBean 和 DisposableBean 。 InitializingBean123public interface InitializingBean &#123; void afterPropertiesSet() throws Exception;&#125; InitializingBean与SmartInitialzingSingleton的区别 SmartInitialzing只作用于单例Bean，InitialzingBean无此要求； SmartInitialzing是在所有的非懒加载单例Bean初始化完成后调用，InitialzingBean在每个Bean初始化后调用。 InitialzingBean调用源码：org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#invokeInitMethods 123456789101112131415161718192021222324252627282930313233343536protected void invokeInitMethods(String beanName, Object bean, @Nullable RootBeanDefinition mbd) throws Throwable &#123; //是否实现了initializingBean接口 boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean &amp;&amp; (mbd == null || !mbd.hasAnyExternallyManagedInitMethod(\"afterPropertiesSet\"))) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(\"Invoking afterPropertiesSet() on bean with name '\" + beanName + \"'\"); &#125; if (System.getSecurityManager() != null) &#123; try &#123; AccessController.doPrivileged((PrivilegedExceptionAction&lt;Object&gt;) () -&gt; &#123; ((InitializingBean) bean).afterPropertiesSet(); return null; &#125;, getAccessControlContext()); &#125; catch (PrivilegedActionException pae) &#123; throw pae.getException(); &#125; &#125; else &#123; //执行自定义的afterPropertiesSet() ((InitializingBean) bean).afterPropertiesSet(); &#125; &#125; if (mbd != null &amp;&amp; bean.getClass() != NullBean.class) &#123; String initMethodName = mbd.getInitMethodName(); if (StringUtils.hasLength(initMethodName) &amp;&amp; !(isInitializingBean &amp;&amp; \"afterPropertiesSet\".equals(initMethodName)) &amp;&amp; !mbd.hasAnyExternallyManagedInitMethod(initMethodName)) &#123; //反射调用指定的initMethod invokeCustomInitMethod(beanName, bean, mbd); &#125; &#125; &#125; SmartInitialzingBean调用源码：org.springframework.beans.factory.support.DefaultListableBeanFactory#preInstantiateSingletons 123456789101112131415161718192021222324@Overridepublic void preInstantiateSingletons() throws BeansException &#123; // 触发所有非延迟加载单例 Bean 的初始化 ...... //触发所有 SmartInitializingSingleton 后初始化的回调 for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; StartupStep smartInitialize = this.getApplicationStartup().start(\"spring.beans.smart-initialize\") .tag(\"beanName\", beanName); SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; smartInitialize.end(); &#125; &#125;&#125; DisposableBeanDisposableBean 接口为单例 bean提供了在容器销毁 bean 时的处理方法，在 bean 被销毁时都会执行重写了的destroy()。 123public interface DisposableBean &#123; void destroy() throws Exception;&#125; 调用源码：org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#destroyBean 123456789101112131415protected void destroyBean(String beanName, @Nullable DisposableBean bean) &#123; ...... if (bean != null) &#123; try &#123; //执行destroy bean.destroy(); &#125; catch (Throwable ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(\"Destruction of bean with name '\" + beanName + \"' threw an exception\", ex); &#125; &#125; &#125; ......&#125; Spring FactoriesSpring Factories是一种类似于Java SPI的机制，在resources/META-INF/spring.factories文件中配置接口的实现类名称(接口名称=实现类)，然后在程序中读取该配置文件并实例化，是spring-boot-starter-xxx的实现基础。 为什么需要spring.factories？如果想要被Spring容器管理的Bean的路径不在spring-boot项目的扫描路径下，该怎么办呢？ 解耦容器注入，帮助外部包(独立于spring-boot项目)注册Bean到spring-boot项目中。 实现原理org.springframework.core.io.support.SpringFactoriesLoader 12345678910111213141516171819202122232425262728//按接口获取spring.fatories文件中的实现类的实例public static &lt;T&gt; List&lt;T&gt; loadFactories(Class&lt;T&gt; factoryType, @Nullable ClassLoader classLoader) &#123; Assert.notNull(factoryType, \"'factoryType' must not be null\"); ClassLoader classLoaderToUse = classLoader; if (classLoaderToUse == null) &#123; classLoaderToUse = SpringFactoriesLoader.class.getClassLoader(); &#125; List&lt;String&gt; factoryImplementationNames = loadFactoryNames(factoryType, classLoaderToUse); if (logger.isTraceEnabled()) &#123; logger.trace(\"Loaded [\" + factoryType.getName() + \"] names: \" + factoryImplementationNames); &#125; List&lt;T&gt; result = new ArrayList&lt;&gt;(factoryImplementationNames.size()); for (String factoryImplementationName : factoryImplementationNames) &#123; result.add(instantiateFactory(factoryImplementationName, factoryType, classLoaderToUse)); &#125; AnnotationAwareOrderComparator.sort(result); return result;&#125;//按接口获取spring.factories文件中的实现类全称public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryType, @Nullable ClassLoader classLoader) &#123; ClassLoader classLoaderToUse = classLoader; if (classLoaderToUse == null) &#123; classLoaderToUse = SpringFactoriesLoader.class.getClassLoader(); &#125; String factoryTypeName = factoryType.getName(); return loadSpringFactories(classLoaderToUse).getOrDefault(factoryTypeName, Collections.emptyList());&#125; 调用链路：org.springframework.boot.SpringApplication#SpringApplication() ==&gt;org.springframework.boot.SpringApplication#getSpringFactoriesInstances() ==&gt;org.springframework.core.io.support.SpringFactoriesLoader#loadFactoryNames ==&gt;org.springframework.core.io.support.SpringFactoriesLoader#loadSpringFactories 例子从数据库加载配置： 12345678910public class DatabaseEnvironmentPostProcessor implements EnvironmentPostProcessor &#123; @Override public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application) &#123; if (environment.getPropertySources().contains(\"databasePropertySources\")) &#123; return; &#125; ... &#125;&#125; spring.factories： 12org.springframework.boot.env.EnvironmentPostProcessor=\\com.web.env.DatabaseEnvironmentPostProcessor 参考：https://juejin.cn/post/7113166186849763358","categories":[{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/categories/Spring/"}],"tags":[{"name":"源码","slug":"源码","permalink":"https://imokkkk.github.io/tags/源码/"},{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/tags/Spring/"}]},{"title":"一个Java对象的大小怎么计算？","slug":"一个Java对象的大小怎么计算？","date":"2022-06-27T02:20:37.207Z","updated":"2022-06-27T02:20:42.064Z","comments":true,"path":"javaobjectsize/","link":"","permalink":"https://imokkkk.github.io/javaobjectsize/","excerpt":"一个Java对象的大小怎么计算？对象头+实例数据+对其填充 对象头 MarkWord：用于存储对象运行时的数据，如hashCode、锁状态标志、GC分代年龄等。64位操作系统占8字节，32位操作系统占4字节。 对象元数据指针(kclass)：对象指向类的指针，虚拟机通过这个指针来确定这个对象是哪一个类的实例。开启压缩指针占4字节，未开启占8字节。 数组长度：只有数组对象才有，占4字节。","text":"一个Java对象的大小怎么计算？对象头+实例数据+对其填充 对象头 MarkWord：用于存储对象运行时的数据，如hashCode、锁状态标志、GC分代年龄等。64位操作系统占8字节，32位操作系统占4字节。 对象元数据指针(kclass)：对象指向类的指针，虚拟机通过这个指针来确定这个对象是哪一个类的实例。开启压缩指针占4字节，未开启占8字节。 数组长度：只有数组对象才有，占4字节。 实例数据 实例数据可能包括两种： 8种基本数据类型 类型 占用空间 boolean 1 byte 1 char 2 short 2 int 4 float 4 long 8 double 8 Reference 4 对象 对其填充(Padding) Java对象内存起始地址必须为8的整数倍，即Java对象大小必须为8的整数倍。当对象头+实例数据大小不为8的整数倍时，将会使用对其填充。 如64位，开启压缩指针的虚拟机上new Object()实际大小： Mark Word(8B) + kclass(4B) + Padding(4B) = 16B 例子 1234// Mark Word(8B) + kclass(4B) + i(4B) = 16Bpublic class Object1&#123; private int i;&#125; 12345678// Mark Word(8B) + kclass(4B) + i(4B) + j(4B) + s(4B) + b(1B) + c(2B) + Padding(5B) = 32Bpublic class ObjectSizeTest &#123; private int i; private int j; private String s; private boolean b; private char c;&#125; 1234567891011// HotSpot创建的对象的字段会默认按从长到短，引用排最后排序：long/double --&gt; int/float --&gt; short/char --&gt; byte/boolean --&gt;Reference//Mark Word(8B) + kclass(4B) + i1(4B) + i2(4B) + b1(1B) + b2(1B) + b3(1B) + Padding(1B) + s(4B) + obj(4B) = 32Bpublic class ObjectSizeTest &#123; private String s; // 4 private int i1; // 4 private byte b1; // 1 private byte b2; // 1 private int i2;// 4 private Object obj; //4 private byte b3; // 1&#125;","categories":[{"name":"JVM","slug":"JVM","permalink":"https://imokkkk.github.io/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://imokkkk.github.io/tags/JVM/"}]},{"title":"Docker+Kubernetes","slug":"Docker+K8s","date":"2022-06-20T06:22:51.218Z","updated":"2022-09-09T08:54:26.016Z","comments":true,"path":"docker/","link":"","permalink":"https://imokkkk.github.io/docker/","excerpt":"Docker原理 Chroot 改变进程及其子进程外显的根目录，chroot设置根目录的程序，不能够对这个之外的文件进行访问，外部也不能读取、更改它的内容。 NameSpace 对内核资源进行隔离，容器中的进程只可以访问当前容器命名空间的资源(进程ID，主机名、用户、文件名等)。 Cgroup 限制隔离进程的资源使用(CPU、内存、磁盘、网络等)。","text":"Docker原理 Chroot 改变进程及其子进程外显的根目录，chroot设置根目录的程序，不能够对这个之外的文件进行访问，外部也不能读取、更改它的内容。 NameSpace 对内核资源进行隔离，容器中的进程只可以访问当前容器命名空间的资源(进程ID，主机名、用户、文件名等)。 Cgroup 限制隔离进程的资源使用(CPU、内存、磁盘、网络等)。 核心概念容器的5种状态初建、运行、停止、暂停、删除。 Docker客户端与服务端交互： Docker命令 REST API SDK Docker服务端dockerd(Docker Daemon)：负责响应和处理来自客户端的请求，然后将其转化为具体操作。 containerd：通过containerd-shim启动并管理runC runC：用来运行容器的轻量级工具 docker attach/execdocker attach：给一个正在运行的容器分配了stdin、stdout、stderr，所有终端窗口同(显示一样的内容；同时阻塞)，使用exit退出时原容器也会退出，可以使用Ctrl+C。 docker exec：相当于fork了一个和容器相同NameSpace的进程。 镜像与容器的区别docker image inspect &lt; image &gt; 查看镜像分层 镜像中的层都是只读的，容器在镜像上多了一个读写层。 docker commit 可以基于运行时的容器生成新的镜像，将读写层数据写到新的镜像中。 所有写入或修改运行时容器的数据都存储在读写层，当容器停止运行时，读写层的数据也会同时删除掉。 写时复制：因为镜像层的数据是只读的，所以我们运行同一个镜像的多个容器副本时，可以共享镜像存储层，节省磁盘。 文件系统AUFS 将镜像层(只读)组织成多个目录(branch)，运行时容器文件作为一层容器层(读写)覆盖在镜像层之上，最后通过联合挂载技术呈现。 联合挂载：同一个挂载点同时挂载多个文件系统，将挂载点的源目录与挂载内容进行整合，使得最终可见的文件系统将会包含整合之后的各层文件和目录。 OverlayFS 将镜像层(只读)称为lowerdir，容器层(读写)称为upperdir，最后联合挂载为mergedir，在容器上的改动，在upperdir、mergeddir中都会体现。 Docker网络docker0网桥 启动Docker Daemon进程之后，会多出一个docker0的网卡，连接容器网段和宿主机网段，IP: 172.17.0.1/16(可修改)。 iptablesDocker会在宿主机系统上增加一些iptables规则，用来管理Docker容器与容器之间及外界的通信。 外界访问Docker是通过iptables做DNAT实现的，DNAT将SNAT中的Source转成Destiantion，表示目的地址转换。 网络模式 bridge模式 默认网络模式，所有Docker容器连接到docker0网桥或自定义网桥上，所有的Docker容器处于同一个子网。 host模式 Docker容器和宿主机使用同一个网络协议栈(同一个network namespace)，和宿主机共享网卡、IP、端口等信息。性能更好，但没做网络隔离。 overlay模式 在多个Docker Daemon之间创建一个分布式网络，允许容器之间加密通讯，需要处理容器之间和主机之间的网络包。 macvlan模式 网卡虚拟化技术，可以在同一个物理网卡上虚拟出多个网卡，通过不同的Mac地址在数据链路层进行网络数据的转发。 none模式 除了自带的IO网卡(loopback 127.0.0.1)外没有其它任何网卡、IP等信息，需要自己添加网卡。 容器间网络通信：link将新创建出来的Docker容器与已有的容器之间创建一个安全通道来做数据交互。 123docker run -d -e MYSQL_ROOT_PASSWORD=123456 -p 3306:3306 --name mysql mysql:latestdocker run --name busybox --link mysql:mysql busybox:latest #--link name or id:alias，第一个参数是目标容器的名字或者ID，第二个alias相当于在busybox Docker容器中访问MySQL Docker容器的host。 host文件修改 容器/etc/hosts文件中多了一条172.17.0.2 mysql e47e603ffb17记录 环境变量 通过link建立连接之后，会在接收容器额外设置一些环境变量保存源容器的信息 123456789101112131415/ # env | grep MYSQLMYSQL_PORT_33060_TCP=tcp://172.17.0.2:33060MYSQL_ENV_MYSQL_MAJOR=8.0MYSQL_PORT_3306_TCP_ADDR=172.17.0.2MYSQL_ENV_MYSQL_ROOT_PASSWORD=123456MYSQL_ENV_GOSU_VERSION=1.7MYSQL_PORT_3306_TCP_PORT=3306MYSQL_PORT_3306_TCP_PROTO=tcpMYSQL_PORT_33060_TCP_ADDR=172.17.0.2MYSQL_PORT=tcp://172.17.0.2:3306MYSQL_PORT_3306_TCP=tcp://172.17.0.2:3306MYSQL_PORT_33060_TCP_PORT=33060MYSQL_ENV_MYSQL_VERSION=8.0.19-1debian9MYSQL_PORT_33060_TCP_PROTO=tcpMYSQL_NAME=/busybox/mysql Docker数据存储模式VolumesVolume会把文件存储在宿主机的指定位置(Linux：/var/lib/docker/volumes/)，这些文件只能由Docker进程修改。 1234[root@VM-4-5-centos ~]# docker volume create my-volmy-vol[root@VM-4-5-centos ~]# docker run -d --name test -v my-vol:/data nginx:latest[root@VM-4-5-centos ~]# docker run -d --name test --mount type=volume,src=myvol,target=/data nginx:latest -v/–volume：volume的名字(匿名可忽略):容器内的挂载点 type=volume,src=&lt; VOLUME-NAME&gt;,dst=&lt; CONTAINER-PATH&gt;,volume-driver=local bind mounts可以将文件存储到宿主机的任意位置，而且别的应用程序也可以修改。 tmpfs只支持linux，只会将数据存储在宿主机的内存中，并不会落盘，容器停止，数据就会被清除。 1docker run -d -it --name test --mount type=tmpfs,target=/data,tmpfs-mode=1770 nginx:latest tmpfs-size：指定tmpfs的大小，默认不受限制，单位byte tmpfs-mode：Linux系统的文件模式，默认1777，任何用户都可以写 为什么说Docker是单进程模型？不管是在容器还是虚拟机中都有一个1号进程(容器：entrypoint启动进程；虚拟机：systemd进程)，然后其它进程都是1号进程的子进程，或子进程的子进程等等。 回收子进程资源 父进程通过系统调用wait()或waitpid()来等待子进程结束，从而回收子进程的资源； 异步：子进程结束之后向父进程发送SIGCCHILD信号，基于此父进程注册一个SIGCHILD信号的处理函数进行子进程的资源回收。 僵尸进程子进程先于父进程退出，并且父进程没有对子进程残留的资源进行回收，就会产生僵尸进程。 孤儿进程父进程先于子进程退出，产生孤儿进程。虚拟机会将孤儿进程的父进程设置为1号进程即systemd进程，然后由systemd对孤儿进程的资源进行回收，而容器的1号进程为entrypoint启动进程，无法处理。 如何避免？ Kubernetes：可以将多个容器编排到一个pod里，共享同一个Linux NameSpace，本质是k8s实例化出一个pause镜像，其它容器加入这个镜像实例化出的NameSpace实现NameSpace共享。 pod中的1号进程变成了pause，其它容器的entrypoint变成了1号进程的子进程。 Kubernetes概述用于容器化应用的容器化应用、自动化部署、扩缩容、管理。 核心功能 服务发现和负载均衡 自动装箱 自动修复 存储编排 应用自动发布与回滚 配置管理 批任务执行 弹性伸缩 核心概念 Pod Kubernetes中的最小调度单元，一个Pod可以由多个容器组成，同一个Pod内容器之间没有进行隔离。容器和Pod间的关系，类似进程组和进程。 Deployment 启动多个应用实例时(启动多个相同的Pod)，Deployment可以理解为一组Pod的管理器。 Service 服务发现和负载均衡是通过Service来做的，Service可以关联一组Pod，Service对象创建成功之后会映射到一个域名和固定的IP，只需要访问这种情况就可以通过这个固定的IP就可以访问后端的Pod中运行的应用了。 Configmap 创建和管理不同环境的配置，将配置和应用解耦。 NameSpace 资源的逻辑空间，包括鉴权、资源管理等。Kubernetes中的每个资源，如Pod、Depolement、Service等都有一个NameSpace属主，不同NameSpace的资源不能跨NameSpace访问，NameSpace内的资源要求命令唯一性。 资源隔离：NameSpace同一个namesace内的资源必须保证名字唯一，不同namespace内的资源可以名字相同。(资源：Pod、Deployment、Service等) Kubernetes自动创建的3个NameSpace： default； kube-system； kube-public 为Namespace设置资源配额 resource quota资源文件 123456789101112131415161718apiVersion: v1kind: Listitems:- apiVersion: v1 kind: ResourceQuota metadata: name: quota spec: hard: configmaps: \"20\" limits.cpu: \"4\" limits.memory: 10Gi persistentvolumeclaims: \"10\" pods: \"30\" requests.storage: 10Ti secrets: \"60\" services: \"40\" services.loadbalancers: \"50 1kybectl apply -f resourcequota.yaml -n myNamespace 为Nmaespace设置资源限制 为了避免单个容器或者pod用光node上的所有可用资源。 LimitRange资源对象 12345678910111213141516171819apiVersion: v1kind: LimitRangemetadata: name: limit-mem-cpu-per-containerspec: limits: - max: cpu: \"800m\" memory: \"1Gi\" - min: cpu: \"100m\" memory: \"99Mi\" default: cpu: \"700m\" memory: \"900Mi\" defaultRequest: cpu: \"110m\" memory: \"111Mi\" type: Container 1kubectl apply -f limitrange.yaml -n myNamespace 查看pod的资源情况 1k8s kubectl describe pods myapp-pod -n myNamespace Pod 解决task co-scheduling的问题 Pod中的容器被自动安排到集群中的同一个物理或虚拟机上，并可以一起进行调度。 管理 资源共享和通信 Pod 内的容器之间没有进行资源隔离，可以进行资源共享和通信。 Pod的生命周期 挂起(Pending)：Pod已被Kubernetes接受，但有一个或多个容器镜像尚未创建。(调度Pod的时间、通过网络下载镜像的时间) 运行中(Running)：该Pod已经绑定到了一个节点上，Pod中所有容器都已被创建，至少有一个容器正在运行或处于启动或重启状态。 成功(Succeeded)：Pod中的所有容器都被成功终止了，并且不会再重启。 失败(Failed)：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。 未知(Unknown)：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。 常用参数 12345678910apiVersion: v1 #表示 api 对象的版本（比如 Pod 就是一种 api 对象）kind: Pod #表明 api 对象类型，Pod 对应的 kind 是 Podmetadata: #包含一些元信息，比如 name 、 labels 等 name: myapp-pod labels: app: myappspec: #定义了 Pod 的一些描述信息，重要信息都是在 spec 这里进行描述的，比如： containers: #containers：Pod 中运行的容器的镜像列表，可以包含多个 - name: myapp-container image: busybox:1.28 command affinity nodeAffinity：描述了 Pod 和 Node 之间的调度关系，比如把 Pod 调度到含有指定的标签的 Node 节点上； requiredDuringSchedulingIgnoredDuringExecution：Pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。 preferredDuringSchedulingIgnoredDuringExecution：优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。 podAffinity：描述了 Pod 之间的调度关系，比如将某两种 Pod 调度到指定的节点上； podAntiAffinity：和 podAffinity 正好相反，这个叫反亲和，比如让某两种 Pod 不要调度到同一个节点。 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myappspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: &lt;label-name&gt; operator: In values: - &lt;value&gt; containers: - name: myapp-container image: busybox:latest command: ['sh', '-c', 'echo Hello Kubernetes! &amp;&amp; sleep 3600'] hostAliases Init Container Pod可以包含多个容器，需要某个或某几个容器优于其他容器启动。 init container总是运行到完成； 每个init container运行完成，下一个容器才会运行，如果有多个init container，则按顺序启动； 如果init container运行失败，Kubernetes会不断重启Pod，直到init container成功为止，除非restartPolicy值为Never。 配置管理ConfigMap和Secret ConfigMap：普通配置存储； Secret：密文存储，如数据库密码等。 ConfigMap不同环境对应不同的配置，将镜像和配置分离。 创建 通过目录 通过文件 通过环境变量文件 直接编写configmap 123456789apiVersion: v1data: ui.properties: | color.good=purple color.bad=yellow allow.textmode=truekind: ConfigMapmetadata: name: ui-config-file 12#编写configmap对象的yaml文件kubectl apply -f .\\ui.yaml 使用 环境变量 通过volume挂载 使用限制 ConfigMap是通过etcd存储的(实际上kubernetes中所有API对象都是存储在etcd中的)，etcd的value默认限制1M大小； 更新问题。环境变量：需要重启pod；volume挂载方式：10s左右更新。 SecretSecret对象类型一般用于保存敏感信息，如密码、令牌和ssh key等。 创建 kubectl命令行； 直接编写Secret 1234567apiVersion: v1kind: Secretdata: username: YWRtaW4= password: MWYyZDFlMmU2N2Rmmetadata: name: user-password-1 1kubectl apply -f .\\db-user-pass.yaml 使用 环境变量 通过volume挂载 容器化守护进程DaemonSetDaemonSet：控制Daemon Pod Daemon Pod： 这个Pod运行在Kubernetes集群中的每一个节点(Node)上； 每个节点只能运行一个Daemon Pod实例； 当有新的节点(Node)加入到Kubernetets集群时，Daemon Pod会自动被拉起； 当有旧节点被删除时，其上运行的Daemon Pod也被删除。 应用场景 存储守护进程，如glusted或者ceph； 日志收集进程，如fluentd或者filebeat； 监控守护进程，如Prometheus的node-exporter； 创建 12345678910111213141516171819202122232425apiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd-app labels: k8s-app: fluentdspec: selector: matchLabels: name: fluentd-app template: metadata: labels: name: fluentd-app spec: containers: - name: fluentd image: fluentd resources: limits: cpu: 100m memory: 200Mi requests: cpu: 100m memory: 200Mi 1234567PS C:\\Users\\Desktop\\KubernetesConfigFiles\\daemonSet&gt; kubectl get podNAME READY STATUS RESTARTS AGEfluentd-app-ts8p2 1/1 Running 1 (14m ago) 19hPS C:\\Users\\Desktop\\KubernetesConfigFiles\\daemonSet&gt; kubectl get daemonsetNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEfluentd-app 1 1 1 1 1 &lt;none&gt; 19h#DESIRED：期望运行的Pod实例的个数；CURRENT：当前运行的Pod实例的个数；READY：状态ready的Pod实例的个数 只在某些指定的节点上面运行Pod 指定.spec.template.spec.nodeSelector，DaemonSet将在能够与Node Selector匹配的节点上创建Pod； 指定.spec.template.spec.affinity，DaemonSet 将在能够与 nodeAffinity 匹配的节点上创建 Pod。 nodeSelector示例： 给某个节点打上特定的标签 12PS C:\\Users\\Desktop\\KubernetesConfigFiles\\daemonSet&gt; kubectl label nodes docker-desktop daemonset-label=masternode/docker-desktop labeled 在 DaemonSet 的 yaml 文件中指定 nodeSelector 1234567891011121314......spec: selector: matchLabels: name: fluentd-app template: metadata: labels: name: fluentd-app spec: nodeSelector: daemonset-label: master containers:...... nodeAffinity示例 4种策略： requiredDuringSchedulingIgnoredDuringExecution：表示 Pod 必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试； requiredDuringSchedulingRequiredDuringExecution：类似 requiredDuringSchedulingIgnoredDuringExecution ，不过如果节点标签发生了变化，不再满足pod指定的条件，则重新选择符合要求的节点； preferredDuringSchedulingIgnoredDuringExecution：表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署； preferredDuringSchedulingIgnoredDuringExecution：表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中RequiredDuringExecution表示如果后面节点标签发生了变化，满足了条件，则重新调度到满足条件的节点。 DaemonSet工作原理 控制器会不断检查状态是不是预期的，如果不是预期的就会做一些处理。 DaemonSet Controller：遍历所有的Node，状态有如下情况： 没有指定的Pod在运行，需要创建； 有指定的Pod在运行，可能有多个 需要将多余的Pod删除； 正好有一个指定的Pod在运行。 ReplicationController和ReplicaSetRepicationControllerRepicationController确保集群内任何时刻都有指定的Pod副本处于运行状态，监控管理集群内跨多个节点的多个Pod。 工作原理 ReplicationController会监听Kubernetes集群内运行的Pod个数，如果多于指定副本数，则删除多余的Pod；如果少于指定的副本数，则启动缺少的Pod。即使只需要一个Pod，也建议使用ReplicationController来创建Pod。 ReplicaSet下一代的Replication Controller，支持新的基于集合的选择器(Deployment，DaemonSet也支持)。 动态伸缩 修改ReplicaSet中的.spec.replicas字段来实现运行的Pod的个数伸缩限制，更新完yaml文件后直接kubectl apply重新应用一下即可； 结合HorizontalPodAutoscaler(水平Pod缩放器)，HPA可以基于CPU利用率(或其它)自动伸缩replication controller、deployment和replica set中的Pod数量。 1234567891011apiVersion: autoscaling/v1 #hpa的版本kind: HorizontalPodAutoscalermetadata: name: frontend-scaler #hpa的名字spec: scaleTargetRef: kind: ReplicaSet #指定目标类型为ReplicaSet name: frontend #指定目标ReplicaSet为frontend minReplicas: 3 #最少pod副本为3 maxReplicas: 10 #最大pod副本为10 targetCPUUtilizationPercentage: 50 #设定cpu百分比，超过50%就增加pod数量 Deployment类似ReplicaSet，更新和扩缩容操作上更加友好。 创建 Deployment 创建的过程会首先创建一个 ReplicaSet，然后由 ReplicaSet 间接创建Pod。Deployment 负责管理 ReplicaSet，ReplicaSet 负责管理 Pod。 更新 Deployment 的更新实际上就是两个 ReplicaSet(OldReplicatSets、NewReplicaSets) 通过StrategyType 做更新的过程。 回滚 12345678#查看历史版本PS C:\\Users\\Desktop\\KubernetesConfigFiles\\deployment&gt; kubectl rollout history deployment nginx-deployment#显示每个版本具体的行为PS C:\\Users\\Desktop\\KubernetesConfigFiles\\deployment&gt; kubectl rollout history deployment nginx-deployment --revision=2#回滚到上一次修改的版本kubectl rollout undo deployment nginx-deployment #回滚到指定的某个版本kubectl rollout undo deployment nginx-deployment --to-revisoin=1 缩放Deployment 123456#扩容kubectl scale deployment nginx-deployment --replicas=7#缩容kubectl scale deployment nginx-deployment --replicas=3#水平自动缩放 Pod，根据 cpu 使用率来进行自动缩放。kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 批处理：Job和CronJobCronJob 是Job 的定时调度。 Pod 作为 Kubernetes 的基本调度单位，Job 的执行最后也是通过 Pod 来运行的。 通过Job去初始化环境，通过CronJob去定时清理集群中的某些资源。 Job通过Job去初始化环境，通过CronJob去定时清理集群中的某些资源。 创建 通过 perl 计算 pi 的小数点后两千位数，并输出： 1234567891011121314151617181920apiVersion: batch/v1kind: Jobmetadata: name: pispec: template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] resources: limits: cpu: 100m memory: 200Mi requests: cpu: 100m memory: 200Mi restartPolicy: Never backoffLimit: 4 并行度： 非并行任务：只会启动一个Pod，Pod成功结束就表示Job正常完成了； 带有固定competion数目的并行任务：spec.completions 定义 Job 至少要完成的 Pod 数据，即 Job 的最小完成数； 具有工作队列的并行任务：通过参数 spec.parallelism 指定一个 Job 在任意时间最多可以启动运行的 Pod数。 清理 1kubectl delete jobs pi 自动清理 TTL(Time To Live)：存活时间 在Job的spec中增加参数ttlSecondsAfterFinished(Job结束之后的存活时间) 12345678apiVersion: batch/v1kind: Jobmetadata: name: pi-with-ttlspec: ttlSecondsAfterFinished: 100 template:...... CronJob类似于Linux的Crontab，不过CronJob的周期性任务是相对于整个Kubernetes集群。 创建 123456789101112131415161718apiVersion: batch/v1beta1kind: CronJobmetadata: name: cronjob-demospec: schedule: \"*/2 * * * *\" #每隔 2 分钟输出当前时间和一串文本信息 “Hello from the Kubernetes cluster” jobTemplate: spec: template: spec: containers: - name: busybox image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 清理 1kubectl delete cronjob cronjob-demo 控制器ReplicaController、Deployment等都是Kubernetes中的控制器，控制循环，不断比较资源的状态是不是期望状态，如果不是期望状态，则执行一些动作，否则什么都不做。 状态 当前状态 Kubernetes可以认为是一种server-agent架构，server可以是API Server等，agent是运行在每个Node上的kubelet。kubelet通过心跳汇报其所在节点上运行的容器状态和节点状态。 期望状态 Kubernetes中的所有API对象都会提交给API Server，然后保存到ETCD中。期望状态来源于YAML文件，也是存储在ETCD中。控制器不直接与ETCD交互，而是通过API Server来中转。ETCD提供watch机制。 Deployment 控制器从 API Server 获取到所有带有特定标签的 Pod，并统计数目，这个就是实际状态； Deployment 对象中的 Replica 字段的值是期望状态； Deployment 控制器比较这两个状态，然后根据比较结果来决定是创建新的 Pod，还是删除已有的 Pod。 StatefulSetDeployment不能用来管理有状态的应用，它认为所有代理的后端的Pod是相同的。 有状态的应用： 后端的多个Pod的角色是不同的，如Zookeeper有一个Leader节点，剩下都是Follower； 状态是存储和应用之间绑定的，如Hadoop中的HDFS会启动很多个DataNode，每个DataNode存储的数据是有区别的。 工作原理 控制的Pod拥有唯一的标识，包括顺序标识、稳定的网络标识和稳定的存储，启动周标识就是固定的，重启之后也不会发生变化。 顺序标识 对具有N个Pod副本的StatefulSet，StatefulSet会为每个Pod分配一个固定的名字，如&lt; statefulset-name&gt;-x，其中 x 介于 0 和 N-1 之间。正如我们上面看到的 Pod 的名称 web-0、web-1、web-2。 网络标识 Stateful通过Headless Service控制Pod的网络标识，网络标识的格式为$(服务名称).$(命名空间).svc.cluster.local，其中cluster.local是集群域。一旦Pod创建成功，就会得到一个匹配的DNS子域，格式为$(pod名称).$(所属服务的 DNS 域名)，其中所属服务由StatefulSet的spec中的serviceName域来设定。由于Pod的名称的固定，所以每个Pod对应的DNS子域也是固定的。 稳定存储 Kubernetes为每个VolumeClaimTemplate域创建一个PV。如果没有声明 StorageClass，就会使用默认的 StorageClass。当Pod被调度以及重新调度(比如Pod重启或Node节点挂掉)到节点上时，它的volumeMounts会挂载与其PersistentVolumeClaims相关联的PV。 当Pod或者StatefulSet被删除时，Pod之前使用的PV并不会自动删除，需要手动删除。 部署和扩缩容 对于包含 N 个 Pod 副本的 StatefulSet，当部署时，Pod 按 0，1，…，N-1 的顺序依次被创建的。 当删除 StatefulSet 时，Pod 按 N-1，…，1，0 的顺序被逆序终止的。 当缩放操作应用到某个 Pod 之前，它前面的所有 Pod 必须是 Running 和 Ready 状态，所谓前面的 Pod 的意思是序号小于当前 Pod 的序号。 在 Pod 终止之前，所有序号大于该 Pod 的 Pod 都必须完全关闭。 使用Service访问一组特定的PodService可以理解为一种访问一组特定Pod的策略。 如Pod 运行了 3 个副本，并且是无状态的。前端访问该应用程序时，不需要关心实际是调用了那个 Pod 实例。通过Service解耦，Service 与后端的多个 Pod 进行关联（通过 selector），前端只需要访问Service 即可。 创建 对80端口的TCP请求转发到标签app=nginx的并且使用TCP端口80的Pod上 ClusterIP 类型的 Service 只能在集群内部进行访问 1234567891011apiVersion: v1kind: Servicemetadata: name: nginx-servicespec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 多端口Service 设置固定IP 在 Service 的定义中通过参数 spec.clusterIP 指定自己的clusterIP。 服务发现 TODO Headless Service 拥有ClusterIP的Service，会自动做负载均衡。 指定负载均衡策略：指定ClusterIP的值为None，此时创建出来的Service则为Headless Service。这个时候做服务发现时，这个Service返回的为后端的Pod列表。 Service类型 ClusterIP：默认的 Service Type，通过集群的内部 IP 暴露服务，只能在集群内部进行访问； NodePort：通过每个 Node 上面的某个端口 （NodePort）暴露服务。通过该端口的请求会自动路由到后端的ClusterIP 服务，这个 ClusterIP 服务是自动创建的。通过 NodePort，我们可以在集群外部访问我们的服务，但是，在生产环境上面并不建议使用 NodePort； LoadBalancer：使用云厂商提供的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到NodePort 和 ClusterIP 服务； ExternalName：通过返回 CNAME 将服务映射到 externalName 字段中的内容； Ingress：严格来说，Ingress 不是一种服务类型，而是用来充当集群的服务的入口点。Ingress 可以将路由规则整合到一个资源中，然后通过同一个 IP 地址暴露多个服务。 NodePort 123456789101112apiVersion: v1kind: Servicemetadata: name: nginx-service-nodeportspec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 30001 targetPort: 80 1234$ kubectl get serviceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx-service ClusterIP 10.0.213.149 &lt;none&gt; 80/TCP 7h26mnginx-service-nodeport NodePort 10.0.8.178 &lt;none&gt; 30001:31633/TCP 1s #前面表示 ClusterIP 对应的端口，也就是 30001；后面的表示 Node 本地对应的端口，是 31633。 NodePort 类型的 Service 后端还是通过 ClusterIP 来实现。 Node本地对应的端口为一个区间随机生成的，默认为30000-32767，可以通过参数–service-node-port-range来指定。 指定NodePort： 12345678910111213apiVersion: v1kind: Servicemetadata: name: nginx-service-nodeportspec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 30001 targetPort: 80 nodePort: 30002 #保证指定的值处于参数 --service-node-port-range 指定的区间内 NodePort：Node节点本地启动的用来监听和转发请求的端口，每个节点上都会启动； Port：NodePort类型的Service自动创建的ClusterIP的端口； TargetPort：ClusterIP转发的目标端口。 所以对于NodePort类型的Service，外部的请求顺序是：NodePort -&gt; Port -&gt; TargetPort Ingress 12345678910111213apiVersion: extensions/v1beta1kind: Ingressmetadata: name: example-ingressspec: rules: - host: www.example.com #host：服务暴露的域名； http: #http：路由转发协议，可以是 http 或者 https paths: - path: /foo #path：路由 router backend: #backend：后端服务，主要包括服务名称和服务端口 serviceName: nginx-service servicePort: 80","categories":[{"name":"云原生","slug":"云原生","permalink":"https://imokkkk.github.io/categories/云原生/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://imokkkk.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://imokkkk.github.io/tags/Kubernetes/"},{"name":"云原生","slug":"云原生","permalink":"https://imokkkk.github.io/tags/云原生/"}]},{"title":"计算机网络","slug":"计算机网络","date":"2021-06-15T14:42:33.436Z","updated":"2021-07-10T14:10:32.525Z","comments":true,"path":"computernetwork/","link":"","permalink":"https://imokkkk.github.io/computernetwork/","excerpt":"1.TCP 为什么握手是 3 次、挥手是 4 次？ 如果一个Host主动向另一个Host发起连接，称为SYN，请求同步； 如果一个Host主动断开请求，称为FIN，请求完成； 如果一个Host给另一个Host发送数据，称为PSH，数据推送。","text":"1.TCP 为什么握手是 3 次、挥手是 4 次？ 如果一个Host主动向另一个Host发起连接，称为SYN，请求同步； 如果一个Host主动断开请求，称为FIN，请求完成； 如果一个Host给另一个Host发送数据，称为PSH，数据推送。 TCP是一个双工协议，建立连接的时候，连接双方都需要向对方发送SYN和ACK。握手阶段没有繁琐的工作，因此一方向另一方发起同步(SYN)之后，另一方可以将自己的ACK和SYN打包作为一条消息回复，因此是3次握手。 挥手阶段，双方都可能有未完成的工作。收到挥手请求的一方，必须马上响应(ACK)，表示收到了挥手请求。最后等所有工作结束，再发送请求中断连接(FIN)，因此是4次挥手。 2.TCP协议是如何恢复数据的顺序，TCP拆包和粘包的作用是什么？TCP拆包：将任务拆分处理，降低整体任务出错的概率，以及减小底层网络处理的压力。拆包过程中需要保证数据经过网络传输，又能恢复到原始的顺序。TCP利用发送字节数(Sequence Number)和接收字节数(Acknowledgement Number)的唯一性来确定封包之间的顺序关系(无论是Seq还是ACK，都是针对对方而言的。是对方发送的数据和对方接受的数据)。粘包是为了防止数据量过小，导致大量的传输，而将多个TCP段合并成一个发送。 3.滑动窗口和流速控制深绿色：已经收到了ACK的段 浅绿色：发送了，但是没有收到ACK的段 白色：没有发送的段 紫色：暂时不能发送的段。 有两个封包到达，标记为绿色。 滑动窗口可以向右滑动 重传 如果部分数据没能收到ACK，如段4迟迟没有收到ACK。 此时滑动窗口只能右移一个位置 如果段4重传成功(接收到ACK)，那么窗口就会继续右移。如果段4发送失败，还是没能收到ACK，那么接收方也会抛弃段5、6、7。这样从段4开始之后的数据都需要重发。 快速重传 例如段1、2、4到了，但是3没到。接收方可以发送多次3的ACK(不发段4的ACK)。如果发送方收到多个3的ACK，就会重发段3。这和超时重发不同，是一种催促机制，接收方希望催促发送方尽快补全某个TCP段。 实际操作中，每个TCP段的大小不同，限制数量会让接收方的缓冲区不好操作，因此实际操作中滑动窗口的大小单位是字节数。 总结 滑动窗口是TCP协议控制可靠性的核心。发送方将数据拆包，变成多个分组。然后将数据放入一个拥有滑动窗口的数组，依次发出，仍然遵循先入先出的顺序，但是窗口中的分组会一次性发送。窗口中序号最大的分组如果收到ACK，窗口就会发生滑动；如果有分组为收到ACK，则会滑动到该窗口。 在多次传输中，网络的平均延迟往往是相对固定的，这样TCP协议可以通过发送方和接收方协商窗口大小控制流速。 4.TCP和UDP的区别UDP UDP，目标是在传输层提供直接发送报文的能力。Datagram是数据传输的最小单位，UDP协议不会帮助拆分数据，它的目标只有一个，就是能发送报文。 UDP的可靠性仅仅就是通过Checksum保证。如果一个数据封包Datagram发生了数据损坏，UDP可以通过Checksum纠错或者修复。 UDP与TCP的区别 目的差异 TCP：提供可靠的网络传输。 UDP：提供报文交换能力基础上尽可能的简化协议。 可靠性差异 TCP：可靠，收到的数据会进行排序。 UDP：不可靠，只管发送数据包。 连接vs无连接 TCP：面向连接，会有握手的过程，传输数据必须先建立连接。 UDP：无连接协议，数据随时都可以发送，只提供发送封包的能力。 流控技术 TCP在发送缓冲区中存储数据，并在接收缓冲区中接收数据，如果接收缓冲区已满，接收方无法处理更多数据，并将其丢弃。UDP没有提供类似的能力。 传输速度 UDP协议简化，封包小，没有连接、可靠性检查等，因此单纯从速度上讲，UDP更快。 理论上，任何一个用TCP协议构造的成熟应用层协议，都可以UDP重构。想要把网络优化到极致，就会用UDP作为底层技术，然后在UDP基础上解决可靠性。 TCP场景： 远程控制(SSH) File Transfer Protocol(FTP) 邮件(SMTP、IMAP等) 点对点文件传输(微信等) UDP场景 网络游戏 音视频传输 DNS ping 直播 模糊地带 HTTP(目前以TCP为主) 文件传输 TCP最核心的价值就是提供封装好的一套解决可靠性的优秀方案。UDP最核心的价值是灵活、轻量、传输速度快。场景不同选择不同。 5.IPv4IP协议自身不能不能保证可靠性(数据无损的到达目的地)。 IP协议接收IP协议上方的Host-To-Host协议传来的数据，然后进行拆分，这个能力叫做分片。然后IP协议为每个片段增加一个IP头，组成一个IP封包。之后，IP协议调用底层的局域网(数据链路层)传送数据。最后IP协议通过寻址和路由最终将封包送达目的地。 延迟：指1bit数据从网络的一个终端传送到另一个终端需要的时间。吞吐量：单位时间内可以传输的平均数据量。如bit/s(bps)。丢包率：指发出去的封包没有到达目的地的比例。 IPv4地址4个8喂排列而成，总共可以编址43亿个地址。如103.16.3.1 寻址与路由的区别 寻址就是通过地址找设备，比如根据地址找到一个公寓。在 IPv4 协议中，寻址找到的是一个设备所在的位置。路由的本质是路径的选择，就好像知道地址，但是到了每个十字路口，还需要选择具体的路径。 所以，要做路由，就必须理解地址，也就是借助寻址的能力。找到最终的设备又要借助路由在每个节点选择数据传输的线路。因此，路由和寻址相辅相成。 6.IPv6相似点 工作原理与IPv4类似，分成切片、增加封包头、路由(寻址)几个阶段。 不同点 IPv6地址 IPv4的地址是4个8位，总共32位，如103.28.7.35，每一个是8位，用0-255的数字表示；IPv6的地址是8个16位，总共128位，如0123:4567:89ab:cdef:0123:4567:89ab:cdef，通常用16进制表示。 IPv6的寻址 全局单播 将消息从一个设备传到另一个设备，和IPv4的发送/接收数据大同小异。IPv6地址太多，因此不需要子网掩码，而是直接将IPv6的地址分区即可。 本地单播 在局域网中，实现设备到设备的通信。本地单播必须以fe80开头，类似IPv4中以127开头。 分组多播 将消息发送给多个接收者。 任意播 将消息发送给多个接收方，并选择一条最优的路径。 IPv6和IPv4的兼容 一个IPv6的客户端想访问IPv4的服务器 客户端通过DNS64服务器查询AAAA记录。(DNS64：一种解决IPv4和IPv6兼容问题的DNS服务，会把IPv4和IPv6地址同时返回) DNS64服务器返回含IPv4地址的AAAA记录。 客户端将对应的IPv4地址请求发送给一个NAT64路由器。 NAT64路由器将IPv6地址转换为IPv4地址，从而访问IPv4网络，并收集结果。 消息返回客户端。 两个IPv6网络被IPv4隔离 隧道的本质就是在两个IPv6的网络出口网关处，实现一段地址转换的程序。 Tunnel是什么？ Tunnel就是隧道，两个网络，用隧道连接，位于两个网络中的设备通信，都可以使用这个隧道。隧道是两个网络间用程序定义的一种通道。具体来说，如果两个IPv6网络被IPv4分隔开，那么两个IPv6网络的出口处(和IPv4的网关处)就可以用程序(或硬件)实现一个隧道，方便两个网络中设备的通信。 7.BIO、NIO和AIO有什么区别？BIO接口设计会直接导致当前线程阻塞。NIO的设计不会触发当前线程的阻塞。AIO为I/O提供了异步的能力，也就是将I/o的响应程序放到一个独立的时间线上去执行。但是通常AIO的提供者还会提供异步编程模型，就是实现一种对异步计算封装的数据结构，并且将异步计算同步回主线的能力。 通常情况下，这3种API都会伴随I/O多路复用。如果底层用红黑树管理注册的文件描述符和事件，可以在很小的开销内由内核将I/O消息发送给指定的线程。另外，还可以使用DMA、内存映射等方式优化I/O。 8.怎样实现RPC框架？ 调用约定和命名 远程调用一个函数，命名空间+类名+方法名 IP、端口 注册和发现 调用的时候，需要根据字符串(命名)去获取IP和端口(机器和服务) Redis hash 注册：上线一个服务时，用Redis的hash对象存储它和它对应的IP地址+端口列表。 发现：根据RPC服务的名称(命名空间+类名+方法名)查找到提供服务的IP + 端口清单并指定某个 IP + 端口(提供服务) 不足：所有RPC调用着都去Redis查询，压力较大，增加缓存，缓存和注册表之间数据不一致。 Zookeeper提供订阅，让RPC调用者订阅到服务地址的变更，及时更新自己的缓存。 多路复用 提升吞吐量：1.顺序传输 2.切片传输。 负载均衡 负载均衡可以看作发现模块的一个子组件，请求到达RPC的网关(或某个路由程序)后，发现组件会提供服务对应的所有实例(IP+端口)，然后负载均衡算法会指定其中一个响应请求。 可用性和容灾 当一个服务实例崩溃的时候，发现模块及时从注册表中删除这个服务实例。 注册表和RPC调用者之间存在不一致现象，而且注册表的更新本身也可能滞后。如确认一个服务有没有崩溃，可能需要一个心跳程序持续请求这个服务，调用到一个不存在或崩溃的服务，需要自己重新发现组件申请新的服务实例(地址+端口)。 临时访问量剧增，需要扩容的场景，上线更多的容器，并且去注册。 常见问题1.一台内存8G左右的服务器，理论上可以同时维护多少个连接？TCP连接上限受限于机器的内存，假设一个TCP连接需要占用的最小内存是8k(发送、接收缓存各4k，当然还要考虑socket描述符等)，那么最大连接数为：8 * 1024 * 1024 / 8 = 1048576个，即约为100w个TCP长连接。但是如果单机建立太多的连接，会报Cant assign requested address的异常，这是因为客户端连接服务端时，操作系统要为每个客户端分配一个端口号，端口号会更快用尽。 2.127.0.0.1，localhost，0.0.0.0 有什么不同？127.0.0.1：本地回环地址，发送到loopback上的数据会被转发到本地应用。 localhost：指代本地计算机，用于访问绑定在loopback上的服务。localhost是一个主机名，不仅可以指向IPv4的本地回环地址，也可以指向IPv6的本地回环地址[::1]。 0.0.0.0：一个特殊的目的IP地址，称作不可路由IP地址，它的用途会被特殊规定。通常情况下，当把一个服务绑定到0.0.0.0，相当于把服务绑定到任意的IP地址。比如一台服务器上有多个网卡，不同网卡连接不同的网络，如果服务绑定到0.0.0.0就可以保证服务在多个IP地址上都可以用。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://imokkkk.github.io/categories/计算机网络/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://imokkkk.github.io/tags/计算机网络/"},{"name":"HTTP","slug":"HTTP","permalink":"https://imokkkk.github.io/tags/HTTP/"},{"name":"TCP","slug":"TCP","permalink":"https://imokkkk.github.io/tags/TCP/"},{"name":"UDP","slug":"UDP","permalink":"https://imokkkk.github.io/tags/UDP/"}]},{"title":"MySQL","slug":"MySQL","date":"2021-06-06T08:06:55.156Z","updated":"2022-06-20T06:28:14.931Z","comments":true,"path":"mysql/","link":"","permalink":"https://imokkkk.github.io/mysql/","excerpt":"1.基础篇1.1 一条SQL查询语句是如何执行的？ Server层 连接器、查询缓存、分析器、优化器、执行器等。涵盖MySQL的大多数核心服务功能，以及所有的内置函数(如日期、时间、数学和加密函数等)，所有跨存储引擎的功能在这一层实现，如存储过程、触发器、视图等。","text":"1.基础篇1.1 一条SQL查询语句是如何执行的？ Server层 连接器、查询缓存、分析器、优化器、执行器等。涵盖MySQL的大多数核心服务功能，以及所有的内置函数(如日期、时间、数学和加密函数等)，所有跨存储引擎的功能在这一层实现，如存储过程、触发器、视图等。 存储引擎层 负责数据的存储和提取。支持InnoDB、MyISAM、Memory等多个存储引擎。MySQL5.5.5版本后默认使用InnoDB。 连接器 连接器负责与客户端建立连接、获取权限、维持和管理连接。 1mysql -h&#123;ip&#125; -P&#123;port&#125; -u&#123;user&#125; -p 一个用户成功建立连接后，即使使用管理员账户对该用户的权限做了修改，也不会影响已经存在的连接权限，只有新建的连接才会使用新的权限设置。 show processlist 可以查看各连接状态 “Sleep”表示空闲连接，如果客户端太长时间没动静(wait_timeout，默认8小时)，连接器自动断开。 数据库长连接指连接成功后，客户端持续有请求，则一直使用同一个连接；短连接指每次执行完几次查询后就断开连接，下次查询再重新建立。MySQL在执行过程中临时使用的内存是管理在连接对象里的，如果长连接累积下来，会导致内存占用过大。 解决办法： 定期断开长连接 MySQL5.7及以后，可以在每次执行一个比较大的操作后，执行mysql_reset_connection来将连接恢复到刚刚创建完的状态(不需要重连和权限验证)。 查询缓存 之前查询过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。但只要有对一个表的更新，那么这个表上的查询缓存就会被全部清除。MySQL8.0版本将查询缓存模块删除掉了。 MySQL提供了”按需使用”的方式，将参数query_cache_type设置为DEMAND，这样对于默认的SQL不使用查询缓存。使用SQL_CACHE显式指定： 1mysql&gt; select SQL_CACHE * from T where ID=10； 分析器 如果没有命中查询缓存，就要开真正开始执行语句。 词法分析 把”select”关键字识别出来，把字符串”T”识别为”表名T”，把字符串”ID”识别为”列ID”。 语法分析 123mysql&gt; elect * from t where ID=1;ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from t where ID=1' at line 1 优化器 在表里有多个索引的时候，决定使用哪个索引；或者一个语句有多表关联(join)的时候，决定各个表的连接顺序。 执行器 开始执行语句，先判断对这个表T是否有权限(如果命中查询缓存，会在查询缓存返回结果时做权限校验)。如果有权限，就开始执行语句。 调用InnoDB引擎接口取这个表的第一行，判断ID是否为10，如果不是则跳过，如果是则将这行存在结果集。 调用引擎接口取”下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 将结果集返回给客户端。 对于有索引的表，第一次调用的是”满足条件的第一行”这个接口，之后循环取”满足条件的下一行”接口。 1.2 日志系统：一条SQL更新语句是如何执行的12mysql&gt; create table T(ID int primary key, c int);mysql&gt; update T set c=c+1 where ID=2; 重要的日志模块：redo log WAL(Write-Ahead Logging)：先写日志，再写磁盘。当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做的。 InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB。从头开始写，写到末尾就又回到开头循环写。 write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos和checkpoint之间空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，这时不能再执行新的更新，得擦掉一些记录，把checkpoint推进一下。 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 重要的日志模块：binlog MySQL整体看包括两块：一块是Server层，它主要做的是MySQL功能层面的事情；还一块是引擎层，负责存储相关的具体事宜。redo log是InnoDB(重做日志)引擎特有的日志，而Server层也有自己的日志，称为binlog(归档日志) 为什么会有两份日志？ 因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎MyIASM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是以插件形式引入MySQL的，使用另外一套日志系统-redo log来实现crash-safe能力。 这两种日志有以下三点不同： redo log是InnoDB引擎特有的，binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log是物理日志，记录的是”在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如”给ID=2这一行的c字段加1”。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的，binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 InnoDB引擎内部执行流程 执行器先找引擎取ID=2这一行。(ID是主键，引擎直接用树搜索到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。) 执行器拿到引擎给的行数据，把这个值加1，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成commit状态，更新完成。 浅色表示在InnoDB内部进行，深色表示在执行器中执行。 1.3 事务隔离事务：保证一组数据库操作，要么全部成功，要么全部失败。 ACID(原子性、一致性、隔离性、持久性) 当数据库上有多个事务同时执行的时候，可能会出现脏读、不可重复读、幻读的问题。SQL标准的事务隔离级别包括： 读未提交：一个事务还没提交，它做的变更就能被其他事务看到。 读提交：一个事务提交之后，它做的变更才会被其他事务看到。 可重复读：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化：对于同一行记录，”写”会加”写锁”，”读”会加”读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 读未提交：V1 = 2，V2、V3 = 2 读提交：V1 = 1，V2 = 2，V3 = 2 可重复读：V1、V2 = 1，V3 = 2；之所以V2还是1，是因为：事务在执行期间看到的数据前后必须一致 串行化：在事务B执行”将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以V1、V2 = 1，V3 = 2 事务隔离的实现 数据库里会创建一个视图，访问的时候以视图的逻辑结果为准。”可重复读”，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图；”读提交”，这个视图是在每个SQL语句开始执行的时候创建的。”读未提交”，直接返回记录上的最新值，没有视图概念；”串行化”，直接用加锁的方式来避免并发访问。 MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，都可以得到前一个状态。当系统里没有比这个回滚日志更早的read-view的时候，回滚日志会被删除。 长事务意味着系统里会存在很老的事务视图，这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这会导致占用大量存储空间。 事务的启动方式 显式启动事务语句，begin或start transaction。配套的语句是commit或者rollback。 set autocommit = 0，将这个线程的自动提交关掉。意味着如果执行一个select语句，这个事务就启动了，而且不会自动提交。这个事务持续存在直到主动执行commit或者rollback，或者断开连接。 1.4 索引索引的常见模型 哈希表 以键-值(key-value)存储数据的结构。多个key经过哈希函数换算，会出现同一个值的情况，此时会拉出一个链表。因为不是有序的，所以哈希索引做区间查询的速度很慢。适用于只有等值查询的场景。 有序数组 支持范围查询，但在插入数据和删除数据时必须挪动后面所有的记录，成本太高。适用于静态存储查询，这类不会再修改的数据。 二叉搜索树 二叉搜索树的特点：父节点左子树所有节点小于父节点的值，右子树所有的节点大于父节点的值。 多叉树就是每个节点有多个儿子，儿子的大小保证从左到右递增。索引不止在内存中，还要写到磁盘上。为了尽可能少的读磁盘，就必须让查询过程访问尽量少的数据块。”N叉树”的”N”取决于数据块的大小。 InnoDB的索引模型 每一个索引在InnoDB中对应一棵B+树。 12345mysql&gt; create table T(id int primary key, k int not null, name varchar(16),index (k))engine=InnoDB; 表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。 索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也称为聚簇索引。 非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引。 基于主键索引和普通索引的查询有什么区别？ select * from T where ID = 500 即主键查询方式，则只需要搜索ID这棵B+树 select * from T where k = 5 即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次，这个过程称为回表。 索引维护 B+树为了维护索引有序性，在插入新值时需要做必要的维护。以上图为例，如果插入新的行ID值为700，则只需要在R5后面插入一个新记录。如果新插入的ID为400，需要逻辑上挪动后面的数据，空出位置。如果R5所在的数据页已满，根据B+树的算法，需要申请一个新的数据页，然后挪动部分数据过去，这个过程称为页分裂，性能会下降。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约50%。 当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。 自增主键：NOT NULL PRIMARY KEY AUTO_INCREMENT。每次插入一条新纪录，都是追加操作，都不涉及挪动记录，也不会触发叶子节点的分裂。 由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约２０个字节，而如果用整形做主键，则只需要４个字节。主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 覆盖索引 如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。 最左前缀原则 索引项是按照索引定义里面出现的字段顺序排序的。当查询所有名字是”张三”的人时，可以快速定位到ID4，然后向后遍历。 不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。 如果通过调整顺序，可以少维护一个索引，那么往往优先考虑这个顺序。 索引下推 1select * from tuser where name like '张 %' and age=10 and ismale=1; MySQL5.6之前，只能从ID3开始一个个回表，到主键索引上找出数据行，再对比字段值。 MySQL5.6之后引入索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的数据，减少回表次数。 无索引下推：回表4次 有索引下推：在(name，age)索引内部就判断了age是否等于10，对于不等于10的记录，直接判断并跳过，只需回表2次。 1.5 全局锁和表锁数据库锁设计的初衷是处理并发问题。 全局锁 全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局锁的方法，命令是Flush tables with read lock(FTWRL)。当需要让整个库处于只读状态(不止DML，还包括DDL)时，可以使用该命令。使用场景是，全库逻辑备份的时候。 InnoDB引擎的库推荐使用一致性读(single-transcation)参数，对应用会更友好，全局锁一般在数据库引擎不支持事务时使用。 为什么不使用set global readonly=true？ readonly的值会被用作其他逻辑，如判断主库备库。 异常处理机制有差别，执行FTWRL后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。readonly，如果客户端发生异常，则数据库就会一直保持readonly状态。 表级锁 MySQL里面的表级别锁有两种：一种是表锁，一种是元数据锁(meta data lock，MDL) 表锁 表锁的语法是lock tables..read/write。与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables不仅会限制别的线程的读写外，也限定了本线程接下来的操作对象。 某个线程A执行lock tables t1 read，t2 write；则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作，连写t1都不允许，自然也不能访问其他表。 元数据锁DML 不需要显示的使用，在访问一个表的时候会被自动加上，保证读写的正确性。 给一个表加字段，或者修改字段，或者加索引。需要扫描全表的数据。 如何安全地给小表加字段？ 首先解决长事务的问题，事务不提交，就会一直占着MDL锁。在alter table语句里面设定等待时间，如果在这个指定的等待时间内未拿到MDL锁，则会放弃，不阻塞后面的业务。之后再通过重试命令重复这个过程。 12ALTER TABLE tb1_name NOWAIT add column ...ALTER TABLE tb1_name WAIT N add column ... 1.6 行锁并不是所有的引擎都支持行锁，如MyISAM不支持行锁。行锁就是针对数据表中行记录的锁，比如事务A更新了一行，而这时事务B也要更新一行，则必须等事务A的操作完成后才能进行更新。 两阶段锁 事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。 在InnoDB事务中，行锁并不是不需要了就立刻释放，而是要等待事务结束时才能释放。 如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 死锁和死锁检测 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源的时候，就会导致这几个线程都进入无限等待的状态，称为死锁。 直接进入等待，直到超时。超时时间参数：innodb_lock_wait_timeout，默认50s 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其它事务得以继续执行。innodb_deadlock_detect：on，默认开启。 正常情况下使用第二种策略，每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，但如果遇到所有事务更新同一行的场景，每个新来的线程都需要判断会不会由于自己加入导致了死锁，会消耗大量的CPU资源。 如何解决热点行更新导致的性能问题？ 在确保该业务不会出现死锁的情况下，临时取消死锁检测。 控制并发度。(中间件、在MySQL里进入引擎之前排队) 将一行改成逻辑上的多行来减少锁冲突，如账户总额等于10个记录值的总和。 1.7 事务到底是隔离的还是不隔离的？当前读：更新数据都是先读后写的，而这个读，只能读当前的值。 事务的可重复读的能力是怎么实现的？可重复读的核心就是一致性读；而事务更新数据的时候，只能用当前读，如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 InnoDB的行数有多个版本，每个数据版本有自己的row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据row trx_id和一致性视图确定数据版本的可见性。对于可重复读：查询只承认在事务启动前就已经提交完成的数据；对于读提交：查询只承认在语句启动前就已经提交完成的数据； 为什么表结构不支持”可重复读”?因为表结构没有对应的行数据，也没有row trx_id，因此只能遵循当前读的逻辑 2.实践篇2.1 普通索引和唯一索引该如何选择？ 查询 select id from T where k=5 普通索引：查找到满足条件的第一个记录(5，500)后，需要查找下一个记录，直到碰到第一个不满足k=5的记录。 唯一索引：查找到第一个满足条件的记录后，就会停止检索。 两者的性能差距：InnoDB的数据是按数据页为单位来读写的，当需要读一条记录的时候，并不是将这个记录从磁盘读出来，而是以页为单位，将其整体读入内存，InnoDB中，每个数据页的大小默认是16KB。对于整形字段，一个数据页可以放近千个key，所以k=5这个记录刚好是数据页最后一个记录的几率会很低。 更新过程 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在chang buffer中，在下次查询需要访问这个数据页的时候将数据页读入内存，然后执行change buffer中与这个页有关的参数。 虽然名字叫做change buffer，实际上它是可以持久化的数据，change buffer在内存中有拷贝，也会被写入磁盘中。 将change buffer中操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭(shutdown)的过程中，也会执行merge操作。 对于唯一索引，比如插入(4,400)这个记录，需要先判断表中是否存在k=4的记录，而这必须要将数据页读入到内存才能判断。如果已经读入内存了，直接更新内存即可，没必要再使用change buffer。因此，唯一索引的更新不能使用change buffer。 innodb_change_buffer_max_size: 50 表示change buffer的大小最多只能占用buffer pool的50%。 插入(4,400)： 更新的目标页在内存中 唯一索引：找到3和5之间的位置，判断没有冲突，插入值。 普通索引：找到3和5之间的位置，插入值。 更新的目标页不在内存中 唯一索引：将数据页读入内存，判断没有冲突，插入值。 普通索引：将更新记录在change buffer。 change buffer的使用场景因为merge的时候是真正进行数据更新的时刻，而change buffer的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做merge之前，change buffer记录的变更越多，收益越大。即适合于写多读少的场景。 change buffer和redo loginsert into t(id,k) values (id1,k1)(id2,k2);假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDBbuffer pool) 中，k2 所在的数据页不在内存中。 Page1在内存中，直接更新内存。 Page2不在内存中，将更新记录在change buffer。 将上述两个动作记录在redo log。 select * from t where k in (k1, k2) 读Page1时，直接从内存返回。 读Page2时，需要把Page2从磁盘读入内存，然后应用change buffer里的操作记录，生成一个正确的版本并返回结果。 redo log主要节省的是随机写磁盘的IO消耗(转为顺序写)，而change buffer主要节省的是随机读磁盘的IO消耗。 2.2 MySQL为什么有的时候会选错索引？优化器的逻辑 扫描行数、是否使用临时表、是否排序等因素。 一个索引上不同的值越多(基数)，这个索引的区分度就越好。 MySQL是怎样得到索引的基数的？ 采样统计：InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到这个索引的基数。而数据表是会持续更新的，索引统计信息也需要同步改变。所以，当变更的数据行超过1/M的时候，会自动触发重新做一次索引统计。 innodb_stats_persistent:on：统计信息会持久存储，默认N是20，M是10。off：统计信息只存储在内存中，默认N是8，M是16。 analyze table t：重新统计索引信息 索引选择异常和处理 1select * from t where a between 1 and 1000 and b betwwen 50000 and 100000 order by b limit 1 采用force index强行选择一个索引 1select * from t force index(a) where a between 1 and 1000 and b between 50000 and 100000 order by b limit 1 修改语句，引导MySQL使用期望的索引 “order by b limit 1”改成”order by b,a limit 1”，语义逻辑相同。之前优化器选择索引b，因为它认为使用索引b可以避免排序(b本身就是索引，不需要再次进行排序，只需要遍历)，所以即使扫描行数多，也判定为代价更小。”order by b,a”，意味着使用这两个索引都需要排序，因此，扫描行数成了影响索引选择的主要条件，于是，此时优化器选择了只需要扫描1000行的索引a。 但这种优化方法并不通用。 新建一个更合适的索引，来提供给优化器做选择，或者删除掉误用的索引 2.3 怎么给字符串字段加索引？12alter table SUser add index index1(email);alter table SUser add index index2(email(6)); 由于email(6)这个索引结构中每个邮箱字段都只取前6个字节，所以占用的空间会更小，但可能会增加额外的记录扫描次数。 1select id,name,emial from SUser where email='zhangssxyz@xxx.com' 如果使用的是index1(即email整个字符串的索引结构) 从index1索引树找到满足索引值是‘zhangssxyz@xxx.com’的这条记录，取得ID2的值。 到主键上查到主键值是ID2的行，判断email的值是正确的，将这条记录加入结果集。 取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足email=’zhangssxyz@xxx.com’的条件了，循环结束。 这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。 如果使用的是index2(即emai(6)索引结构) 从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1； 到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃； 取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集； 重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。 在这个过程中，要回主键索引取 4 次数据，也就是扫描了 4 行。 使用前缀索引，定义好长度，就可以做到即节省空间，又不用额外增加太多的查询成本。 怎么确定该使用多长的前缀呢？ 区分度越高越好，意味着重复的键值越少。因此可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。 1select count(ditstinct email) as L from SUser 依次选取不同长度的前缀的值，如 1select count(ditstinct left(emial,4)) as L4,count(ditstinct left(emial,5)) as L5,count(ditstinct left(emial,6)) as L6,count(ditstinct left(emial,7)) as L7 from SUser 使用前缀索引很可能损失区分度，所以需要预先设定一个可以接收的损失比例，比如 5%。然后，在返回的 L4~L7 中，找出不小于 L * 95% 的值，假设这里 L6、L7都满足，你就可以选择前缀长度为 6。 前缀索引对覆盖索引的影响 1select id,email from SUser where email='zhangssxyz@xxx.com’; 与前面例子中的SQL语句相比，这个语句只要求返回id和email字段。 1select id,name,email from SUser where email='zhangssxyz@xxx.com’; 如果使用index1(即email整个字符串的索引结构)的话，可以利用覆盖索引，从index1查到结果后直接就返回了，不需要回表；而如果使用index2(即email(6)索引结构)的话，就不得不回到ID索引再去判断email的值。 即使将index2的定义修改为email(18)的前缀索引，这时候虽然index2已经包含了所有的信息，但InnoDB还是要回到id索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信息。 其他方式 使用倒序存储 如果存储身份证号的时候把它倒过来存，每次查询的时候，可以这样写： 1select field_list from t where id_card = reverse('input_id_card_string') 使用hash字段 可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。 1alter table t add id_card_crc int unsigned, add index(id_card_crc) 每次插入新记录的时候，都同时用crc32()这个函数得到校验码填到这个新字段。由于校验码可能会冲突，所以查询语句where部分要判断id_card的值是否精确相同。 1select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string' 使用倒序存储和使用 hash 字段的异同点 倒叙存储方式在主键索引上，不会消耗额外的存储空间，而hash字段方法需要增加一个字段。 倒序方式每次写和读的时候，都需要额外调一次reverse函数，而hash字段需要额外调一次crc32()函数，单纯从计算复杂度来看，reverse函数额外消耗的CPU资源会更小。 hash字段方式的查询性能相对更加稳定，因为crc32()算出来的值虽然有冲突的概率，但概率非常小，可以认为每次查询平均扫描行数接近1，而倒序存储方式毕竟还是前缀索引的方式，即还是会增加扫描行数。 2.4 为啥SQL语句偶尔会变”慢”？InnoDB在处理更新语句的时候，只做了写日志这一个磁盘操作(redo log)，在更新内存写完redo log后，就返回给客户端，本次更新成功。 flush：把内存中的数据写入磁盘的过程 当内存数据页跟磁盘数据页内容不一致的时候，称这个内存为”脏页”。内存数据写入到磁盘后，内存就和磁盘上的数据页上的数据页的内容就一致了，称为”干净页”。 平时执行很快的操作：写内存和日志偶尔”抖”一下：刷脏页(flush) 什么情况下会引发数据库的flush操作? InnoDB的redo log写满了。这时系统会停止所有更新操作，把checkpont往前推进，redo log留出空间可以继续写。 把checkpoint位置从CP推进到CP’，就需要将两个点之间的日志，对应的所有脏页都flush到磁盘上。 系统内存不足。当需要新的内存页，而内存不够用的时候，淘汰一些数据页，空出内存。如果淘汰的是”脏页”，就需要先将脏页写到磁盘。 MySQL认为系统”空闲”的时候就刷一点”脏页”。 MySQL正常关闭的时候，会把内存的脏页都flush到磁盘上，这样MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 对性能的影响 “redo log 写满了，要 flush 脏页”，这种情况，所有的更新都被堵塞，需要避免。 “内存不够用了，要先将脏页写到磁盘”，InnoDB使用缓冲池管理内存，缓冲池中的内存页有3种状态：还没有使用的、使用了并且是干净页、使用了并且是脏页。 当要读入的数据页没有在内存中的时候，就必须到缓冲池申请一个数据页。这时需要把最久不使用的数据页从内存中淘汰掉。如果淘汰的是一个干净页，就直接释放出来复用；如果是脏页，就必须先刷到磁盘，变成干净页后才能复用。 一个查询要淘汰的脏页个数太多。 日志写满，更新全部堵住。 InnoDB刷脏页的控制策略 参考因素：脏页比例，redo log写盘速度。 innodb_max_dirty_pages_pct：脏页比例上限，默认75% F1(M) InnoDB每次写日志都有一个序号，当前写入的序号跟checkpoint对应的序号之间的差值，假设为F2(N)(N越大，算出来的值越大)。 F1(M)和F2(N)的最大值为R，接下来引擎就可以按照innodb_io_capacity定义的能力乘以R%来控制刷脏页的数据。 合理设置innodb_io_capacity，多关注脏页比例，不要让它接近75%。 innodb_flush_neighbors，值为1时，对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷(机械硬盘时代，减少随机IO)。值为0时，只刷自己的，MySQL8.0之后，默认为0。 2.5 为什么表数据删掉一半，表文件大小不变？参数innodb_file_per_table innodb_file_per_table：MySQL5.6.6版本后默认为ON OFF：表的数据放在系统共享表空间，也就是跟数据字典放在一起。 ON：每个InnoDB表数据存储在一个.ibd为后缀的文件中。 推荐设置为ON，一个表单独存储为一个文件更容易管理，不需要这个表的时候，通过drop table命令，系统就会直接删除这个文件。如果放在共享表空间中，即使表删掉了，空间也是不会回收的。 数据删除流程 假设，我们需要删除掉R4这个记录，InnoDB引擎只会把R4这个记录标记为删除。如果之后要再插入一个ID在300和600之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。 InnoDB的数据是按页存储的，如果删除一个数据页上的所有记录，整个数据页就可以被复用了，但是，数据页的复用跟记录的复用是不同的。记录的复用，只限于符合范围条件的数据。而当整个页从B+树里面摘掉以后，可以复用到任何位置。如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另一个数据页就被标记为可复用。 如果用delete命令删除整个表的数据，所有的数据页都会被标记为可复用。但磁盘上，文件大小不变。 不止是删除数据会造成空洞，插入数据也会。 如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。 由于page A满了，申请一个新的页面page B来保存数据。页面分裂完成后，page A的末尾就留下了空洞。 重建表 新建一个与表A结构相同的表B，然后按照主键ID递增的顺序，把数据一行行地从表A里读出来再插入到表B中。 1alert table A engine=InnoDB MySQL5.6版本开始引入Online DDL(往临时表插入数据的过程中，表A中不能有更新)。 建立一个临时文件，扫描表A主键的所有数据页。 用数据页中表A的记录生成B+树，存储到临时文件。 生成临时文件的过程中，将所有对A的操作记录在一个日志文件(row log)中，对应图中state2的状态。 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据与表A相同的数据文件，对应图中state3的状态。 用临时文件替换表A的数据文件。 2.6 count(*)的实现方式MyISAM引擎把一个表的总行数存在了磁盘上，count(*)会直接返回这个数(不含where条件)。 InnoDB引擎需要把数据一行一行的从引擎里面读出来，然后累计计数。 为什么InnoDB不像MyISAM一样把数字存放起来？ 和InnoDB的事务设计有关，可重复读是它的默认的隔离级别，通过多版本并发控制(MVCC)实现。每一行记录都要判断自己是否对这个会话可见，因此count(*)，InnoDB只好把数据一行一行的读出依次判断。 MyISAM表虽然count(*)很快，但不支持事务； show table status 命令虽然返回很快，但是不准确； InnoDB直接count(*)会遍历全表，虽然结果准确，但会导致性能问题。 不同的count用法 count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。 原则： server原则要什么就什么； InnoDB只给必要的值； 现在的优化器只优化了count(*)的语义取行数。 count(主键id) InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空后，按行累加。 count(1) InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个数字”1”进去，判断是不可能为空的，按行累加。 count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。 count(字段) 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加； 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。 count(*) 不取值。count(*) 肯定不是 null，按行累加。 效率：count(字段)&lt;count(主键id)&lt;count(1)≈count(*) 2.7 “order by”是如何工作的？1234567891011CREATE TABLE `t` (`id` int(11) NOT NULL,`city` varchar(16) NOT NULL,`name` varchar(16) NOT NULL,`age` int(11) NOT NULL,`addr` varchar(128) DEFAULT NULL,PRIMARY KEY (`id`),KEY `city` (`city`)) ENGINE=InnoDB;select city,name,age from t where city='杭州' order by name limit 1000; 全字段排序 Extra：”Using filesort”表示需要排序，MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。 初始化sort_buffer，确定放入name、city、age三个字段； 从索引city找到第一个满足city=’杭州’条件的主键id，即图中的ID_X； 到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中； 从索引city取下一个记录的主键id； 重复步骤3、4直到city的值不满足查询条件为止，对应的主键id也就是图中的ID_Y； 对sort_buffer中的数据按照字段name快速排序； 按照排序结果取前1000行返回给客户端。 sort_buffer_size：MySQl为排序开辟的内存的大小。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成，否则利用磁盘临时文件辅助排序(外部排序。 外部排序一般使用归并算法，即将要排序的数据分成12份，每一份单独排序后，将这12个有序文件再合并成一个有序的大文件。如果sort_buffer_size超过了需要排序的数据量的大小，number_of_tmp_files就是0，sort_buffer_size越小，需要分成的份数越多，number_of_tmp_files的值就越大。 只对原表的数据读了一遍，剩下的操作都是在sort_buffer或临时文件中执行，但如果要查询返回的字段特别多，则需要分成很多临时文件，排序性能很差。 rowid排序 1set max_length_for_sort_data = 16 如果单行的长度超过这个值，MySQL就认为单行太大，需要换个算法(city、name、age这3个字段的定义总长度36)。新的算法放入sort_buffer的字段，只要排序的列(即name字段)和主键id。 初始化sort_buffer，确定放入两个字段，即name和id； 从索引city找到第一个满足city=’杭州’条件的主键id，即图中的ID_X； 到主键id索引取出整行，取name、id字段，存入sort_buffer中； 从索引city取下一个记录的主键id； 重复步骤3、4直到city的值不满足查询条件为止，对应的主键id也就是图中的ID_Y； 对sort_buffer中的数据按照字段name快速排序； 遍历排序结果，取前1000行，并按照id的值回到原表取出city、name和age三个字段返回给客户端。 rowid多访问了一次表t的主键索引，即步骤7。 如果内存够，就要多利用内存，尽量减少磁盘访问。 MySQL之所以要生成临时表并且在临时表上做排序操作，是因为原来的数据都是无序的。 联合索引 如在这张表上创建一个city和name的联合索引： 1alter table t add index city_user(city,name) 从索引(city,name)找到第一个满足city=’杭州’条件的主键id； 到主键id索引取出整行，取name、city、age三个字段的值，作为结果集的一部分直接返回； 从索引(city,name)取下一个记录主键id； 重复步骤2、3，直到查到第1000条记录，或者是不满足city=’杭州’时循环结束。 覆盖索引 覆盖索引上的信息足够满足查询请求，不需要再回到主键索引上取数据。 1alter table t add index city_user_age(city,name,age) 从索引(city,name,age)找到第一个满足city=’杭州’条件的记录，取出其中的city、name和age这3个字段的值，作为结果集的一部分直接返回； 从索引(city,name,age)取下一个记录，同样取出这3个字段的值，作为结果集的一部分直接返回； 重复执行步骤2，直到查到第1000条记录，或者是不满足city=’杭州’时循环结束。 2.8 为什么这些SQL语句逻辑相同，但性能差异巨大 条件字段函数操作 123456789CREATE TABLE `tradelog` (`id` int(11) NOT NULL,`tradeid` varchar(32) DEFAULT NULL,`operator` int(11) DEFAULT NULL,`t_modified` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`), KEY `t_modified` (`t_modified`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 查询所有年份中7月份的交易记录总数： 1select count(*) from tradelog where month(t_modified)=7; 下面是t_modified索引的示意图，方框上面的数字就是month()函数对应的值。 B+树的快速定位能力来源于同一层兄弟节点的有序性，对索引字段做函数操作，可能会破环索引值的有序性，因此优化器决定放弃走树搜索功能，导致全索引扫描(并不是放弃这个索引)。 1select count(*) from tradelog where(t_modified &gt;='2016-7-1' and t_modified &lt;'2016-7-1') or (t_modified &gt;='2017-7-1' and t_modified &lt;'2017-7-1') or (t_modified &gt;='2018-7-1' and t_modified &lt;'2018-7-1') 隐式类型转换 1select * from tradelog where tradeid=110717; tradeid的字段类型是varchar(32)，而输入的参数是整形，需要做类型转换。 字符串和数字作比较：将字符串转换成数字 1select * from tradelog where CAST(tradeid AS signed int) = 110717; 因此触发：对索引字段做函数操作，优化器会放弃走树搜索功能。 隐式字符编码转换 12345678910111213141516171819202122CREATE TABLE `trade_detail` (`id` int(11) NOT NULL,`tradeid` varchar(32) DEFAULT NULL,`trade step` int(11) DEFAULT NULL, /* 操作步骤 */`step_info` varchar(32) DEFAULT NULL, /* 步骤信息 */PRIMARY KEY (`id`),KEY `tradeid` (`tradeid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into tradelog values(1, 'aaaaaaaa', 1000, now());insert into tradelog values(2, 'aaaaaaab', 1000, now());insert into tradelog values(3, 'aaaaaaac', 1000, now());insert into trade_detail values(1, 'aaaaaaaa', 1, 'add');insert into trade_detail values(2, 'aaaaaaaa', 2, 'update');insert into trade_detail values(3, 'aaaaaaaa', 3, 'commit');insert into trade_detail values(4, 'aaaaaaab', 1, 'add');insert into trade_detail values(5, 'aaaaaaab', 2, 'update');insert into trade_detail values(6, 'aaaaaaab', 3, 'update again');insert into trade_detail values(7, 'aaaaaaab', 4, 'commit');insert into trade_detail values(8, 'aaaaaaac', 1, 'add');insert into trade_detail values(9, 'aaaaaaac', 2, 'update');insert into trade_detail values(10, 'aaaaaaac', 3, 'update again');insert into trade_detail values(11, 'aaaaaaac', 4, 'commit'); 查询id=2的交易的所有操作步骤信息： 1select d.* from tradelog l,trade_detail d where d.tradeid=l.tradeid and l.id=2; 优化器会先在交易记录表tradelog上查到id=2的行，这个步骤使用了主键索引，row=1表示只扫描一行； 第2行key=NULL，表示没用上交易详情表trade_detail上的tradeid索引，进行了全表扫描。 从tradelog表中取tradeid字段，再去trade_detail表里查询匹配字段。因此，tradelog称为驱动表，把trade_detail表称为被驱动表，把tradeid称为关联字段。 根据id在tradelog表中找到L2这一行； 从L2中取出tradeid字段的值； 根据tradeid值到trade_detail表中查找条件匹配的行。explain的结果里面第二行的key=NULL表示的就是，这个过程就是通过遍历主键索引的方式，一个个的判断tradeid的值是否匹配。 1select * from trade_detail where tradeid=$L2.tradeid.value $L2.tradeid.value的字符集就是utf8mb4，字符集utf8mb4是utf8的超集，两个不同类型的字符串比较时，MySQL会把utf8字符串转成utf8mb4字符集再做比较。所以需要将被驱动数据表里的字段一个一个转成utf8mb4，再与L2比较，类似于： 1select * from trade_detail where CONVERT(tradeid USING uth8mb4)=$L2.tradeid.value 再次触发了：对索引字段做函数操作，优化器会放弃走树搜索功能。 连接过程中要求在被驱动表的索引字段上加函数操作，是直接导致对被驱动表做全表扫描的原因。 1select operator from tradelog where traideid =$R4.tradeid.value; 改写为： 1select operator from tradelog where traideid =CONVERT($R4.tradeid.value USING utf8mb4); CONVERT函数加在输入参数上，可以用上被驱动表的tradeid索引。 优化： 1select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; 把trade_detail表上的tradeid字段的字符集也改成utf8mb4 1alter table trade_detail modify tradeid varchar(32) CHARACTER SET utf8mb4 default null; 修改sql语句，主动把l.tradeid转成utf8，避免了被驱动表上的字符编码转换。 1select d.* from tradelog l,trade_detail d where d.tradeid=CONVERT(l.trade.id USING utf8) on l.id=2; 2.9 为什么我只查一行的语句，也执行这么慢？123456789101112131415161718CREATE TABLE `t` (`id` int(11) NOT NULL,`c` int(11) DEFAULT NULL,PRIMARY KEY (`id`)) ENGINE=InnoDB;delimiter ;;create procedure idata()begindeclare i int;set i=1;while(i&lt;=100000)doinsert into t values(i,i);set i=i+1;end while;end;;delimiter ;call idata(); 查询长时间不返回 大概率表t被锁住了，执行show processlist命令查看当前语句状态。 等MDL锁 Wating for table metadata lock状态表示的是，现在有一个线程正在表t上请求或持有MDL写锁，把select语句堵住了。 找到谁持有MDL锁，然后把它kill掉。通过查询sys.schema_table_lockwaits这张表，直接找出造成阻塞的process id，把这个连接用kill命令断开即可。 1select blocking_pid from sys.schema_table_lock_waits; 等flush Waiting for table flush表示，有一个线程正要对表t做flush操作。 12flush table t with read lock; #只关闭表tflush table with read lock; #关闭所有打开的表 正常情况下，这两个语句执行都很快，可能的情况：有一个flush tables 命令被别的语句堵住了，然后它又堵住了select语句。 等行锁 sessionA sessionB begin;update t set c=c+1 where id=1; select * from where id=1 lock in share mode; show processlist 查询是谁占有了写锁： 123select locked_table,waiting_pid,waiting_query,blocking_trx_id,blocking_pid from sys.innodb_lock_waits;kill 4 查询慢 查看是否开启慢查询 12show variables like 'slow_query%';show variables like 'long_query_time'; 设置慢查询 1set long_query_time=0; sessionA sessionB start transaction with consistent snapshot; update t set c=c+1;//执行100万池 select * from t where id=1; select * from where id=1 lock in share mode; session B更新完100万次，生成了100万个回滚日志(undo log)。(undo log里记录的其实是”把2改成1”，”把3改成2”这样的操作逻辑) 带lock in share mode的SQL语句，是当前读，因此会直接读到1000001这个结果，所以速度很快；而select * from t where id=1是一致性读，因此需要从1000001开始，依次执行log，执行100万次以后，才将1这个结果返回。 2.10 幻读 幻读有什么问题？ 语义 数据一致性 如何解决幻读？ 产生幻读的原因：行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的”间隙”。 间隙锁(Gap Lock) 锁的就是两个值之间的空隙。 12345678910CREATE TABLE `t` (`id` int(11) NOT NULL,`c` int(11) DEFAULT NULL,`d` int(11) DEFAULT NULL,PRIMARY KEY (`id`),KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 当执行select * from t where d=5 for update的时候，就不止给数据库中已有的6个记录加上了行锁，还同时加了7个间隙锁。这样就确保了无法再插入新的记录。即在一行行扫描的过程中，不仅给行加上了行锁，还给行两边的空袭，也加上了间隙锁。 行锁分为读锁和写锁： 跟行锁有冲突关系的是”另外一个行锁”。 但跟间隙锁存在冲突关系的，是”往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。 间隙锁和行锁合称next-key lock，next-key-lock解决了幻读的问题，但可能会导致同样的语句锁住更大的范围。 间隙锁在可重复读隔离级别下才会生效，把隔离级别设为读提交，就没有间隙锁了。但同时可能会出现数据和日志不一致问题，需要把binlog格式设置为row。 2.11 next-key lock 原则1：加锁的基本单位是next-key lock，前开后闭((5,10]) 原则2：查找过程中访问到的对象才会加锁 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化成行锁 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为此 删除数据的时候尽量加limit，不仅可以控制删除数据的条数，让操作更安全，还可减小加锁的范围。 next-key lock实际上是由间隙锁加行锁实现的。 2.12 应急提高性能的方法 短连接风暴 如果使用的是短链接，业务高峰期，连接数暴增。MySQL建立连接成本很高(网络连接3次握手、登录权限判断和获得这个连接的读写权限)。 数据库处理不及时；机器负载较高时，处理现有请求耗时变长，每个连接保持的时间也更长；连接数上限：max_connections，超过这个值时，会拒绝接下来的连接请求，并报错提示”Too many connections”。 先处理掉那些占着连接但不工作的线程 max_connections，只要连着就占用一个计数位置。kill connection处理掉不需要保持的连接；设置wait_tineout参数(一个线程空闲wait_timeout这么多秒以后，就会被MySQL直接断开连接)。 优先断开事务外空闲太久的连接，如果这样还不够，再考虑断开事务内的空闲太久的连接。(从 information_schema.innodb_trx 查询事务状态) kill connection + id，一个客户端处于sleep状态，它的连接被服务端主动断开，这个客户端并不会马上知道。直到客户端在发起下一个请求的时候，才会收到报错”ERROR 2013 (HY000): Lost connection toMySQL server during query”。 减少连接过程的消耗 让数据库跳过权限验证阶段，重启数据库，并使用-skip-grant-tables参数启动，整个MySQL会跳过所有的权限验证，包括连接过程和语句执行过程，风险极高。 MySQL8.0，如果启用-skip-grant-tables参数，MySQL会默认把-skip-networking参数打开，表示数据库只能被本地的客户端连接。 慢查询性能问题 引发慢查询的可能： 索引没有设计好 SQL语句没写好 MySQL选错索引 索引没有设计好 紧急创建索引，MySQL5.6之后，创建索引支持Online DDL，直接执行alter table语句。 在备库B上执行set sql_log_bin=off，即不写binlog，然后执行alter table语句加上索引； 执行主备切换； 这时候主库是B，备库是A。在A上执行set sql_bin_log=off，然后执行alter table语句加上索引。 SQL语句没写好 MySQL5.7之后提供了query_rewrite功能，可以把输入的一种语句改写成另外一种模式。 如语句被错误地写成了 select * from t where id + 1 = 10000，可以通过下面的方式，增加一个语句改写规则。 1234mysql&gt; insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values (\"select * from t where id + 1 = ?\", \"select * from t where id = ? - 1\", \"db1\");call query_rewrite.flush_rewrite_rules();-- 这个存储过程，让插入的新规则生效，即\"查询重写\" MySQL选错索引 使用查询重写功能，给原来的语句加上force index。 通过以下方式，预先发现问题： 上线前，在测试环境，把慢查询日志(show log)打开，并把long_query_time设为0。(确保每个语句都会被记录入慢查询日志) 在测试表里插入线上数据，进行回归测试。 观察慢查询日志里每类语句的输出，特别留意Rows_examined(每次执行过程中实际扫描的记录数)字段是否与预期一致。 QPS突增问题 业务突然出现高峰或应用程序bug，导致某个语句QPS暴增。 由全新业务的bug导致的，能够确定业务方会下掉这个功能，可以从数据库直接把白名单去掉。(白名单机制) 如果使用的是单独的数据库用户，可以用管理员账号把这个用户删掉。(业务账户分离) 如果新增的功能和主体功能是部署在一起的，可以通过查询重写功能，单独把这个语句以select 1的结果返回。 副作用： 如果别的功能里也使用了这个SQL语句模板，会有误伤。 很多业务不是一个语句就完成逻辑的，单独把这个语句以select 1的结果返回，可能导致后面的业务逻辑一起失败。 2.13 MySQL是如何保证数据不丢失的WAL机制(Write-Ahead Logging 先写日志，再写磁盘)，只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启后，数据可以恢复。 binlog的写入机制 事务执行过程中，先把日志写到binglog cache，事务提交的时候，再把binlog cache写到binlog文件中。 系统给binlog cache分配了一片内存，每个线程一个，参数binlog_cache_size用于控制单个线程内binlog cache所占内存的大小。如果超过，就要暂存到磁盘。 每个线程有自己的binlog cache，但共用同一份binlog文件。write：把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，速度较快。fsync：将数据持久化到磁盘，占用磁盘的IOPS。 参数sync_binlog： sync_binlog=0时，每次提交事务都只write，不fsync； sync_binlog=1时，每次提交事务都会执行fsync； sync_binlog=N(N&gt;1)时，表示每次提交事务都write，但累积N个事务后才能fsync。 如果IO出现瓶颈，可以将sync_binlog设为一个较大的值。但实际考虑到丢失日志量的可控性，常见的是将其设置为100-1000。但是，将sync_binlog设置为N，对应的风险是：如果主机发生异常重启，会丢失最近N个事务的binlog日志。 redo log的写入机制 redo log buffer：在事务执行的过程中，生成的redo log是要先写到redo log buffer的。 存在redo log buffer中，物理上是在MySQL进程内存中，图中红色部分。 写到磁盘(write)，但是没有持久化(fsync)，物理上是在文件系统的page cache里，图中黄色部分。 持久化到磁盘，对应hard disk，图中绿色部分。 innodb_flush_log_at_trx_commit参数： 为0时，表示每次事务提交时都只是把redo log留在redo log buffer中； 为1时，表示每次事务提交时都将redo log持久化到磁盘。 为2时，表示每次事务提交时都只是把redo log写到page cache。 InnoDB有一个后台线程，每隔1秒，就会把redo log buffer中的日志，调用write写到文件系统的page cahce，然后调用fsync持久化到磁盘。 事务执行中间过程的redo log也是直接卸载redo log buffer中的，这些redo log也会被后台线程一起持久化到磁盘。所以，一个没有提交的事务的redo log，也是可能已经持久化到磁盘的。 除了后台线程的轮询操作，还有两种场景会让一个没有提交的事务的redo log写入到磁盘中： redo log buffer占用的空间即将达到innodb_log_buffer_size一半的时候，后台线程会主动写盘。由于这个事务并没有提交，所以这个写盘动作只是write，而没有调用fsync，即只留在了文件系统的page cache。 并行的事务提交时，顺带将这个事务的redo log buffer持久化到磁盘。 两阶段提交：时序上redo log先prepare，再写binlog，最后再把redo log commit。 如果把innodb_flush_log_at_trx_commit设置为1，那么redo log在prepare阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖prepare的redo log，再加上binlog来恢复的。 每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB就认为redo log在commit的时候就不需要fsync了，只会write到文件系统的page cache中就够了。 “双1”配置：sync_binlog和innodb_flush_log_at_trx_commit都设置为1，即一个事务完整提交前，需要等待两次刷盘，一次是redo log(prepare阶段)，一次是binlog。 组提交 日志逻辑序列号(LSN)：单调递增，用来对应redo log的一个个写入点。每次写入长度为length的redo log，LSN的值就会加上length。LSN也会写到InnoDB的数据页中，来确保数据页不会被多次执行重复的redo log。 三个并发事务在prepare阶段，都写完redo log buffer，持久化到磁盘的过程，对应的LSN分别是50、120和160。 trx1是第一个到达的，会被选为这组的leader； 等trx1要开始写盘的时候，这个组里已经有了三个事务，这时候LSN也变成了160。 trx1去写盘的时候，带的就是LSN=160，因此等trx1返回时，所有LSN小于等于160的redo log，都已经被持久化到磁盘； 这时候trx2和trx3就可以直接返回了。 一次组提交里面，组员越多，节约磁盘IOPS的效果越好。在并发更新场景下，第一个事务写完redo log buffer以后，接下来这个fsync越晚调用，组员可能越多，节约IOPS的效果就越好。 “拖时间” 两阶段提交： 其实”写binlog”是分成两步的： 先把binlog从binlog cache中写到磁盘上的binlog文件； 调用fsync持久化。 MySQL为了让组提交的效果更好，把redo log做fsync的时间拖到了步骤1之后，则上图变成了： 这样一来，binlog也可以组提交了，不过通常第3步执行的会很快，所以 binlog 的 write 和 fsync 间的间隔时间短，导致能集合到一起持久化的 binlog 比较少，因此 binlog 的组提交的效果通常不如 redo log 的效果那么好。 可以通过binlog_group_commit_sync_delay和binlog_group_commit_sync_no_delay_count来提升binlog组提交的效果。 binlog_group_commit_sync_delay：延迟多少微妙后才调用fsync； binlog_group_commit_sync_no_delay_count：累积多少次以后才调用fsync。 这两个条件是或的关系，只要一个满足条件就会调用fsync。因此，当 binlog_group_commit_sync_delay 设置为 0 的时候，binlog_group_commit_sync_no_delay_count 也无效了。 WAL制作得益于： redo log和binlog都是顺序写，磁盘的顺序写比随机写速度要快。 组提交机制，可以大幅度降低磁盘的IOPS消耗。 如果你的 MySQL 现在出现了性能瓶颈，而且瓶颈在 IO 上，可以通过哪些方法来提升性能呢？ 设置binlog_group_commit_sync_delay和binlog_group_commit_sync_no_delay_count参数，减少binlog的写盘次数。基于”额外的故意等待”实现，因此可能会增加语句的响应时间，但没有丢失数据的风险。 将sync_binlog设置为大于1的值(常见的为100-1000)。风险：主机断电时会丢binlog日志。 将innodb_flush_log_at_trx_commit设置为2。风险：主机断电时会丢binlog日志。(不建议设置成 0。因为把这个参数设置成 0，表示 redo log 只保存在内存中，这样的话 MySQL 本身异常重启也会丢数据，风险太大。而 redo log 写到文件系统的 page cache 的速度也是很快的，所以将这个参数设置成 2 跟设置成 0 其实性能差不多，但这样做 MySQL 异常重启时就不会丢数据了，相比之下风险会更小。) 2.14 MySQL是怎么保证主备一致的MySQL主备的基本原理 客户端的读写都直接访问节点A，而节点B是A的备库，只是将A的更新都同步过来，到本地执行。需要切换的时候，就切换成状态2。这时客户端读写访问节点B，而节点A是B的备库。 备库虽然没有被直接访问，但仍建议设为只读(readonly)模式： 有时一些运营类的查询语句会放到备库上查询，设置为只读可以防止误操作； 防止切换逻辑有bug，如切换过程中出现双写，造成主备不一致； 可以通过readonly状态，判断节点的角色。 readonly状态，怎么与主库保持同步更新？ readonly设置对超级(super)权限用户是无效的，而用于同步更新的线程，就拥有超级权限。 备库B跟主库A之间维持了一个长连接，主库A内部有一个线程，专门服务备库B的这个长连接。 一个事务日志同步的完整过程： 在备库B上通过chang master命令，设置主库A的IP、端口、用户名、密码，以及从哪个位置开始请求binlog，这个位置包含文件名和日志偏移量。 在备库上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread(主要负责与主库建立连接)和sql_thread。 主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B； 备库B拿到binlog之后，写到本地文件，称为中转日志(ready log)； sql_thread读取中转日志，解析出日志里的命令，并执行。 多线程复制方案的引入，sql_thread演化成了多个线程。 binlog的三种格式对比 1delete from t where a&gt;=4 and t_modified&lt;='2018-11-10' limit 1; statement 记录到binlog里的语句原文，在主库执行时，用的是索引a，而在备库执行时，却用了索引t_modified，可能会造成主备不一致。 row row格式的binlog里没有了SQL语句原文，而是替换成了两个event：Table_map(用于说明接下来操作的表)和Delete_rows(用于定义删除的行为) mixed 因为有些statement格式的binlog可能会导致主备不一致，所以要使用row格式。但row格式，很占空间。如一个delete语句删掉10万行数据，用statement的话就是一个SQL语句被记录到binlog中，占用几十个字节的空间。但如果是row格式，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。 mixed格式：MySQL会自己判断这条SQL语句是否可能引起主备不一致，如果有可能，就用row格式，否则就用statement格式。(线上至少应该把 binlog 的格式设置为 mixed) 恢复数据 现在越来越多场景要求把MySQL的binlog格式设为row，如恢复数据。 binlog恢复数据的标准做法：用mysqlbinlog工具解析出来，然后把解析结果整个发给MySQL执行，例如： 1mysqlbinlog master.000001 --start-position=2738 --stop-position=2942 | mysql -h127.0.0.1 -P13000 -u$user -p$pwd; 含义：将master.00001文件里面从2738字节到第2973字节中间这段内容解析出来，放到MySQL去执行。 循环复制问题 双M结构和M-S结构，其实区别只是多了一条线，即：节点A和B之间总是互为主备关系，这样在切换的时候就不用再修改主备关系。 但如果业务逻辑在A上更新了一条语句，然后再把生成的 binlog 发给节点 B，节点 B 执行完这条更新语句后也会生成 binlog。（建议把参数 log_slave_updates 设置为 on，表示备库执行 relay log 后生成 binlog）。那么，如果节点 A 同时是节点 B 的备库，相当于又把节点 B 新生成的 binlog 拿过来执行了一次，然后节点 A 和 B 间，会不断地循环执行这个更新语句，也就是循环复制了。 规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系； 一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的binlog； 每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。 日志执行流程： 从节点A更新的事务，binlog里面记的都是A的server id； 传到节点B执行一次后，节点B生成的binlog的server id也是A的server id； 再传回给节点A，A判断这个server id与自己的相同，就不会再处理这个日志。所以，死循环就断掉了。 2.15 MySQL是怎么保证高可用的？ 主备延迟 主备切换：可能是一个主动运维动作，如软件升级、主库所在机器按计划下线等，也可能是被动操作，比如主库所在机器掉电等。 同步延迟 主库A执行完成一个事务，写入binlog，这个时刻记为T1； 之后传给备库B，备库B接收完成这个binlog的时刻记为T2； 备库B执行完成这个事务，这个时刻记为T3。 主备延迟：同一个事务在备库执行完成的时间和主库执行完成的时间之间的差值，即T3-T1。 执行show slave status命令，返回结果seconds_behind_master(时间精度：秒)，用于表示当前备库延迟了多少秒。 seconds_behind_master的计算方法： 每个事务binlog都有一个时间字段，用于记录主库上写入的时间； 备库取出当前正在执行的事务的时间字段的值，计算它与当前系统时间的差值，即seconds_behind_master。 主备库机器时间设置不一致，会不会导致主备延迟的值不准确？ 不会，备库连接到主库时，会执行SELECT UNIX_TIMESTAMP()函数来获取当前主库的系统时间，如果不一致，备库在计算seconds_behind_master时会扣掉差值。 主备延迟主要来源是：备库接收完binlog和执行完成这个事务之间的时间差。 主备延迟的来源 有些部署条件下，备库所在机器的性能要比主库所在的机器性能差。 对称部署 备库压力大 忽视备库的压力控制，一些分析查询语句在备库耗费了大量的CPU资源，影响了同步速度，造成主备延迟。 一主多从，多接几个从库，分担读的压力； 通过binlog输出到外部系统，如Hadoop这类系统，让外部系统提供统计类查询的能力。 大事务 主库上必须等事务执行完成后才写入binlog，再传入给备库。如果一个主库上的语句执行10分钟，那这个事务可能导致主备延迟10分钟。 如一次性delete大量数据，大表DDL(建议使用gh-ost方案)。 备库的并行复制能力 主备切换不同的策略： 可靠性优先策略 判断备库B现在的seconds_behind_master，如果小于某个值(如5秒)继续下一步，否则重试这一步； 把主库A改成只读状态，即把readonly设为true； 判断备库B的senconds_behind_master的值，直到这个值变为0； 把备库B改成可读写状态，即把readonly设为false； 把业务请求切换到B库。 不可用时间：步骤2到步骤5，步骤3耗费时间(步骤1保证步骤3尽可能短)。 可用性优先策略 步骤4、5调整到最开始执行，及不等主备数据同步，直接把连接切到备库B，并且让备库B可以读写。 几乎没有不可用时间，可能出现数据不一致。 123456 CREATE TABLE `t` (`id` int(11) unsigned NOT NULL AUTO_INCREMENT,`c` int(11) unsigned DEFAULT NULL,PRIMARY KEY (`id`)) ENGINE=InnoDB;insert into t(c) values(1),(2),(3); 继续执行： 12insert into t(c) values(4);insert into t(c) values(5); 假设，现在主库上有大量的更新，导致主备延迟5秒。在插入一条c=4的语句后，发起了主备切换。 步骤2中，主库A执行完insert语句，插入一行数据(4,4)，之后开始进行主备切换； 步骤3中，由于主备之间有5秒延迟，所以备库B还未应用”插入c=4”这个中转日志，就开始接收”插入c=5”的命令； 步骤4中，备库B插入了一行数据(4,5)，并且把这个binlog发给主库A； 步骤 5 中，备库B执行”插入 c=4”这个中转日志，插入了一行数据(5,4)。而直接在备库B执行的”插入 c=5”这个语句，传到主库 A，就插入了一行新数据((5,5)。 主库A和备库B上出现了两行不一致的数据。 可用优先策略，设置bnlog_format=row会怎样？ row格式在记录binlog的时候，会记录新插入的行的所有字段值，最后只会有一行不一致，两边的主备同步的应用线程会报错 duplicate key error 并停止。备库 B 的 (5,4) 和主库 A 的 (5,5) 这两行数据，都不会被对方执行。 结论： 使用 row 格式的 binlog 时，数据不一致的问题更容易被发现。 主备切换的可用性优先策略会导致数据不一致。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://imokkkk.github.io/categories/数据库/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://imokkkk.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://imokkkk.github.io/tags/数据库/"}]},{"title":"代理模式(JDK Proxy与CGLIB Proxy)","slug":"代理模式","date":"2021-05-23T14:15:34.269Z","updated":"2021-05-23T14:36:45.692Z","comments":true,"path":"proxymode/","link":"","permalink":"https://imokkkk.github.io/proxymode/","excerpt":"1.静态代理 售卖香水接口 123456/* * 定义真实对象和代理对象的公共接口 */public interface SellPerfume &#123; void sellPerfume(double price);&#125; 定义香水提供商，实现接口 123456public class ChanelFactory implements SellPerfume &#123; @Override public void sellPerfume(double price) &#123; System.out.println(\"成功购买香奈儿品牌的香水，价格是：\" + price + \"元！\"); &#125;&#125;","text":"1.静态代理 售卖香水接口 123456/* * 定义真实对象和代理对象的公共接口 */public interface SellPerfume &#123; void sellPerfume(double price);&#125; 定义香水提供商，实现接口 123456public class ChanelFactory implements SellPerfume &#123; @Override public void sellPerfume(double price) &#123; System.out.println(\"成功购买香奈儿品牌的香水，价格是：\" + price + \"元！\"); &#125;&#125; 定义代理类 123456789101112131415161718192021222324public class XiaoHongSellProxy implements SellPerfume &#123; /* * 代理对象内部保存对真实目标对象的引用，控制其它对象对目标对象的访问。 */ private ChanelFactory chanelFactory; public XiaoHongSellProxy(ChanelFactory chanelFactory) &#123; this.chanelFactory = chanelFactory; &#125; @Override public void sellPerfume(double price) &#123; doSomethingBeforeSell(); chanelFactory.sellPerfume(price); doSomethingAfterSell(); &#125; private void doSomethingBeforeSell() &#123; System.out.println(\"小红代理购买香水前的额外操作...\"); &#125; private void doSomethingAfterSell() &#123; System.out.println(\"小红代理购买香水后的额外操作...\"); &#125;&#125; 购买香水 123456789101112131415161718/* * 访问者仅能通过代理对象访问真实目标对象，不可直接访问目标对象 */public class XiaoMing &#123; public static void main(String[] args) &#123; ChanelFactory chanelFactory = new ChanelFactory(); XiaoHongSellProxy xiaoHongSellProxy = new XiaoHongSellProxy(chanelFactory); /* * 代理对象并不是真正提供服务的对象，它只是替访问者访问目标对象的一个中间人， * 真正提供服务的还是目标对象，而代理对象的作用就是在目标对象提供服务之前或之后能够执行额外的逻辑 */ xiaoHongSellProxy.sellPerfume(100); &#125;&#125;小红代理购买香水前的额外操作...成功购买香奈儿品牌的香水，价格是：100.0元！小红代理购买香水后的额外操作... 代理模式的定义：给目标对象提供一个代理对象，代理对象包含该目标对象，并控制对该目标对象的访问。 代理模式的目的：通过代理对象的隔离，可以在对目标对象的访问前后增加额外的业务逻辑，实现功能增强；通过代理对象访问目标对象，可以防止系统大量的直接对目标对象进行不正确的访问。 2.静态代理与动态代理共同点：都能实现代理模式；代理对象和目标对象都需要实现一个公共接口。 不同点： 动态代理产生代理对象的时机是运行时动态生成，它没有Java源文件，直接生成字节码文件实例化代理对象，而静态代理的代理对象，在程序编译时已经写好了Java文件，直接new一个代理对象即可。 动态代理比静态代理更加稳健，对程序的可维护性和扩展性更加友好。 3.动态代理面对新的需求时，不需要修改代理对象的代码，只需要新增接口对象，在客户端调用即可完成新的代理。 3.1 JDK ProxyJDK提供的一个动态代理机制，涉及到Proxy和InvocationHandler两个核心类。 代理对象是在程序运行过程中，有代理工厂动态生成，代理对象本身不存在Java源文件。 代理工厂需要实现InvocationHanlder接口并实现invoke()方法 1234567891011121314151617181920212223242526272829public class SellProxyFactory implements InvocationHandler &#123; // 代理的真实对象 private Object object; public SellProxyFactory(Object object) &#123; this.object = object; &#125; private void doSomethingAfter() &#123; System.out.println(\"执行代理后的额外操作...\"); &#125; private void doSomethingBefore() &#123; System.out.println(\"执行代理前的额外操作...\"); &#125; /** * @param proxy 代理对象 * @param method 真正执行的方法 * @param args 调用第二个参数method时传入的参数列表值 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; doSomethingBefore(); Object invokeObject = method.invoke(object, args); doSomethingAfter(); return invokeObject; &#125;&#125; 生成代理对象需要用到Proxy类，里面的静态方法newProxyInstance可以生成任意一个代理对象 123456 /** * @param loader 加载动态代理的类的类加载器 * @param method 代理类实现的接口，可以传入多个接口 * @param args 指定代理类的调用处理程序，即调用接口中的方法时，会找到该代理工厂h，执行invoke()方法 */Proxy.newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces,InvocationHandler h) 新增红酒代理功能： 创建新的红酒供应商和售卖红酒接口 123456789/* * 红酒供应商 */public class RedWineFactory implements SellWine &#123; @Override public void SellWine(double price) &#123; System.out.println(\"成功售卖一瓶红酒，价格：\" + price + \"元\"); &#125;&#125; 123456/* * 售卖红酒接口 */public interface SellWine &#123; void SellWine(double price);&#125; 在客户端实例化一个代理对象，然后向该代理对象购买红酒 1234567891011121314151617181920212223242526public class XiaoMing &#123; public static void main(String[] args) &#123; // buyChannel(); buyRedWine(); &#125; static void buyChannel() &#123; ChanelFactory chanelFactory = new ChanelFactory(); SellProxyFactory sellProxyFactory = new SellProxyFactory(chanelFactory); SellPerfume sellPerfume = (SellPerfume)Proxy.newProxyInstance(chanelFactory.getClass().getClassLoader(), chanelFactory.getClass().getInterfaces(), sellProxyFactory); sellPerfume.sellPerfume(100); &#125; static void buyRedWine() &#123; // 实例化一个红酒供应商 RedWineFactory redWineFactory = new RedWineFactory(); // 实例化代理工厂，传入红酒供应商引用控制对其的访问 SellProxyFactory sellProxyFactory = new SellProxyFactory(redWineFactory); // 实例化代理对象 SellWine sellWine = (SellWine)Proxy.newProxyInstance(redWineFactory.getClass().getClassLoader(), redWineFactory.getClass().getInterfaces(), sellProxyFactory); // 代理售卖红酒 sellWine.SellWine(100); &#125;&#125; 总结： JDK动态代理的使用方法 代理工厂需要实现InvocationHandle接口，调用代理方法会转向执行invoke()方法。 生成代理对象需要使用Proxy对象中的newProxyInsatnce()方法，返回对象可强转成传入的其中一个接口，然后调用接口方法即可实现代理。 JDK动态代理的特点 目标对象强制需要实现一个接口，否则无法使用JDK动态代理。 3.2 CGLIBCGLIB不是JDK自带的动态代理，它需要导入第三方依赖，它是一个字节码生成类库，能够在运行时动态生成代理类对Java类和Java接口扩展。CGLIB不仅能够为Java接口做代理，而且能够为普通的Java类做代理，而JDK Proxy只能为实现了接口的Java类做代理。 CGLIB可以代理没有实现接口的Java类 导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib-nodep&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; CGLIB代理中有两个核心的类：MetondInterceptor接口和Enhancer类，前者是实现一个代理工厂的根接口，后者是创建动态代理对象的类。 定义代理工厂 1234567891011121314151617181920212223242526272829303132333435363738394041public class SellProxyFactory implements MethodInterceptor &#123; // 关联真实对象，控制真实对象的访问 private Object object; // 从代理工厂获取一个代理对象实例，等价于创建小红代理 public Object getProxyInstance(Object object) &#123; this.object = object; Enhancer enhancer = new Enhancer(); // 设置需要增强类的类加载器 enhancer.setClassLoader(object.getClass().getClassLoader()); // 设置被代理类，真实对象 enhancer.setSuperclass(object.getClass()); // 设置方法拦截器，代理工厂 enhancer.setCallback(this); // 创建代理类 return enhancer.create(); &#125; private void doSomethingBefore() &#123; System.out.println(\"执行方法前额外的操作...\"); &#125; private void doSomethingAfter() &#123; System.out.println(\"执行方法后额外的操作...\"); &#125; /** * @param o 被代理对象 * @param method 被拦截的方法 * @param objects 被拦截方法的所有入参值 * @param methodProxy 方法代理，用于调用原始的方法 */ @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; doSomethingBefore(); Object invokeSuperObject = methodProxy.invokeSuper(o, objects); doSomethingAfter(); return invokeSuperObject; &#125;&#125; 12345678public class XiaoMing &#123; public static void main(String[] args) &#123; SellProxyFactory sellProxyFactory = new SellProxyFactory(); //获取一个代理实例 ChanelFactory chanelFactoryInstance = (ChanelFactory) sellProxyFactory.getProxyInstance(new ChanelFactory()); chanelFactoryInstance.sellPerfume(100); &#125;&#125; 总结： CGLIB的使用方法 代理工厂需要实现MethodInterceptor接口，并重写方法，内部关联真实对象，控制第三者对真实对象的访问；代理工厂内部暴露getInstance(Object object)方法，用于从代理工厂中获取一个代理对象实例。 Enhancer类用于从代理工厂中实例化一个代理对象，给调用者提供代理服务。 JDK Proxy和CGLIB的对比 JDK Proxy CGLIB 代理工厂实现接口 InvocationHandler MethodInterceptor 构造代理对象给Client服务 Proxy Enhancer 不同点： CGLIB可以代理大部分类；而JDK Proxy仅能够代理实现了接口的类 CGLIB采用动态创建被代理类的子类实现方法拦截的方法，所以CGLIB不能代理被final关键字修饰的类和方法。 4.动态代理的实际运用AOP允许我们将重复的代码逻辑抽取出来形成一个单独的覆盖层，在执行代码时可以将覆盖层嵌入到原代码逻辑里面去。 如下图，method1和method2都需要在方法执行前后记录日志，AOP可以将大量重复的Log.info代码包装到额外的一层，监听方法的执行，当方法被调用时，通用的日志记录层会拦截掉该方法，在该方法调用前后记录日志，这样可以让方法专注于自己的业务逻辑而无需关注其它不必要的信息。 Spring AOP有许多功能：提供缓存、提供日志环绕、事务处理…… 事务 @Transactional 每个有关数据库的操作都有保证一个事务内的所有操作，要么全部执行成功，要么全部执行失败，传统的事务失败回滚和成功提交是使用try…catch代码块完成的 1234567891011121314SqlSession session = null;try&#123; session = getSqlSessionFactory().openSession(false); session.update(\"...\", new Object()); // 事务提交 session.commit();&#125;catch(Exception e)&#123; // 事务回滚 session.rollback(); throw e;&#125;finally&#123; // 关闭事务 session.close();&#125; 如果多个方法都需要写这一段逻辑非常冗余，所以Spring封装了一个注解@Transactional，使用它后，调用方法时会监视方法，如果方法上含有该注解，就会自动把数据库相关操作的代码包裹起来，类似上面一段代码。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/categories/Spring/"}],"tags":[{"name":"代理模式","slug":"代理模式","permalink":"https://imokkkk.github.io/tags/代理模式/"}]},{"title":"计算机网络与网络编程","slug":"计算机网络与网络编程","date":"2021-05-08T13:19:59.249Z","updated":"2023-02-03T08:12:27.098Z","comments":true,"path":"65346/","link":"","permalink":"https://imokkkk.github.io/65346/","excerpt":"1.软件结构1.1 C/S结构全称为Client/Server结构，是指客户端和服务器结构。常见程序有ＱＱ、迅雷等软件。 1.2 B/S结构全称为Browser/Server结构，是指浏览器和服务器结构。常见浏览器有谷歌、火狐等。","text":"1.软件结构1.1 C/S结构全称为Client/Server结构，是指客户端和服务器结构。常见程序有ＱＱ、迅雷等软件。 1.2 B/S结构全称为Browser/Server结构，是指浏览器和服务器结构。常见浏览器有谷歌、火狐等。 两种架构各有优势，但是无论哪种架构，都离不开网络的支持。网络编程，就是在一定的协议下，实现两台计算机的通信的程序。 2.网络通信协议 网络通信协议：通过计算机网络可以使多台计算机实现连接，位于同一个网络中的计算机在进行连接和通信时需要遵守一定的规则，这就好比在道路中行驶的汽车一定要遵守交通规则一样。在计算机网络中，这些连接和通信的规则被称为网络通信协议，它对数据的传输格式、传输速率、传输步骤等做了统一规定，通信双方必须同时遵守才能完成数据交换。 TCP/IP协议： 传输控制协议/因特网互联协议( Transmission Control Protocol/Internet Protocol)，是Internet最基本、最广泛的协议。它定义了计算机如何连入因特网，以及数据如何在它们之间传输的标准。它的内部包含一系列的用于处理数据通信的协议，并采用了4层的分层模型，每一层都呼叫它的下一层所提供的协议来完成自己的需求。上图中，TCP/IP协议中的四层分别是应用层、传输层、网络层和链路层，每层分别负责不同的通信功能。链路层：链路层是用于定义物理传输通道，通常是对某些网络连接设备的驱动协议，例如针对光纤、网线提供的驱动。网络层：网络层是整个TCP/IP协议的核心，它主要用于将传输的数据进行分组，将分组数据发送到目标计算机或者网络。运输层：主要使网络程序进行通信，在进行网络通信时，可以采用TCP协议，也可以采用UDP协议。应用层：主要负责应用程序的协议，例如HTTP协议、FTP协议等。 OSI七层模型: 应用层 负责对软件提供接口时程序能使用网络服务 表示层 应用程序和网络之间的翻译官 会话层 负责在网络中的两节点之间建立和维持通信 传输层 建立端到端之间的连接，数据的分段和重组 网络层 将网络地址翻译成对应的mac地址，指导数据包的转发 数据链路层 将网络层接收到的数据包封装为特定的数据帧，使其在不可靠的物理链路上进行可靠的数据传递 物理层 建立、维护、断开物理连接。（由底层网络定义协议） 3.UDP与TCP协议3.1 UDP协议​ UDP是无连接通信协议，即在数据传输时，数据的发送端和接收端不建立逻辑连接。由于使用UDP协议消耗资源小，通信效率高，所以通常都会用于音频、视频和普通数据的传输例如视频会议都使用UDP协议，因为这种情况即使偶尔丢失一两个数据包，也不会对接收结果产生太大影响。但是在使用UDP协议传送数据时，由于UDP的面向无连接性，不能保证数据的完整性，因此在传输重要数据时不建议使用UDP协议。UDP的交换过程如下图所示。 3.2 TCP协议​ TCP协议是面向连接的通信协议，即在传输数据前先在发送端和接收端建立逻辑连接，然后再传输数据，它提供了两台计算机之间可靠无差错的数据传输。每次连接的创建都需要经过“三次握手”。 第一次握手，客户端向服务器端发出连接请求，等待服务器确认 第二次握手，服务器端向客户端回送一个响应，通知客户端收到了连接请求 第三次握手，客户端再次向服务器端发送确认信息，确认连接。整个交互过程如下图所示由于TCP协议的面向连接特性，它可以保证传输数据的安全性，所以是一个被广泛采用的协议，例如在下载文件时，如果数据接收不完整，将会导致文件数据丢失而不能被打开，因此，下载文件时必须采用TCP协议。 4.相关面试题 TCP断开连接的四次挥手第一次挥手：客户端发送一个FIN包（seq=x），进入FIN_WAIT（结束等待）状态第二次挥手：服务器收到FIN包，发回一个ACK包(ack=x+1)，进入CLOSE_WAIT（关闭等待）状态第三次挥手：服务器关闭客户端的连接，并发送一个FIN包(seq=y)，进入LAST_ACK（最后确认）状态第四次挥手：客户端发回ACK(ack=y+1)包确认，发送完毕后，连接断开 需要三次握手的原因为了防止失效的连接请求报文突然又传送到服务器产生错误。假如不三次握手，客户端发送连接确认给服务端就立即建立连接，如果有个连接请求阻塞了很久才到服务端，而此时本来已经关闭了连接的又重新建立了连接，然而等了很久都没有数据发送，这就会白白浪费资源 Http协议与Https协议 Http协议即超文本传输协议，是一种基于TCP的应用层协议，还是一种无状态协议。用于服务器和客户端的数据传输，客户端和服务器使用URL来建立连接和传输数据。客户端发送Http请求给服务器，服务器根据请求返回Html、文本或多媒体文件给客户端 Https协议是一种安全的Http协议。Http协议是一种明文传输的协议，存在被窃听，信息篡改等安全隐患，在Http协议的基础上加入了SSL或TLS协议，实现了数据的加密传输。因为加上了加密的协议，所以Https的响应速度会比Http慢很多。并不是所有情况下都需要使用Https协议，对于隐私的，重要的信息最好用Https协议，不重要的或者可以公开的信息就没有必要用Https协议 Http请求报文和响应报文 请求报文包括请求行，请求头，空行和请求体（GET请求没有请求体） 响应报文包括状态行，响应头，空行和响应体 Http请求常见状态码 200 OK，请求成功 404 Not Found，对应的URL上不存在资源 405 Method Not Allowed，请求不被允许，即请求方式错误 500 Internal Server Error，服务器内部错误，发现严重BUG，要及时修复 GET请求与POST请求的区别 GET请求一般用于获取服务器上的资源，是幂等的。POST请求一般用于对服务器上资源进行更新，非幂等的（幂等即每次请求返回结果一样） GET请求没有请求体，请求参数跟是在URL后面的，所以使用GET请求时请求参数用户是可以直接看到的。POST请求有请求体，请求参数放在请求体，对用户是不可见的。相对来说POST请求比GET请求更安全 GET请求的参数长度有限制，这是因为URL长度有限导致的。POST请求的参数长度可以认为是无限制的 TCP 和 UDP的区别 TCP是一种面向连接的可靠传输协议，UDP是面向无连接的不可靠传输协议 TCP支持报文传输，还支持字节流的传输。而UDP协议只支持传输报文 TCP数据报格式比较复杂，传输过程数据不容易丢失和出错，而UDP数据报格式较为简单，容易丢失 TCP传输在接收端会进行重排，所以是有序的，UDP则不保证有序 TCP速度慢，UDP速度快 TCP有流量控制和拥塞控制，而UDP没有 应用层协议有哪些 DNS协议，域名解析系统。基于TCP和UDP的协议，通过DNS可以将域名转换成IP地址 SMTP协议，电子邮件协议。基于TCP的协议，通过SMTP协议可以发送电子邮件，SMTP通信的过程建立连接、邮件传送、连接释放 Telnet协议，远程终端协议。基于TCP的协议，通过Telnet协议可以对远程的终端进行控制 Http协议，超文本传输协议。基于TCP的协议，通过Http协议实现客户端和服务端的数据传输 FTP协议，文件传输协议。基于TCP的协议，通过FTP协议达到相互传输文件的效果 OSI参考模型与TCP/IP参考模型(1) OSI参考模型由7层组成：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层(2) TCP/IP参考模型由4层组成：主机-网络层、网际层、传输层、应用层(3) 对应关系中，OSI参考模型的物理层、数据链路层对应TCP/IP的主机-网络层，网络层对应网际层，传输层对应传输层，会话层、表示层、应用层对应应用层 cookie 和 session的区别(1) cookie由于把信息保存在客户端中。session把信息保存在服务器中(2) cookie性能更高一点，速度较快，用户的信息存在各自的浏览器中，可以分担服务器的一部分存储工作。session速度较慢，所有用户的信息都存在服务器中，在高并发时必然影响服务器性能(3) cookie有限制大小，在4K以内。session没有限制(4) cookie对用户是透明的，安全性低，不重要的或者可以公开的信息保存在cookie。session对用户是不可见的，安全性高，重要信息应该保存在session forward 和 redirect的区别(1) forward为转发，进行forward操作后，请求URL不发生变化，并且会把请求的数据携带到下一个请求中。redirect是重定向，进行redirect操作后，请求URL是发生变化的(2) forward是服务器内部请求转发，不可以请求到其它站点，redirect是服务器通知客户端重新请求，可以请求到其它站点(3) forward速度快，redirect速度慢 DNS劫持和DNS污染(1) DNS劫持：指用户访问一个域名时，DNS服务器故意将此地址指向一个错误的IP地址的行为。比如进入一个网站显示的却是另外一个网站的内容(2) DNS污染：指用户访问一个域名时，国内的服务器(非DNS)监控到用户访问的已经被标记地址时，服务器伪装成DNS服务器向用户发回错误的地址的行为。比如国内不能访问Google、YouTube等 5.TCP通信程序5.1 简单的TCP网络程序TCP通信分析图解 【服务端】启动,创建ServerSocket对象，等待连接。 【客户端】启动,创建Socket对象，请求连接。 【服务端】接收连接,调用accept方法，并返回一个Socket对象。 【客户端】Socket对象，获取OutputStream，向服务端写出数据。 【服务端】Scoket对象，获取InputStream，读取客户端发送的数据。 到此，客户端向服务端发送数据成功。自此，服务端向客户端回写数据。 【服务端】Socket对象，获取OutputStream，向客户端回写数据。 【客户端】Scoket对象，获取InputStream，解析回写数据。 【客户端】释放资源，断开连接。 服务端实现： 12345678910111213141516171819202122232425262728public class ServerTCP &#123; public static void main(String[] args) throws IOException &#123; System.out.println(\"服务端启动 , 等待连接 .... \"); // 1.创建 ServerSocket对象，绑定端口，开始等待连接 ServerSocket ss = new ServerSocket(6666); // 2.接收连接 accept 方法, 返回 socket 对象. Socket server = ss.accept(); // 3.通过socket 获取输入流 InputStream is = server.getInputStream(); // 4.一次性读取数据 // 4.1 创建字节数组 byte[] b = new byte[1024]; // 4.2 据读取到字节数组中. int len = is.read(b)； // 4.3 解析数组,打印字符串信息 String msg = new String(b, 0, len); System.out.println(msg); // =================回写数据======================= // 5. 通过 socket 获取输出流 OutputStream out = server.getOutputStream(); // 6. 回写数据 out.write(\"我很好,谢谢你\".getBytes()); // 7.关闭资源. out.close(); is.close(); server.close(); &#125;&#125; 客户端实现： 12345678910111213141516171819202122public class ClientTCP &#123; public static void main(String[] args) throws Exception &#123; System.out.println(\"客户端 发送数据\"); // 1.创建 Socket ( ip , port ) , 确定连接到哪里. Socket client = new Socket(\"localhost\", 6666); // 2.通过Scoket,获取输出流对象 OutputStream os = client.getOutputStream(); // 3.写出数据. os.write(\"你好么? tcp ,我来了\".getBytes()); // ==============解析回写========================= // 4. 通过Scoket,获取 输入流对象 InputStream in = client.getInputStream(); // 5. 读取数据数据 byte[] b = new byte[100]; int len = in.read(b); System.out.println(new String(b, 0, len)); // 6. 关闭资源 . in.close(); os.close(); client.close(); &#125;&#125; 5.2 文件上传文件上传分析图解 【客户端】输入流，从硬盘读取文件数据到程序中。 【客户端】输出流，写出文件数据到服务端。 【服务端】输入流，读取文件数据到服务端程序。 【服务端】输出流，写出文件数据到服务器硬盘中。 【服务端】获取输出流，回写数据。 【客户端】获取输入流，解析回写数据。 服务端实现: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class FileUpload_Server &#123; public static void main(String[] args) throws IOException &#123; System.out.println(\"服务器 启动..... \"); // 1. 创建服务端ServerSocket ServerSocket serverSocket = new ServerSocket(6666); // 2. 循环接收,建立连接 while (true) &#123; Socket accept = serverSocket.accept(); /* 3. socket对象交给子线程处理,进行读写操作 Runnable接口中,只有一个run方法,使用lambda表达式简化格式 */ new Thread(() -&gt; &#123; try ( //3.1 获取输入流对象 BufferedInputStream bis = new BufferedInputStream(accept.getInputStream()); //3.2 创建输出流对象, 保存到本地 . FileOutputStream fis = new FileOutputStream(System.currentTimeMillis() + \".jpg\"); BufferedOutputStream bos = new BufferedOutputStream(fis); ) &#123; // 3.3 读写数据 byte[] b = new byte[1024 * 8]; int len; while ((len = bis.read(b)) != -1) &#123; bos.write(b, 0, len); &#125; // 4.=======信息回写=========================== System.out.println(\"back ........\"); OutputStream out = accept.getOutputStream(); out.write(\"上传成功\".getBytes()); out.close(); //================================ //5. 关闭 资源 bos.close(); bis.close(); accept.close(); System.out.println(\"文件上传已保存\"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125; &#125;&#125; 客户端实现： 12345678910111213141516171819202122232425262728293031public class FileUpload_Client &#123; public static void main(String[] args) throws IOException &#123; // 1.创建流对象 // 1.1 创建输入流,读取本地文件 BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\"test.jpg\")); // 1.2 创建输出流,写到服务端 Socket socket = new Socket(\"localhost\", 6666); BufferedOutputStream bos = new BufferedOutputStream(socket.getOutputStream()); //2.写出数据. byte[] b = new byte[1024 * 8 ]; int len ; while (( len = bis.read(b))!=-1) &#123; bos.write(b, 0, len); &#125; // 关闭输出流,通知服务端,写出数据完毕 socket.shutdownOutput(); System.out.println(\"文件发送完毕\"); // 3. =====解析回写============ InputStream in = socket.getInputStream(); byte[] back = new byte[20]; in.read(back); System.out.println(new String(back)); in.close(); // ============================ // 4.释放资源 socket.close(); bis.close(); &#125;&#125;","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://imokkkk.github.io/categories/计算机网络/"}],"tags":[{"name":"javase","slug":"javase","permalink":"https://imokkkk.github.io/tags/javase/"}]},{"title":"线程池","slug":"线程池","date":"2021-05-08T13:19:59.242Z","updated":"2020-11-03T12:25:05.495Z","comments":true,"path":"ThreadPool/","link":"","permalink":"https://imokkkk.github.io/ThreadPool/","excerpt":"线程池1.概述","text":"线程池1.概述 原理：当提交一个任务时，线程池创建一个新线程执行任务，直到当前线程数等于corePoolSize；如果当前线程数为 corePoolSize，继续提交的任务被保存到阻塞队列中，等待被执行；如果阻塞队列满了，那就创建新的线程执行当前任务；直到线程池中的线程数达到 maxPoolSize，这时再有任务来，只能执行 reject() 处理该任务。优点： 降低资源消耗； 提高响应速度； 提高线程的可管理性。 缺点：TODO 1.1 四种常用ExecutorService特性 类型 核心线程数 最大线程数 KeepAlive时间(存活时间) 任务队列 拒绝策略 newCachedThreadPool(可缓存线程池) 0 Integer.MAX_VALUE 60s SynchronousQueue 线程池无限大，当执行第二个任务已经完成，会复用执行第一个任务的线程。 newFixedThreadPool(定长线程池) 指定大小 指定大小(与核心线程数相同) 0 LinkedBlockingQueue 线程池大小固定，没有可用的线程的时候，任务会放在队列等待，队列的长度无限制。 newSingleThreadExexutor 1 1 0 LinkedBlockingQueue 单线程化的线程池，适用于业务逻辑上只允许1个线程进行处理的场景，保证所有任务按照指定顺序FIFO(先进先出)，LIFO(后进先出)，优先级执行。 newScheduledThreadPool 指定大小 Integer.MAX_VALUE 0 DelayedWordQueue 定长线程池，支持定时及周期性任务执行。 1.2 ThreadPoolExecutor《阿里巴巴 Java 开发手册》中规定线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。而且线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式。这样的处理方式能够更加明确线程池的运行规则，规避资源耗尽的风险。 1234567ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 参数：corePoolSize：核心线程数，指定了线程池中的线程池数量，它的数量决定了添加的任务是开辟新的线程去执行，还是放到workQueue任务队列中； maximumPoolSize：指定了线程池中的最大线程数量，这个参数会根据使用的workQueue任务队列的类型，决定线程池会开辟的最大线程数量。 核心和最大线程数大小仅在构建时设置，但也可以使用 setCorePoolSize() 和 setMaximumPoolSize() 进行动态更改。keepAliveTime：当线程池中的空闲线程数量超过corePoolSize时，多余的线程会在多长时间内被销毁。如果线程池在以后会变得更加活跃，则应构建线程或者使用setKeepAliveTime(long, TimeUnit)方法。 unit：keepAliveTime的单位 workQueue：阻塞队列(用来保存等待被执行的任务) ArrayBlockingQueue：基于数组结构的有界任务队列，按照FIFO排序任务。若有新的任务需要执行时，线程会创建新的线程，直到创建的线程数量达到corePoolSize时，则会将新的任务加入到等待队列中。若等待队列已满，即超过ArrayBlockingQueue初始化的容量，则继续创建线程数量达到maximumPoolSize，则执行拒绝策略。这种情况下，线程数量的上限与有界任务队列的状态有直接关系，如果有界任务队列的初始容量比较大或者没有达到超负荷状态，线程数将会一直维持在corePoolSize以下，反之，则会以maximumPoolSize为最大线程数上限。 没有预定义容量的LinkedBlockingQueue：基于链表结构的无界任务队列，按照FIFO排序任务。使用无界任务队列，线程池的任务队列可以无限制的添加新的任务，当线程数达到corePoolSize后就不会再增加了。使用无界任务队列将导致新任务在队列中等待，从而导致maximumPoolSize的值没有任何作用。当使用这种任务队列模式时，一定要注意任务提交与处理之间的协调与控制，不然会出现队列中的任务由于无法及时处理导致一直增长，直到最后资源耗尽的问题。这种队列方式可以用于平滑瞬时大量请求。 SynchronousQueue：一个不存储元素的阻塞队列，每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于ArrayBlockingQueue。直接握手队列：它将任务交给线程而不需要保留，如果没有线程立即可用来运行它，那么排队任务的尝试将失败，因此构建新的线程，如果达到maximumPoolSize设置的最大值，则根据设置的handler执行拒绝策略。在这种情况下，需要对程序的并发量有个准确的评估，才能设置合适的maximumPoolSize数量避免执行拒绝策略。应注意，当任务持续以平均提交速度大于平均处理速度时，会导致线程数量会无限增长问题。 PriorityBlockingQueue：具有优先级的无界任务队列。优先任务队列：特殊的无界任务队列，无论添加了多少个任务，线程数量都不会超过corePoolSize。其它队列一般是按照FIFO(先进先出)的规则处理任务，而PriorityBlockingQueue队列可以自定义规则根据任务的优先级顺序先后执行。 threadFactory：线程工程，用于创建线程。如果未另行指定，则使用Executors.defaultThreadFactory默认工厂，使其全部位于同一个ThreadGroup中，并具有相同的NORM_PRIORITY优先级和非守护进程状态。通过不同的ThreadFactory可以更改线程的名称，线程组，优先级，守护进程状态等。privilegedThreadFactory：继承自defaultThreadFactory，主要添加了访问权限校验。 handler：拒绝策略，创建线程池时，为防止资源被耗尽，任务队列都会选择创建有界任务队列，但如果出现任务队列已满且线程池创建的线程数达到maximumPoolSize时，这时就需要指定ThreadPoolExecutor的RejectedExecutionHandler参数即合理的拒绝策略，来处理线程池”超载”的情况。ThreadPoolExecutor自带的拒绝策略如下： AbortPolicy：默认策略，丢掉任务直接抛出RejectedExecutionException异常，阻止系统正常工作。 CallerRunsPolicy：如果线程池的线程池的线程数量达到上限，该策略会把拒绝的任务放在调用者线程当中运行，如果执行程序已关闭，则会丢弃该任务。 DiscardPolicy：该策略会默默丢弃无法处理的任务，不会抛出任何异常，使用此策略，业务场景中需允许任务的丢失。 DiscardOldestPolicy：该策略会丢弃任务队列中最老的一个任务，也就是当前任务队列中最先被添加进去的。即每次移除队头元素后再尝试入队。 2.使用12345678910111213141516171819public class TestThreadPool &#123; private static final ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(8, 16, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;&gt;(100), new ThreadPoolExecutor.AbortPolicy()); private static class testTask implements Runnable &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125; &#125; public static void main(String[] args) &#123; testTask testTask = new testTask(); for (int i = 0; i &lt; 50; i++) &#123; threadPoolExecutor.submit(testTask); &#125; threadPoolExecutor.shutdown(); &#125;&#125;","categories":[{"name":"多线程","slug":"多线程","permalink":"https://imokkkk.github.io/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"https://imokkkk.github.io/tags/多线程/"},{"name":"并发编程","slug":"并发编程","permalink":"https://imokkkk.github.io/tags/并发编程/"}]},{"title":"微信公众号开发(四)处理语音消息","slug":"微信公众号开发(四)处理语音消息","date":"2021-05-08T13:19:59.224Z","updated":"2020-04-29T02:44:38.015Z","comments":true,"path":"35386/","link":"","permalink":"https://imokkkk.github.io/35386/","excerpt":"1.语音识别接口为了实现微信公众号与用户的多样化交互, 本章进行处理用户语音消息的开发. 开发者进入微信公众平台 https://mp.weixin.qq.com/ —接口权限—对话服务—接受消息—打开接收语音识别结果接口权限","text":"1.语音识别接口为了实现微信公众号与用户的多样化交互, 本章进行处理用户语音消息的开发. 开发者进入微信公众平台 https://mp.weixin.qq.com/ —接口权限—对话服务—接受消息—打开接收语音识别结果接口权限 2.获取语音识别结果请注意，开通语音识别后，用户每次发送语音给公众号时，微信会在推送的语音消息XML数据包中，增加一个Recognition字段（注：由于客户端缓存，开发者开启或者关闭语音识别功能，对新关注者立刻生效，对已关注用户需要24小时生效。开发者可以重新关注此帐号进行测试）。 开启语音识别后的语音XML数据包如下： 12345678910&lt;xml&gt; &lt;ToUserName&gt;&lt; ![CDATA[toUser] ]&gt;&lt;/ToUserName&gt; &lt;FromUserName&gt;&lt; ![CDATA[fromUser] ]&gt;&lt;/FromUserName&gt; &lt;CreateTime&gt;1357290913&lt;/CreateTime&gt; &lt;MsgType&gt;&lt; ![CDATA[voice] ]&gt;&lt;/MsgType&gt; &lt;MediaId&gt;&lt; ![CDATA[media_id] ]&gt;&lt;/MediaId&gt; &lt;Format&gt;&lt; ![CDATA[Format] ]&gt;&lt;/Format&gt; &lt;Recognition&gt;&lt; ![CDATA[腾讯微信团队] ]&gt;&lt;/Recognition&gt; &lt;MsgId&gt;1234567890123456&lt;/MsgId&gt;&lt;/xml&gt; 语音消息参数说明 开通语音识别功能以后，用户每次发送语音给微信公众号，微信会在推送语音消息XML数据包中添加一个Recongnition字段，该字段为语音识别出的文本内容. 3.功能实现实体类VoiceMessage 1234@Datapublic class VoiceMessage extends BaseMessage&#123; private String Recognition;&#125; MessageUtil 1234public static String voiceMessageToXml(VoiceMessage voiceMessage) &#123; xstream.alias(\"xml\", voiceMessage.getClass()); return xstream.toXML(voiceMessage);&#125; MsgService 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Servicepublic class MsgService &#123; private static final Logger LOGGER = LoggerFactory.getLogger(MsgService.class); public String processRequest(HttpServletRequest request) &#123; String respMessage = null; System.out.println(LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))); try &#123; // xml请求解析 Map&lt;String, String&gt; requestMap = MessageUtil.xmlToMap(request); // 发送方帐号（open_id） String fromUserName = requestMap.get(\"FromUserName\"); // 公众帐号 String toUserName = requestMap.get(\"ToUserName\"); // 消息类型 String msgType = requestMap.get(\"MsgType\"); // 消息内容 String content = requestMap.get(\"Content\"); String recognition = requestMap.get(\"Recognition\"); LOGGER.info(\"FromUserName is:\" + fromUserName + \", ToUserName is:\" + toUserName + \", MsgType is:\" + msgType); if (msgType.equals(MessageUtil.REQ_MESSAGE_TYPE_VOICE))&#123; System.out.println(recognition); if(recognition.indexOf(\"环境信息\")!=-1)&#123; Map map = IoTPopApiUtil.IoTpop(); Map ioTpop = JSON.parseObject(JSONObject.toJSONString(map), Map.class); Object data = ioTpop.get(\"data\"); String str = data.toString(); int index=str.indexOf(\"[\"); String result=str.substring(index); String jsonStr = result.substring(0, result.length() - 1); JSONArray array = JSONArray.parseArray(jsonStr); List&lt;Pi&gt; pi = JSONObject.parseArray(array.toJSONString(),Pi.class); String returnText=\"当前温度:\"+pi.get(3).getValue()+\"°C\"+\"\\n\" +\"当前湿度:\"+pi.get(2).getValue()+\"%\"+\"\\n\" +\"当前光照强度:\"+pi.get(4).getValue()+\"Lux\"+\"\\n\" +\"当前气压:\"+pi.get(1).getValue()+\"hPa\"+\"\\n\" +\"当前海拔:\"+pi.get(0).getValue()+\"m\"+\"\\n\" +\"降雨情况:\"+(pi.get(5).getValue()==1?\"降雨\":\"未降雨\"); //文本消息 TextMessage text = new TextMessage(); text.setContent(returnText); text.setToUserName(fromUserName); text.setFromUserName(toUserName); text.setCreateTime(LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))); text.setMsgType(MessageUtil.RESP_MESSAGE_TYPE_TEXT); respMessage = MessageUtil.textMessageToXml(text); &#125; if(recognition.indexOf(\"天气\")!=-1)&#123; //自动回复 NewsMessage newmsg = new NewsMessage(); newmsg.setToUserName(fromUserName); newmsg.setFromUserName(toUserName); newmsg.setCreateTime(LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))); newmsg.setMsgType(MessageUtil.RESP_MESSAGE_TYPE_NEWS); newmsg.setFuncFlag(0); List&lt;Article&gt; articleList = new ArrayList&lt;&gt;(); Article article = new Article(); article.setTitle(\"天气预报\"); article.setDescription(\"点击了解未来天气详情...\"); article.setPicUrl(\"https://xxxx.oss-cn-beijing.aliyuncs.com/ep.png\"); article.setUrl(\"https://widget-page.heweather.net/h5/index.html?bg=1&amp;md=0123456&amp;lc=accu&amp;key=4bdfe35a67bb4b53bee844f6ce7a4b5c\"); articleList.add(article); // 设置图文消息个数 newmsg.setArticleCount(articleList.size()); // 设置图文消息包含的图文集合 newmsg.setArticles(articleList); // 将图文消息对象转换成xml字符串 respMessage = MessageUtil.newsMessageToXml(newmsg); &#125; &#125; &#125; catch (Exception e) &#123; LOGGER.error(\"error......\"); &#125; return respMessage; &#125;&#125; 4.测试","categories":[],"tags":[{"name":"tool","slug":"tool","permalink":"https://imokkkk.github.io/tags/tool/"}]},{"title":"微信公众号开发(五)部署项目到阿里云服务器","slug":"微信公众号开发(五)部署项目到阿里云服务器","date":"2021-05-08T13:19:59.223Z","updated":"2020-05-28T02:18:40.540Z","comments":true,"path":"deployment/","link":"","permalink":"https://imokkkk.github.io/deployment/","excerpt":"1.准备工作 可以正常运行提供服务的项目 一台云服务器 2.项目打包首先将我们在IDEA下的项目进行打包，这边基于的是maven项目的打包。点击菜单栏 File → Project Structure → Artifacts 添加一个jar","text":"1.准备工作 可以正常运行提供服务的项目 一台云服务器 2.项目打包首先将我们在IDEA下的项目进行打包，这边基于的是maven项目的打包。点击菜单栏 File → Project Structure → Artifacts 添加一个jar 之后，对添加的jar进行配置 点击右侧竖排菜单栏的maven project 然后点开Lifecycle，先clean再package，注意在打包之前，要将项目的启动端口号改为80，8080端口是本机端口，不适用于服务器。 当下方控制器显示BUILD SUCCESS时说明打包成功 这时候我们可以在项目的文件目录下看到多了个target目录，点开目录移动到最下方我们可以看到项目jar包,我们可以从电脑磁盘中将放置该项目的文件夹打开并找到该项目文件。 3.导入服务器使用FTP工具负责构建完成的项目jar包到云服务器 4.运行项目4.1 配置Java运行环境 查看yum库中的Java安装包 1yum -y list java* 以yum库中java-1.8.0为例, “*”表示将java-1.8.0的所有相关Java程序都安装上 1yum -y install java-1.8.0-openjdk* 检查是否安装成功输入 java -version javac 4.2 启动项目查询一下80端口是否已开放，开放了80端口后我们就可以启动我们的项目了，通过输入指令：Java -jar [jar包的完整文件名(.jar别忘了加)] 如下图所示。这样我们的项目就开始启动了 1java -jar WeChat-1.0-SNAPSHOT.jar","categories":[{"name":"项目","slug":"项目","permalink":"https://imokkkk.github.io/categories/项目/"}],"tags":[{"name":"项目部署","slug":"项目部署","permalink":"https://imokkkk.github.io/tags/项目部署/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://imokkkk.github.io/tags/Spring-Boot/"},{"name":"微信公众号","slug":"微信公众号","permalink":"https://imokkkk.github.io/tags/微信公众号/"}]},{"title":"微信公众号开发(二)自动回复功能实现简单的天气查询","slug":"微信公众号开发(二)自动回复功能实现简单的天气查询","date":"2021-05-08T13:19:59.221Z","updated":"2020-04-29T02:44:38.058Z","comments":true,"path":"39972/","link":"","permalink":"https://imokkkk.github.io/39972/","excerpt":"1.前言微信公众平台服务器配置通过后，就能进行下面的开发啦 首先可以查看官方的说明文档：https://developers.weixin.qq.com/doc/offiaccount/Getting_Started/Overview.html 普通消息的类型分为7种：","text":"1.前言微信公众平台服务器配置通过后，就能进行下面的开发啦 首先可以查看官方的说明文档：https://developers.weixin.qq.com/doc/offiaccount/Getting_Started/Overview.html 普通消息的类型分为7种： 文本消息 图片消息 语音消息 视频消息 小视频消息 地理位置消息 链接消息 本文使用的是文本消息与图片消息 2.图文消息的自动回复2.1 文本消息文本消息的XML结构是： 12345678&lt;xml&gt; &lt;ToUserName&gt;&lt;![CDATA[toUser]]&gt;&lt;/ToUserName&gt; &lt;FromUserName&gt;&lt;![CDATA[fromUser]]&gt;&lt;/FromUserName&gt; &lt;CreateTime&gt;1348831860&lt;/CreateTime&gt; &lt;MsgType&gt;&lt;![CDATA[text]]&gt;&lt;/MsgType&gt; &lt;Content&gt;&lt;![CDATA[this is a test]]&gt;&lt;/Content&gt; &lt;MsgId&gt;1234567890123456&lt;/MsgId&gt;&lt;/xml&gt; 参数包含： 定义一个BaseMessage，消息基类，封装通用属性： 123456789101112131415161718192021/** * 消息基类（普通用户 -&gt; 公众帐号） * */@Datapublic class BaseMessage &#123; // 开发者微信号 private String ToUserName; // 发送方帐号（一个OpenID） private String FromUserName; // 消息创建时间 （整型） private String CreateTime; // 消息类型（text/image/location/link） private String MsgType; // 消息id，64位整型 private long MsgId; /** * 位0x0001被标志时，星标刚收到的消息 */ private int FuncFlag;&#125; 接下来定义文本消息属性TextMessage： 12345678/** * 文本消息 */@Datapublic class TextMessage extends BaseMessage&#123; // 消息内容 private String Content;&#125; 2.2 图片消息图片消息的XML结构是： 123456789&lt;xml&gt; &lt;ToUserName&gt;&lt;![CDATA[toUser]]&gt;&lt;/ToUserName&gt; &lt;FromUserName&gt;&lt;![CDATA[fromUser]]&gt;&lt;/FromUserName&gt; &lt;CreateTime&gt;1348831860&lt;/CreateTime&gt; &lt;MsgType&gt;&lt;![CDATA[image]]&gt;&lt;/MsgType&gt; &lt;PicUrl&gt;&lt;![CDATA[this is a url]]&gt;&lt;/PicUrl&gt; &lt;MediaId&gt;&lt;![CDATA[media_id]]&gt;&lt;/MediaId&gt; &lt;MsgId&gt;1234567890123456&lt;/MsgId&gt;&lt;/xml&gt; 参数包含： 123@Datapublic class ImageMessage extends BaseMessage&#123;&#125; 2.3 图文消息1234567891011121314151617181920@Datapublic class Article &#123; /** * 图文消息描述 */ private String Description; /** * 图片链接，支持JPG、PNG格式，&lt;br&gt; * 较好的效果为大图640*320，小图80*80 */ private String PicUrl; /** * 图文消息名称 */ private String Title; /** * 点击图文消息跳转链接 */ private String Url;&#125; 123456789101112131415/** * &lt;p&gt; 图文消息 &lt;/p&gt; */@Setter@Getterpublic class NewsMessage extends BaseMessage&#123; /** * 图文消息个数，限制为10条以内 */ private Integer ArticleCount; /** * 多条图文消息信息，默认第一个item为大图 */ private List&lt;Article&gt; Articles;&#125; 3.功能实现3.1 工具类MessageUtil 解析微信发来的请求（xml） 将响应消息的Java对象转换成xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180public class MessageUtil &#123; /** * 返回消息类型：文本 */ public static final String RESP_MESSAGE_TYPE_TEXT = \"text\"; /** * 返回消息类型：音乐 */ public static final String RESP_MESSAGE_TYPE_MUSIC = \"music\"; /** * 返回消息类型：图文 */ public static final String RESP_MESSAGE_TYPE_NEWS = \"news\"; /** * 请求消息类型：文本 */ public static final String REQ_MESSAGE_TYPE_TEXT = \"text\"; /** * 请求消息类型：图片 */ public static final String REQ_MESSAGE_TYPE_IMAGE = \"image\"; /** * 请求消息类型：链接 */ public static final String REQ_MESSAGE_TYPE_LINK = \"link\"; /** * 请求消息类型：地理位置 */ public static final String REQ_MESSAGE_TYPE_LOCATION = \"location\"; /** * 请求消息类型：音频 */ public static final String REQ_MESSAGE_TYPE_VOICE = \"voice\"; /** * 请求消息类型：推送 */ public static final String REQ_MESSAGE_TYPE_EVENT = \"event\"; /** * 事件类型：subscribe(订阅) */ public static final String EVENT_TYPE_SUBSCRIBE = \"subscribe\"; /** * 事件类型：unsubscribe(取消订阅) */ public static final String EVENT_TYPE_UNSUBSCRIBE = \"unsubscribe\"; /** * 事件类型：CLICK(自定义菜单点击事件) */ public static final String EVENT_TYPE_CLICK = \"CLICK\"; /** * xml转换为map * @param request * @return * @throws IOException */ public static Map&lt;String, String&gt; xmlToMap(HttpServletRequest request) throws IOException &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); SAXReader reader = new SAXReader(); InputStream ins = null; try &#123; ins = request.getInputStream(); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; Document doc = null; try &#123; doc = reader.read(ins); Element root = doc.getRootElement(); List&lt;Element&gt; list = root.elements(); for (Element e : list) &#123; map.put(e.getName(), e.getText()); &#125; return map; &#125; catch (DocumentException e1) &#123; e1.printStackTrace(); &#125;finally&#123; ins.close(); &#125; return null; &#125; /** * @Description: 解析微信发来的请求（XML） * @param @param request * @param @return * @param @throws Exception * @author dapengniao * @date 2016年3月7日 上午10:04:02 */ public static Map&lt;String, String&gt; parseXml(HttpServletRequest request) throws Exception &#123; // 将解析结果存储在HashMap中 Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); // 从request中取得输入流 InputStream inputStream = request.getInputStream(); // 读取输入流 SAXReader reader = new SAXReader(); Document document = reader.read(inputStream); // 得到xml根元素 Element root = document.getRootElement(); // 得到根元素的所有子节点 List&lt;Element&gt; elementList = root.elements(); // 遍历所有子节点 for (Element e : elementList) &#123; map.put(e.getName(), e.getText()); &#125; // 释放资源 inputStream.close(); inputStream = null; return map; &#125;// public static XStream xstream = new XStream(); /** * 文本消息对象转换成xml * * @param textMessage 文本消息对象 * @return xml */ public static String textMessageToXml(TextMessage textMessage)&#123;// XStream xstream = new XStream(); xstream.alias(\"xml\", textMessage.getClass()); return xstream.toXML(textMessage); &#125; /** * @Description: 图文消息对象转换成xml * @param @param newsMessage * @param @return * @author dapengniao * @date 2016年3月8日 下午4:14:09 */ public static String newsMessageToXml(NewsMessage newsMessage) &#123; xstream.alias(\"xml\", newsMessage.getClass()); xstream.alias(\"item\", new Article().getClass()); return xstream.toXML(newsMessage); &#125; /** * @Description: 图片消息对象转换成xml * @param @param imageMessage * @param @return * @author dapengniao * @date 2016年3月9日 上午9:25:51 */ public static String imageMessageToXml(ImageMessage imageMessage) &#123; xstream.alias(\"xml\", imageMessage.getClass()); return xstream.toXML(imageMessage); &#125; /** * 对象到xml的处理 */ private static XStream xstream = new XStream(new XppDriver() &#123; public HierarchicalStreamWriter createWriter(Writer out) &#123; return new PrettyPrintWriter(out) &#123; // 对所有xml节点的转换都增加CDATA标记 boolean cdata = true; @SuppressWarnings(\"rawtypes\") public void startNode(String name, Class clazz) &#123; super.startNode(name, clazz); &#125; protected void writeText(QuickWriter writer, String text) &#123; if (cdata) &#123; writer.write(\"&lt;![CDATA[\"); writer.write(text); writer.write(\"]]&gt;\"); &#125; else &#123; writer.write(text); &#125; &#125; &#125;; &#125; &#125;);&#125; 3.2 实现当用户发送消息给公众号时（或某些特定的用户操作引发的事件推送时），会产生一个POST请求，开发者可以在响应包（Get）中返回特定XML结构，来对该消息进行响应（现支持回复文本、图片、图文、语音、视频、音乐）。 上一篇文章，已经创建了IndexController ，里面的GET方法用来验证token，下面直接加一个POST方法，用于进行消息管理。消息接收POST和微信认证GET是同一个接口（开发者填写的URL） Controller 12345678910111213141516171819202122@PostMappingpublic void msgProcess(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; try &#123; request.setCharacterEncoding(\"UTF-8\"); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; response.setCharacterEncoding(\"UTF-8\"); // 调用核心业务类接收消息、处理消息 String respMessage = msgService.processRequest(request); // 响应消息 PrintWriter out = null; try &#123; out = response.getWriter(); out.print(respMessage); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; out.close(); out = null; &#125;&#125; Service 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Servicepublic class MsgService &#123; private static final Logger LOGGER = LoggerFactory.getLogger(MsgService.class); public String processRequest(HttpServletRequest request) &#123; String respMessage = null; System.out.println(LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))); try &#123; // xml请求解析 Map&lt;String, String&gt; requestMap = MessageUtil.xmlToMap(request); // 发送方帐号（open_id） String fromUserName = requestMap.get(\"FromUserName\"); // 公众帐号 String toUserName = requestMap.get(\"ToUserName\"); // 消息类型 String msgType = requestMap.get(\"MsgType\"); // 消息内容 String content = requestMap.get(\"Content\"); LOGGER.info(\"FromUserName is:\" + fromUserName + \", ToUserName is:\" + toUserName + \", MsgType is:\" + msgType); // 文本消息 if (msgType.equals(MessageUtil.REQ_MESSAGE_TYPE_TEXT)) &#123; //这里根据关键字执行相应的逻辑，只有你想不到的，没有做不到的 if (content.indexOf(\"天气\")!=-1) &#123; //自动回复 NewsMessage newmsg = new NewsMessage(); newmsg.setToUserName(fromUserName); newmsg.setFromUserName(toUserName); newmsg.setCreateTime(LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))); newmsg.setMsgType(MessageUtil.RESP_MESSAGE_TYPE_NEWS); newmsg.setFuncFlag(0); List&lt;Article&gt; articleList = new ArrayList&lt;&gt;(); Article article = new Article(); article.setTitle(\"天气预报\"); article.setDescription(\"点击了解未来天气详情...\"); article.setPicUrl(\"https://lwy-image.oss-cn-beijing.aliyuncs.com/ep.png\"); article.setUrl(\"https://widget-page.heweather.net/h5/index.html?bg=1&amp;md=0123456&amp;lc=auto&amp;key=f1688db9422246fc969a6ba559075097\"); articleList.add(article); // 设置图文消息个数 newmsg.setArticleCount(articleList.size()); // 设置图文消息包含的图文集合 newmsg.setArticles(articleList); // 将图文消息对象转换成xml字符串 respMessage = MessageUtil.newsMessageToXml(newmsg); &#125; &#125; &#125; catch (Exception e) &#123; LOGGER.error(\"error......\"); &#125; return respMessage; &#125;&#125; 测试 源码参考 https://github.com/zhouminpz/wechatPublicAccount-","categories":[],"tags":[{"name":"tool","slug":"tool","permalink":"https://imokkkk.github.io/tags/tool/"}]},{"title":"微信公众号开发(三)快递信息查询","slug":"微信公众号开发(三)快递信息查询","date":"2021-05-08T13:19:59.220Z","updated":"2020-04-29T02:44:38.057Z","comments":true,"path":"12221/","link":"","permalink":"https://imokkkk.github.io/12221/","excerpt":"1.快递查询API这里使用的是阿里云全国快递物流查询-快递查询接口:https://market.aliyun.com/products/56928004/cmapi021863.html 该接口支持只通过快递运单号查询物流信息, 不需要在额外设置参数. 该种方式95%能自动识别, 填写查询速度会更快, 已经满足一般开发的需求, 并能极大方便开发者的使用.","text":"1.快递查询API这里使用的是阿里云全国快递物流查询-快递查询接口:https://market.aliyun.com/products/56928004/cmapi021863.html 该接口支持只通过快递运单号查询物流信息, 不需要在额外设置参数. 该种方式95%能自动识别, 填写查询速度会更快, 已经满足一般开发的需求, 并能极大方便开发者的使用. 请求参数说明 返回结果说明 官方提供的示例代码: 1234567891011121314151617181920212223242526272829303132333435public static void main(String[] args) &#123; String host = \"https://wuliu.market.alicloudapi.com\"; String path = \"/kdi\"; String method = \"GET\"; System.out.println(\"请先替换成自己的AppCode\"); String appcode = \"833509fd73fe1124838xxxxxxxx\"; // !!!替换填写自己的AppCode 在买家中心查看 Map&lt;String, String&gt; headers = new HashMap&lt;String, String&gt;(); headers.put(\"Authorization\", \"APPCODE \" + appcode); //格式为:Authorization:APPCODE 83359fd73fe11248385f570e3c139xxx Map&lt;String, String&gt; querys = new HashMap&lt;String, String&gt;(); querys.put(\"no\", \"462587770684\");// !!! 请求参数 querys.put(\"type\", \"zto\");// !!! 请求参数 //JDK 1.8示例代码请在这里下载： http://code.fegine.com/Tools.zip try &#123; /** * 重要提示如下: * HttpUtils请从 * https://github.com/aliyun/api-gateway-demo-sign-java/blob/master/src/main/java/com/aliyun/api/gateway/demo/util/HttpUtils.java * 或者直接下载： * http://code.fegine.com/HttpUtils.zip * 下载 * * 相应的依赖请参照 * https://github.com/aliyun/api-gateway-demo-sign-java/blob/master/pom.xml * 相关jar包（非pom）直接下载： * http://code.fegine.com/aliyun-jar.zip */ HttpResponse response = HttpUtils.doGet(host, path, method, headers, querys); //System.out.println(response.toString());如不输出json, 请打开这行代码，打印调试头部状态码。 //状态码: 200 正常；400 URL无效；401 appCode错误； 403 次数用完； 500 API网管错误 //获取response的body System.out.println(EntityUtils.toString(response.getEntity())); //输出json &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 正常返回示例: 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; \"status\": \"0\",/* status 0:正常查询 201:快递单号错误 203:快递公司不存在 204:快递公司识别失败 205:没有信息 207:该单号被限制，错误单号 */ \"msg\": \"ok\", \"result\": &#123; \"number\": \"780098068058\", \"type\": \"zto\", \"list\": [&#123; \"time\": \"2018-03-09 11:59:26\", \"status\": \"【石家庄市】快件已在【长安三部】 签收,签收人: 本人,感谢使用中通快递,期待再次为您服务!\" &#125;, &#123; \"time\": \"2018-03-09 09:03:10\", \"status\": \"【石家庄市】 快件已到达 【长安三部】（0311-85344265）,业务员 容晓光（13081105270） 正在第1次派件, 请保持电话畅通,并耐心等待\" &#125;, &#123; \"time\": \"2018-03-08 23:43:44\", \"status\": \"【石家庄市】 快件离开 【石家庄】 发往 【长安三部】\" &#125;, &#123; \"time\": \"2018-03-08 21:00:44\", \"status\": \"【石家庄市】 快件到达 【石家庄】\" &#125;, &#123; \"time\": \"2018-03-07 01:38:45\", \"status\": \"【广州市】 快件离开 【广州中心】 发往 【石家庄】\" &#125;, &#123; \"time\": \"2018-03-07 01:36:53\", \"status\": \"【广州市】 快件到达 【广州中心】\" &#125;, &#123; \"time\": \"2018-03-07 00:40:57\", \"status\": \"【广州市】 快件离开 【广州花都】 发往 【石家庄中转】\" &#125;, &#123; \"time\": \"2018-03-07 00:01:55\", \"status\": \"【广州市】 【广州花都】（020-37738523） 的 马溪 （18998345739） 已揽收\" &#125;], \"deliverystatus\": \"3\", /* 0：快递收件(揽件)1.在途中 2.正在派件 3.已签收 4.派送失败 5.疑难件 6.退件签收 */ \"issign\": \"1\", /* 1.是否签收 */ \"expName\": \"中通快递\", /* 快递公司名称 */ \"expSite\": \"www.zto.com\", /* 快递公司官网 */ \"expPhone\": \"95311\", /* 快递公司电话 */ \"courier\": \"容晓光\", /* 快递员 或 快递站(没有则为空)*/ \"courierPhone\":\"13081105270\", /* 快递员电话 (没有则为空) */ \"updateTime\":\"2019-08-27 13:56:19\", /* 快递轨迹信息最新时间 */ \"takeTime\":\"2天20小时14分\", /* 发货到收货消耗时长 (截止最新轨迹) */ \"logo\":\"http://img3.fegine.com/express/zto.jpg\" /* 快递公司LOGO */ &#125;&#125; 失败返回示例: 123456789&#123; \"status\": \"205\", /* status状态码见产品详情 */ \"msg\": \"没有信息\", \"result\": &#123; \"number\": \"1111ADECD1234\", \"type\": \"AUTO\", \"list\": [] &#125;&#125; 错误码定义: 错误码 错误信息 描述 201 快递单号错误 status：快递单号错误 203 快递公司不存在 status：快递公司不存在 204 快递公司识别失败 status：快递公司识别失败 205 没有信息 status：没有信息 207 该单号被限制，错误单号 status：该单号被限制，错误单号；一个单号对应多个快递公司，请求须指定快递公司 0 正常 status：正常查询 2.核心代码工具类: HttpUtils(官方提供) 下载地址 http://code.fegine.com/HttpUtils.zip TextUtil 用于判断输入发送的消息是否为英文字母+数字或纯数字(即符合快递运单号基本规则) 1234567public class TextUtil &#123; public static boolean DecText(String text)&#123; Pattern p=Pattern.compile(\"^[A-Za-z0-9]+$\"); //正则表达式 Matcher matcher = p.matcher(text); return matcher.matches(); &#125;&#125; ExpressUtil 调用API查询物流信息 1234567891011121314151617181920212223public class ExpressUtil &#123; public static String QueryExpress(String num) throws Exception &#123; String host = \"https://wuliu.market.alicloudapi.com\"; String path = \"/kdi\"; String method = \"GET\"; String appcode = \"06a9e928218141bxxxxxxx\"; // !!!替换填写自己的AppCode 在买家中心查看 Map&lt;String, String&gt; headers = new HashMap&lt;String, String&gt;(); headers.put(\"Authorization\", \"APPCODE \" + appcode); //格式为:Authorization:APPCODE 83359fd73fe11248385f570e3c139xxx Map&lt;String, String&gt; querys = new HashMap&lt;String, String&gt;(); querys.put(\"no\", num);// !!! 请求参数 HttpResponse response = HttpUtils.doGet(host, path, method, headers, querys); //System.out.println(response.toString()); //获取response的body String str = EntityUtils.toString(response.getEntity());//输出json JSONObject jsonObject = JSONObject.parseObject(str); // 获取到key为result的值 String result = jsonObject.getString(\"result\"); jsonObject = JSONObject.parseObject(result); // 获取到key为list的值 String list = jsonObject.getString(\"list\"); return list; &#125;&#125; 此时返回的数据为: 12345678910111213141516171819202122[&#123; \"time\": \"2020-02-29 19:45:12\", \"status\": \"快件由【浙江嘉善公司】发往【下一站浙江嘉兴转运中心】，扫描员【何海桃】\"&#125;, &#123; \"time\": \"2020-02-29 19:45:12\", \"status\": \"快件在【浙江嘉善公司】进行装车，扫描员【何海桃】，车签号【】\"&#125;, &#123; \"time\": \"2020-02-29 19:42:13\", \"status\": \"快件由【浙江嘉善公司】发往【下一站浙江嘉兴转运中心】，扫描员【何德文】\"&#125;, &#123; \"time\": \"2020-02-29 19:42:13\", \"status\": \"快件在【浙江嘉善公司】进行装包，扫描员【何德文】，袋号【9005261902881】\"&#125;, &#123; \"time\": \"2020-02-29 19:41:07\", \"status\": \"快件由【浙江嘉善公司】发往【下一站浙江嘉兴转运中心】，扫描员【何德文】\"&#125;, &#123; \"time\": \"2020-02-29 19:29:40\", \"status\": \"【浙江嘉善公司】的【公司称重（)】已收件，扫描员【公司出港1】\"&#125;, &#123; \"time\": \"2020-02-29 18:32:52\", \"status\": \"快件由【浙江嘉善公司】发往【下一站浙江嘉兴转运中心】，扫描员【何德文】\"&#125;] MsgService 1234567891011121314151617181920212223if (TextUtil.DecText(content)==true)&#123; String str = ExpressUtil.QueryExpress(content); List&lt;HashMap&gt; r = JSON.parseArray(str, HashMap.class); StringBuilder stringBuilder = new StringBuilder(); for (int i = r.size() - 1; i &gt;= 0; i--) &#123; System.out.println(r.get(i).get(\"time\") + \":\" + r.get(i).get(\"status\")); String string = r.get(i).get(\"time\") + \":\" + r.get(i).get(\"status\"); if (i == 0) &#123; stringBuilder.append(string); &#125; else &#123; stringBuilder.append(string).append(\"\\n\"); &#125; &#125; System.out.println(stringBuilder); //文本消息 TextMessage text = new TextMessage(); text.setContent(stringBuilder+\"\"); text.setToUserName(fromUserName); text.setFromUserName(toUserName); text.setCreateTime(LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))); text.setMsgType(MessageUtil.RESP_MESSAGE_TYPE_TEXT); respMessage = MessageUtil.textMessageToXml(text); &#125; 3.功能测试","categories":[],"tags":[{"name":"tool","slug":"tool","permalink":"https://imokkkk.github.io/tags/tool/"}]},{"title":"微信公众号开发(一)开发者接入微信公众号","slug":"微信公众号开发(一)开发者接入微信公众号","date":"2021-05-08T13:19:59.218Z","updated":"2020-04-29T02:44:38.014Z","comments":true,"path":"38103/","link":"","permalink":"https://imokkkk.github.io/38103/","excerpt":"微信公众号开发(一)开发者接入微信公众号1.前言该文章基于JDK1.8 springboot2.1.7.RELEASE环境 实现开发者第一次接入微信公众号后台的需求 2.准备工作2.1 进入微信公众平台注册账号 https://mp.weixin.qq.com/ 个人用户建议注册订阅号","text":"微信公众号开发(一)开发者接入微信公众号1.前言该文章基于JDK1.8 springboot2.1.7.RELEASE环境 实现开发者第一次接入微信公众号后台的需求 2.准备工作2.1 进入微信公众平台注册账号 https://mp.weixin.qq.com/ 个人用户建议注册订阅号 2.2 内网穿透因为要直接用内网本机开发调试，微信网页授权在回调时要访问本机，所以直接做个内网穿透，可以直接在外网访问到本机，做法如下： 登录 https://natapp.cn/ （我用的natapp.cn，你可以用其他类似的，个人感觉这个不错） 购买隧道：购买后使用方式: 参考官方教程：https://natapp.cn/article/natapp_newbie 使用后会得到natapp分配的网址，如 xxx.natapp.cn，这个地址就可以访问到开发本机。 下载并配置config.ini, 运行natapp 3.接入认证成为开发者 可参考微信官方开发文档 https://developers.weixin.qq.com/doc/offiaccount/Basic_Information/Access_Overview.html 3.1 填写服务器配置登录微信公众号开发平台:https://mp.weixin.qq.com/ 开发—开发者工具—公众平台测试账号 Tips: 微信公众号接口必须以http://或https://开头，分别支持80端口和443端口！ 这里的url可以选择自己买的服务器地址，记得必须开放80端口去使用！ 或者使用内网映射外网工具生成一个域名地址供给你开发使用，此方法自行百度，如下就是其中一种使用~ 目前提交是无法配置成功的, 不要着急 3.2 提交验证URL有效性3.2.1 搭建SpingBoot工程项目结构 pom.xml 123456789101112131415161718192021&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.7.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.binarywang&lt;/groupId&gt; &lt;artifactId&gt;weixin-java-mp&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; application.yaml 123456wechat: mpAppId: xxxxxxxx #公众平台测试账号---测试号信息, 目前可以不填 mpAppSecret: xxxxxx #公众平台测试账号---测试号信息, 目前可以不填 mpToken: xxxxx #与前面在公众平台测试账号---接口配置信息所填写保持一致server: port: 80 #端口号 sha1加密工具类 1234567891011121314151617181920public class SecurityUtil &#123; public static String sha1(String str) &#123; try &#123; StringBuilder sb = new StringBuilder(); MessageDigest digest = MessageDigest.getInstance(\"sha1\"); // 放入加密字符串 digest.update(str.getBytes()); // 进行加密 byte[] digestMsg = digest.digest(); // byte转换16进制 for (byte b : digestMsg) &#123; sb.append(String.format(\"%02x\", b)); &#125; return sb.toString(); &#125; catch (NoSuchAlgorithmException e) &#123; e.printStackTrace(); &#125; return str; &#125;&#125; 12345678@Data@Component@ConfigurationProperties(prefix = \"wechat\")public class WechatAccountConfig &#123; private String mpAppId; private String mpAppSecret; private String mpToken;&#125; Controller 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Slf4j@RestController@RequestMapping(\"/wechat/index\")public class IndexController &#123; @Autowired private WechatAccountConfig wechatAccountConfig; /** * 处理微信认证：验证服务器地址的有效性，get提交 * signature: 微信加密签名，signature结合了开发者填写的token参数和请求中的timestamp参数、nonce参数。 * timestamp 时间戳 * nonce: 随机数 * echostr: 随机字符串 */ @GetMapping public void checkSignature(HttpServletRequest request, HttpServletResponse response) throws IOException, IOException &#123; System.out.println(\"============= 处理微信认证 ===============\"); // 拿到微信的请求参数 String signature = request.getParameter(\"signature\"); String timestamp = request.getParameter(\"timestamp\"); String nonce = request.getParameter(\"nonce\"); String echostr = request.getParameter(\"echostr\"); // TODO 这里的token是微信公众平台上自己所配的！ String token = wechatAccountConfig.getMpToken(); // ① 将token、timestamp、nonce三个参数进行字典序排序 b a d c h ==&gt;a b c d h String[] strArr = &#123;token, timestamp, nonce&#125;; // 字典排序 Arrays.sort(strArr); // ② 将三个参数字符串拼接成一个字符串进行sha1加密 StringBuffer sb = new StringBuffer(); // 字符串拼接 for (String str : strArr) &#123; sb.append(str); &#125; // 加密 String sha1Str = SecurityUtil.sha1(sb.toString()); // ③ 开发者获得加密后的字符串可与signature对比，标识该请求来源于微信 if (sha1Str.equals(signature)) &#123; // 如果相等，就是来自微信请求 // 若确认此次GET请求来自微信服务器，原样返回echostr参数内容，则接入生效 response.getWriter().println(echostr); &#125; &#125;&#125; 启动类 123456@SpringBootApplicationpublic class WeChatService &#123; public static void main(String[] args) &#123; SpringApplication.run(WeChatService.class); &#125;&#125; 3.2.2 测试 启动该SpringBoot项目 回到公众平台测试账号—接口配置信息, 点击提交即可","categories":[],"tags":[{"name":"tool","slug":"tool","permalink":"https://imokkkk.github.io/tags/tool/"}]},{"title":"常用设计模式","slug":"常用设计模式","date":"2021-05-08T13:19:59.217Z","updated":"2020-04-29T02:44:38.055Z","comments":true,"path":"14808/","link":"","permalink":"https://imokkkk.github.io/14808/","excerpt":"Java 中一般认为有23种设计模式, 下面介绍几种常见的设计模式。总体来说设计模式分为三大类： 创建型模式, 共5五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共7种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共11种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。","text":"Java 中一般认为有23种设计模式, 下面介绍几种常见的设计模式。总体来说设计模式分为三大类： 创建型模式, 共5五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共7种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共11种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 1.单例模式所谓的单例设计指的是一个类只允许产生一个实例化对象。最好理解的一种设计模式，分为懒汉式和饿汉式。 饿汉式: 构造方法私有化，外部无法产生新的实例化对象，只能通过static方法取得实例化对象 123456789101112131415161718class Singleton &#123; /** * 在类的内部可以访问私有结构，所以可以在类的内部产生实例化对象 */ private static Singleton instance = new Singleton(); /** * private 声明构造 */ private Singleton() &#123; &#125; /** * 返回对象实例 */ public static Singleton getInstance() &#123; return instance; &#125;&#125; 懒汉式: 当第一次去使用Singleton对象的时候才会为其产生实例化对象的操作 123456789101112131415161718192021222324class Singleton &#123; /** * 声明变量 */ private static volatile Singleton singleton = null; /** * 私有构造方法 */ private Singleton() &#123; &#125; public static Singleton getInstance() &#123; // 还未实例化 if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; ​ 当多个线程并发执行 getInstance() 方法时，懒汉式会存在线程安全问题，所以用到了 synchronized 来实现线程的同步，当一个线程获得锁的时候其他线程就只能在外等待其执行完毕。而饿汉式则不存在线程安全的问题。 适用场景： 单例模式只允许创建一个对象，因此节省内存，加快对象访问速度，因此对象需要被公用的场合适合使用，如多个模块使用同一个数据源连接对象等等。如： (1) 需要频繁实例化然后销毁的对象。 (2) 创建对象时耗时过多或者耗资源过多，但又经常用到的对象。 (3) 有状态的工具类对象。 (4) 频繁访问数据库或文件的对象。以下都是单例模式的经典使用场景： (1) 资源共享的情况下，避免由于资源操作时导致的性能或损耗等。如上述中的日志文件，应用配置。 (2) 控制资源的情况下，方便资源之间的互相通信。如线程池等。 2.观察者模式一个对象(subject)被其他多个对象(observer)所依赖。则当一个对象变化时，发出通知，其它依赖该对象的对象都会收到通知，并且随着变化。 3.装饰者模式对已有的业务逻辑进一步的封装, 使其增加额外的功能, 要求装饰对象和被装饰对象实现同一个接口，装饰对象持有被装饰对象的实例。 适用环境: ​ (1) 在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责。 ​ (2) 处理那些可以撤消的职责。 ​ (3) 当不能采用生成子类的方法进行扩充时。一种情况是，可能有大量独立的扩展，为支持每一种组合将产生大量的 子类，使得子类数目呈爆炸性增长。另一种情况可能是因为类定义被隐藏，或类定义不能用于生成子类。 4.适配器模式适配器模式（Adapter Pattern）是作为两个不兼容的接口之间的桥梁。这种类型的设计模式属于结构型模式，它结合了两个独立接口的功能。 使用场景：有动机地修改一个正常运行的系统的接口，这时应该考虑使用适配器模式。 注意事项：适配器不是在详细设计时添加的，而是解决正在服役的项目的问题。 5.工厂模式5.1 简单工厂模式简单工厂模式就是把对类的创建初始化全都交给一个工厂来执行，而用户不需要去关心创建的过程是什么样的，只用告诉工厂我想要什么就行了。而这种方法的缺点也很明显，违背了设计模式的开闭原则，因为如果你要增加工厂可以初始化的类的时候，你必须对工厂进行改建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 抽象产品类abstract class Car &#123; public void run(); public void stop();&#125; // 具体实现类class Benz implements Car &#123; public void run() &#123; System.out.println(\"Benz开始启动了。。。。。\"); &#125; public void stop() &#123; System.out.println(\"Benz停车了。。。。。\"); &#125;&#125; class Ford implements Car &#123; public void run() &#123; System.out.println(\"Ford开始启动了。。。\"); &#125; public void stop() &#123; System.out.println(\"Ford停车了。。。。\"); &#125;&#125; // 工厂类class Factory &#123; public static Car getCarInstance(String type) &#123; Car c = null; if (\"Benz\".equals(type)) &#123; c = new Benz(); &#125; if (\"Ford\".equals(type)) &#123; c = new Ford(); &#125; return c; &#125;&#125; public class Test &#123; public static void main(String[] args) &#123; Car c = Factory.getCarInstance(\"Benz\"); if (c != null) &#123; c.run(); c.stop(); &#125; else &#123; System.out.println(\"造不了这种汽车。。。\"); &#125; &#125; 5.2 工厂方法模式设计一个工厂的接口，你想要什么东西，就写个类继承于这个工厂，这样就不用修改什么，直接添加就行了。就相当于，我这个工厂是用来生汽车的，而要什么品牌的汽车具体分到了每个车间，如果新多了一种品牌的汽车，直接新增一个车间就行了。那么问题又来了，如果想要生产大炮怎么办？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 抽象产品角色public interface Moveable &#123; void run();&#125; // 具体产品角色public class Plane implements Moveable &#123; @Override public void run() &#123; System.out.println(\"plane....\"); &#125;&#125; public class Broom implements Moveable &#123; @Override public void run() &#123; System.out.println(\"broom.....\"); &#125;&#125; // 抽象工厂public abstract class VehicleFactory &#123; abstract Moveable create();&#125; // 具体工厂public class PlaneFactory extends VehicleFactory &#123; public Moveable create() &#123; return new Plane(); &#125;&#125; public class BroomFactory extends VehicleFactory &#123; public Moveable create() &#123; return new Broom(); &#125;&#125; // 测试类public class Test &#123; public static void main(String[] args) &#123; VehicleFactory factory = new BroomFactory(); Moveable m = factory.create(); m.run(); &#125;&#125; 5.3 抽象工厂模式与工厂方法模式不同的是，工厂方法模式中的工厂只生产单一的产品，而抽象工厂模式中的工厂生产多个产品 123456789101112131415161718192021222324252627282930313233//抽象工厂类public abstract class AbstractFactory &#123; public abstract Vehicle createVehicle(); public abstract Weapon createWeapon(); public abstract Food createFood();&#125;//具体工厂类，其中Food,Vehicle，Weapon是抽象类，public class DefaultFactory extends AbstractFactory&#123; @Override public Food createFood() &#123; return new Apple(); &#125; @Override public Vehicle createVehicle() &#123; return new Car(); &#125; @Override public Weapon createWeapon() &#123; return new AK47(); &#125;&#125;//测试类public class Test &#123; public static void main(String[] args) &#123; AbstractFactory f = new DefaultFactory(); Vehicle v = f.createVehicle(); v.run(); Weapon w = f.createWeapon(); w.shoot(); Food a = f.createFood(); a.printName(); &#125;&#125;","categories":[],"tags":[{"name":"javase","slug":"javase","permalink":"https://imokkkk.github.io/tags/javase/"}]},{"title":"四种常见的排序算法","slug":"四种常见的排序算法","date":"2021-05-08T13:19:59.192Z","updated":"2020-04-29T02:44:38.011Z","comments":true,"path":"23116/","link":"","permalink":"https://imokkkk.github.io/23116/","excerpt":"1.冒泡排序思想: 每一趟将待排序序列中最大元素移到最后，剩下的为新的待排序序列, 重复上述步骤直到排完所有元素。这只是冒泡排序的一种, 当然也可以从后往前排 平均时间复杂度: O(n^2)","text":"1.冒泡排序思想: 每一趟将待排序序列中最大元素移到最后，剩下的为新的待排序序列, 重复上述步骤直到排完所有元素。这只是冒泡排序的一种, 当然也可以从后往前排 平均时间复杂度: O(n^2) 1234567891011121314151617181920public class Demo &#123; public void bubbleSort(int arr[]) &#123; for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] arr=&#123;4, 8, 7, 5, 6, 3, 1&#125;; Demo d = new Demo(); d.selectSort(arr); System.out.println(Arrays.toString(arr)); &#125;&#125; 2.选择排序思想: 每一趟从待排序序列选择一个最小的元素放在已排好序列的末尾, 剩下的为待排序序列, 重复上述步骤直至完成排序 平均时间复杂度: O(n^2) 123456789101112131415161718192021public void selectSort(int arr[]) &#123; //选择 for (int i = 0; i &lt; arr.length; i++) &#123; //默认第一个是最小的。 int min = arr[i]; //记录最小的下标 int index = i; //通过与后面的数据进行比较得出，最小值和下标 for (int j = i + 1; j &lt; arr.length; j++) &#123; if (min &gt; arr[j]) &#123; min = arr[j]; index = j; &#125; &#125; //然后将最小值与本次循环的，开始值交换 int temp = arr[i]; arr[i] = min; arr[index] = temp; //说明：将i前面的数据看成一个排好的队列，i后面的看成一个无序队列。每次只需要找无需的最小值，做替换 &#125;&#125; 3.插入排序思想: 1. 默认从第二个数据开始比较。 2.如果第二个数据比第一个小，则交换。然后在用第三个数据比较，如果比前面小，则插入（狡猾）。否则，退出循环 3.说明：默认将第一数据看成有序列表，后面无序的列表循环每一个数据，如果比前面的数据小则插入（交换）。否则退出 平均时间复杂度: O(n^2) 1234567891011121314151617public void insertSort(int arr[]) &#123; //插入排序 for (int i = 1; i &lt; arr.length; i++) &#123; //外层循环，从第二个开始比较 for (int j = i; j &gt; 0; j--) &#123; //内存循环，与前面排好序的数据比较，如果后面的数据小于前面的则交换 if (arr[j] &lt; arr[j - 1]) &#123; int temp = arr[j - 1]; arr[j - 1] = arr[j]; arr[j] = temp; &#125; else &#123; //如果不小于，说明插入完毕，退出内层循环 break; &#125; &#125; &#125;&#125; 4.快速排序采用分治法的思想：首先设置一个轴值pivot，然后以这个轴值为划分基准将待排序序列分成比pivot大和比pivot小的两部分，接下来对划分完的子序列进行快排直到子序列为一个元素为止。 平均时间复杂度: O(n*log(n)) 123456789101112131415161718192021222324252627282930public class Demo &#123; public void quickSort(int arr[], int low, int high) &#123; //pivot:位索引;p_pos:轴值 int pivot, p_pos, i, temp; if (low &lt; high) &#123; p_pos = low; pivot = arr[p_pos]; for (i = low + 1; i &lt;= high; i++) &#123; if (arr[i] &gt; pivot) &#123; p_pos++; temp = arr[p_pos]; arr[p_pos] = arr[i]; arr[i] = temp; &#125; &#125; temp = arr[low]; arr[low] = arr[p_pos]; arr[p_pos] = temp; //分而治之 quickSort(arr, low, p_pos - 1);//排序左半部分 quickSort(arr, p_pos + 1, high);//排序右半部分 &#125; &#125; public static void main(String[] args) &#123; int[] arr = &#123;4, 8, 7, 5, 6, 3, 1&#125;; Demo d = new Demo(); d.quickSort(arr, 0, arr.length - 1); System.out.println(Arrays.toString(arr)); &#125;&#125; 注: 各排序时间复杂度 排序方法 平均情况 最好情况 最坏情况 辅助空间 稳定性 冒泡排序 O(n^2) O(n) O(n^2) O(1) 稳定 选择排序 O(n^2) O(n^2) O(n^2) O(1) 不稳定 插入排序 O(n^2) O(n) O(n^2) O(1) 稳定 希尔排序O(n*log(n))~O(n^2) O(n^1.3) O(n^2) O(1) 不稳定 堆排序 O(nlog(n)) O(nlog(n)) O(n*log(n)) O(1) 不稳定 归并排序 O(nlog(n)) O(nlog(n)) O(n*log(n)) O(n) 稳定 快速排序 O(nlog(n)) O(nlog(n)) O(n^2) O(1) 不稳定","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://imokkkk.github.io/tags/面试/"}]},{"title":"使用tree命令导出文件夹/文件的目录树","slug":"使用tree命令导出文件夹或文件的目录树","date":"2021-05-08T13:19:59.188Z","updated":"2020-05-01T13:37:12.901Z","comments":true,"path":"shell_tree/","link":"","permalink":"https://imokkkk.github.io/shell_tree/","excerpt":"Windows和Linux都有tree命令，主要功能是创建文件列表，将所有文件以树的形式列出来 1.Windows的tree命令使用tree命令导出windows的文件夹/文件的目录树","text":"Windows和Linux都有tree命令，主要功能是创建文件列表，将所有文件以树的形式列出来 1.Windows的tree命令使用tree命令导出windows的文件夹/文件的目录树 1234TREE [drive:][path] [/F] [/A]/F 显示每个文件夹中文件的名称。（带扩展名）/A 使用 ASCII 字符，而不使用扩展字符。(如果要显示中文，例如 tree /f /A &gt;tree.txt)tree /f &gt; list.txt 将带扩展名的文件目录输出到list.txt文件中 2.Linux的tree命令Linux下的tree就比较强大了，但一般系统并不自带这个命令，需要手动下载安装：sudo apt-get install tree 。 1234567891011121314151617181920-a 显示所有文件和目录。-A 使用ASNI绘图字符显示树状图而非以ASCII字符组合。-C 在文件和目录清单加上色彩，便于区分各种类型。-d 显示目录名称而非内容。-D 列出文件或目录的更改时间。-f 在每个文件或目录之前，显示完整的相对路径名称。-F 在执行文件，目录，Socket，符号连接，管道名称名称，各自加上\"*\",\"/\",\"=\",\"@\",\"|\"号。-g 列出文件或目录的所属群组名称，没有对应的名称时，则显示群组识别码。-i 不以阶梯状列出文件或目录名称。-I 不显示符合范本样式的文件或目录名称。-l 如遇到性质为符号连接的目录，直接列出该连接所指向的原始目录。-n 不在文件和目录清单加上色彩。-N 直接列出文件和目录名称，包括控制字符。-p 列出权限标示。-P 只显示符合范本样式的文件或目录名称。-q 用\"?\"号取代控制字符，列出文件和目录名称。-s 列出文件或目录大小。-t 用文件和目录的更改时间排序。-u 列出文件或目录的拥有者名称，没有对应的名称时，则显示用户识别码。-x 将范围局限在现行的文件系统中，若指定目录下的某些子目录，其存放于另一个文件系统上，则将该子目录予以排除在寻找范围外。 3.测试执行命令行tree /f &gt; tree.txt","categories":[{"name":"shell","slug":"shell","permalink":"https://imokkkk.github.io/categories/shell/"}],"tags":[{"name":"tool","slug":"tool","permalink":"https://imokkkk.github.io/tags/tool/"}]},{"title":"Vue开发常用命令","slug":"Vue开发常用命令","date":"2021-05-08T13:19:59.186Z","updated":"2020-05-28T06:27:34.810Z","comments":true,"path":"Vue_shell/","link":"","permalink":"https://imokkkk.github.io/Vue_shell/","excerpt":"1.创建vue脚手架项目 1vue init webpack 项目名 2.axios 安装axios 1npm install axios --save-dev 配置main.js中引入axios","text":"1.创建vue脚手架项目 1vue init webpack 项目名 2.axios 安装axios 1npm install axios --save-dev 配置main.js中引入axios 123import axios from 'axios';Vue.prototype.$http=axios; 使用axios在需要发送异步请求的位置:123456this.$http.get( \"url\" ) .then(res =&gt; &#123; &#125;); 3.ElementUI 安装ElementUI 1npm i element-ui -S 配置main.js中引入ElementUI 12345import ElementUI from 'element-ui';import 'element-ui/lib/theme-chalk/index.css';//在vue脚手架中使用elementuiVue.use(ElementUI) 4.项目打包 打包在项目根目录中执行如下命令: 1npm run build 注意:vue脚手架打包的项目必须在服务器上运行不能直接双击运行 打包之后当前项目中变化在打包之后项目中出现dist目录,dist目录就是vue脚手架项目生产目录或者说是直接部署目录 与后端合并部署复制dist目录到Java工程resource下的static文件夹,index.html中修改路径 1234&lt;link href=/dist/static/css/app...../&gt;...&lt;script type=text/javascript src=/dist/static/js/ma....&gt;&lt;/script&gt;... 访问项目浏览器访问: http://localhost:8080/dist/index.html","categories":[{"name":"前端","slug":"前端","permalink":"https://imokkkk.github.io/categories/前端/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"https://imokkkk.github.io/tags/Vue/"}]},{"title":"top K问题的优化解法","slug":"top-K问题的优化解法","date":"2021-05-08T13:19:59.185Z","updated":"2020-04-29T02:44:38.006Z","comments":true,"path":"1060/","link":"","permalink":"https://imokkkk.github.io/1060/","excerpt":"top K问题的优化解法​ 在大规模数据处理中，经常会遇到的一类问题：在海量数据中找出出现频率最好的前k个数，或者从海量数据中找出最大的前k个数，这类问题通常被称为top K问题。例如，在搜索引擎中，统计搜索最热门的10个查询词；在歌曲库中统计下载最高的前10首歌等。","text":"top K问题的优化解法​ 在大规模数据处理中，经常会遇到的一类问题：在海量数据中找出出现频率最好的前k个数，或者从海量数据中找出最大的前k个数，这类问题通常被称为top K问题。例如，在搜索引擎中，统计搜索最热门的10个查询词；在歌曲库中统计下载最高的前10首歌等。 100亿数据找出最大的1000个数字 对于海量数据处理，思路基本上是：必须分块处理，然后再合并起来。 1.局部淘汰法​ 用一个容器保存前1000个数，然后将剩余的所有数字一一与容器内的最小数字相比，如果所有后续的元素都比容器内的1000个数还小，那么容器内这个1000个数就是最大1000个数。如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为O(n+m^2)，其中m为容器的大小。 ​ 这个容器可以用（小顶堆）最小堆来实现。我们知道完全二叉树有几个非常重要的特性，就是假如该二叉树中总共有N个节点，那么该二叉树的深度就是log2N，对于小顶堆来说移动根元素到底部或者移动底部元素到根部只需要log2N，相比N来说时间复杂度优化太多了（1亿的logN值是26-27的一个浮点数）。基本的思路就是先从文件中取出1000个元素构建一个小顶堆数组k，然后依次对剩下的100亿-1000个数字进行遍历m，如果m大于小顶堆的根元素，即k[0]，那么用m取代k[0]，对新的数组进行重新构建组成一个新的小顶堆。这个算法的时间复杂度是O((100亿-1000)log(1000))，即O((N-M)logM)，空间复杂度是M 这个算法优点是性能尚可，空间复杂度低，IO读取比较频繁，对系统压力大。 2.分治法(1) 将100亿个数据分为1000个大分区，每个区1000万个数据 (2) 每个大分区再细分成100个小分区。总共就有1000*100=10万个分区 (3) 计算每个小分区上最大的1000个数 (4) 合并每个大分区细分出来的小分区。每个大分区有100个小分区，我们已经找出了每个小分区的前1000个数。将这100个分区的1000*100个数合并，找出每个大分区的前1000个数 (5) 合并大分区。我们有1000个大分区，上一步已找出每个大分区的前1000个数。我们将这1000*1000个数合并，找出前1000.这1000个数就是所有数据中最大的1000个数 3.Hash法​ 如果这1亿个数据里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的1000个数。","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"https://imokkkk.github.io/tags/算法/"}]},{"title":"SQL执行慢分析及SQL语句优化","slug":"SQL执行慢分析及SQL语句优化","date":"2021-05-08T13:19:59.176Z","updated":"2020-04-29T02:44:37.936Z","comments":true,"path":"31546/","link":"","permalink":"https://imokkkk.github.io/31546/","excerpt":"1.SQL语句执行速度慢一个SQL语句执行的速度很慢, 分两种情况讨论: 大多数情况下很正常, 偶尔很慢, 则有如下原因: 数据库在刷新脏页 执行的时候, 遇到锁, 如表锁, 行锁 一直执行很慢, 则有如下原因","text":"1.SQL语句执行速度慢一个SQL语句执行的速度很慢, 分两种情况讨论: 大多数情况下很正常, 偶尔很慢, 则有如下原因: 数据库在刷新脏页 执行的时候, 遇到锁, 如表锁, 行锁 一直执行很慢, 则有如下原因 没有用上索引: 例如该字段没有索引, 由于对字段进行运算, 函数操作导致无法使用索引 数据库选错了索引 2.SQL语句的优化 设置合适的字段属性 字段的长度越小, 占用的内存越小, 性能就越好; 例如,中国的手机号码为11位, vachar的长度设置为11位即可 使用join语法 join语法分为内连接, 左(外)连接, 右(外)连接 尽量少使用select * select *会进行全表查询, 消耗的性能较大 在查找唯一一条数据的时候, 使用limit 1, 在查找到数据的时候会终止查找 使用limit分页 尽量少使用排序order by, order by DESC, order by ASC 避免进行类型转换 使用索引 优点: 加快索引速度 缺点: 创建索引和维护索引需要耗费时间和精力 ​ 索引需要占用空间 ​ 进行数据的增删改查时需要动态维护索引","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://imokkkk.github.io/tags/面试/"}]},{"title":"Spring AOP","slug":"Spring-AOP","date":"2021-05-08T13:19:59.159Z","updated":"2020-08-19T06:47:53.564Z","comments":true,"path":"spring_aop/","link":"","permalink":"https://imokkkk.github.io/spring_aop/","excerpt":"1.AOP概述1.1 什么是AOP？AOP(Aspect Oriented Programming 面向切面编程)，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。常用于日志记录，性能统计，安全控制，事务处理，异常处理等等。","text":"1.AOP概述1.1 什么是AOP？AOP(Aspect Oriented Programming 面向切面编程)，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。常用于日志记录，性能统计，安全控制，事务处理，异常处理等等。 1.2 AOP术语切面(Aspect)：由横切关注点构成的特殊对象。连接点(Join Point)：连接点是指在程序执行过程中某个特定的点，比如某方法调用的时候或者处理异常的时候；通知(Advice)：指在切面的某个特定的连接点上执行的动作。Spring切面可以应用5种通知： 前置通知(Before):在目标方法或者说连接点被调用前执行的通知； 后置通知(After)：指在某个连接点完成后执行的通知； 返回通知(After-returning)：指在某个连接点成功执行之后执行的通知； 异常通知(After-throwing)：指在方法抛出异常后执行的通知； 环绕通知(Around)：指包围一个连接点通知，在被通知的方法调用之前和之后执行自定义的方法。 切点(Pointcut)：指匹配连接点的断言。通知与一个切入点表达式关联，并在满足这个切入的连接点上运行，例如：当执行某个特定的名称的方法。引入(Introduction)：引入也被称为内部类型声明，声明额外的方法或者某个类型的字段。目标对象(Target Object)：目标对象是被一个或者多个切面所通知的对象。AOP代理(AOP Proxy)：向目标对象应用通知之后创建的对象。织入(Wearving)：增强添加到目标类具体连接点上的过程。AOP有三种织入的方式：编译期织入、类装载期织入、动态代理织入(spring采用动态代理织入)。 1.3 通知执行顺序 正常情况@Around -&gt;@Before-&gt;主方法体-&gt;@Around中pjp.proceed()-&gt;@After-&gt;@AfterReturning 存在异常 异常在Around中pjp.proceed()之前@Around -&gt; @After -&gt; @AfterThrowing 异常在Around中pjp.proceed()之后@Around -&gt;@Before-&gt;主方法体-&gt;@Around中pjp.proceed()-&gt;@After-&gt;@AfterThrowing1.4 实现原理 JDK动态代理(JDK提供，只能代理接口) 使用动态代理可以为一个或多个接口在运行期动态生成实现对象，生成的对象中实现接口的方法时可以添加增强代码，从而实现AOP。缺点是只能针对接口进行代理，另外由于动态代理是通过反射实现的，有时可能要考虑反射调用的开销。 CGLib动态代理: (适用CGLib工具)采用动态的字节码生成技术，运行时动态生成指定类的一个子类对象，并覆盖其中特定方法，覆盖方法时可以添加增强代码，从而实现AOP 。1.5 Pointcut切入点的语法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * 1、使用within表达式匹配 * 下面示例表示匹配com.example.controller包下所有的类的方法 */@Pointcut(\"within(com.example.controller..*)\")public void pointcutWithin()&#123;&#125;/** * 2、this匹配目标指定的方法，此处就是HelloController的方法 */@Pointcut(\"this(com.example.controller.HelloController)\")public void pointcutThis()&#123;&#125;/** * 3、target匹配实现UserInfoService接口的目标对象 */@Pointcut(\"target(com.leo.service.UserInfoService)\")public void pointcutTarge()&#123;&#125;/** * 4、bean匹配所有以Service结尾的bean里面的方法， * 注意：使用自动注入的时候默认实现类首字母小写为bean的id */@Pointcut(\"bean(*ServiceImpl)\")public void pointcutBean()&#123;&#125;/** * 5、args匹配第一个入参是String类型的方法 */@Pointcut(\"args(String, ..)\")public void pointcutArgs()&#123;&#125;/** * 6、@annotation匹配是@Controller类型的方法 */@Pointcut(\"@annotation(org.springframework.stereotype.Controller)\")public void pointcutAnnocation()&#123;&#125;/** * 7、@within匹配@Controller注解下的方法，要求注解的@Controller级别为@Retention(RetentionPolicy.CLASS) */@Pointcut(\"@within(org.springframework.stereotype.Controller)\")public void pointcutWithinAnno()&#123;&#125;/** * 8、@target匹配的是@Controller的类下面的方法，要求注解的@Controller级别为@Retention(RetentionPolicy.RUNTIME) */@Pointcut(\"@target(org.springframework.stereotype.Controller)\")public void pointcutTargetAnno()&#123;&#125;/** * 9、@args匹配参数中标注为@Sevice的注解的方法 */@Pointcut(\"@args(org.springframework.stereotype.Service)\")public void pointcutArgsAnno()&#123;&#125;/** * 10、使用excution表达式 * execution( * modifier-pattern? //用于匹配public、private等访问修饰符 * ret-type-pattern //用于匹配返回值类型，不可省略 * declaring-type-pattern? //用于匹配包类型 * name-pattern(param-pattern) //用于匹配类中的方法，不可省略 * throws-pattern? //用于匹配抛出异常的方法 * ) * * 下面的表达式解释为：匹配com.example.controller.HelloController类中以hello开头的修饰符为public返回类型任意的方法 */@Pointcut(value = \"execution(public * com.example.controller.HelloController.hello*(..))\")public void pointCut() &#123;&#125; 2.AOP实践2.1 HTTP接口鉴权需求： 可以定制地为某些指定的 HTTP RESTful api 提供权限验证功能。 当调用方的权限不符时, 返回错误。相关依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; 2.1.1 自定义注解1234@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface AuthChecker &#123;&#125; AuthChecker 注解是一个方法注解，它用于注解 RequestMapping 方法。 2.1.2 aspect的实现12345678910111213141516171819202122232425262728293031323334353637@Component@Aspectpublic class HttpAopAdviseDefine &#123; //定义一个 Pointcut, 使用切点表达式函数来描述对哪些Join point使用advice. @Pointcut(\"@annotation(com.example.annotation.AuthChecker)\") public void pointcut()&#123; &#125; //定义advice @Around(\"pointcut()\") public Object checkAuth(ProceedingJoinPoint proceedingJoinPoint)&#123; HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); //检查用户所传递的 token 是否合法 String token = getToken(request); if (!token.equalsIgnoreCase(\"111\"))&#123; return \"token不合法！\"; &#125; try &#123; return proceedingJoinPoint.proceed(); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); return null; &#125; &#125; private String getToken(HttpServletRequest request) &#123; Cookie[] cookies = request.getCookies(); if (cookies == null) &#123; return \"\"; &#125; for (Cookie cookie : cookies) &#123; if (cookie.getName().equalsIgnoreCase(\"token\")) &#123; return cookie.getValue(); &#125; &#125; return \"\"; &#125;&#125; 2.1.3 Controller12345678910111213@RestController@RequestMapping(\"/aop/http\")public class AopController &#123; @GetMapping(\"alive\") public String alive()&#123; return \"服务一切正常\"; &#125; @AuthChecker @GetMapping(\"login\") public String login()&#123; return \"登录成功！\"; &#125;&#125; 2.1.4 测试token缺失/不正确：token正确： 2.2 方法调用日志需求: 某个服务下的方法的调用需要有log记录调用的参数以及返回结果。 当方法调用出异常时，有特殊处理，例如打印异常 log，报警等。2.2.1 aspect 的实现123456789101112131415161718192021222324252627282930313233@Component@Aspectpublic class LogAopAdviseDefine &#123; private Logger logger = LoggerFactory.getLogger(getClass()); // 定义一个Pointcut, 使用切点表达式函数来描述对哪些 Join point 使用 advise. @Pointcut(\"within(com.example.service..*)\") public void poincut() &#123; &#125; // 定义advise @Before(\"poincut()\") public void logMethodInvokeParam(JoinPoint joinPoint) &#123; logger.info(\"---Before method &#123;&#125; invoke, param: &#123;&#125;---\", joinPoint.getSignature().toShortString(), joinPoint.getArgs()); &#125; @AfterReturning(pointcut = \"poincut()\",returning = \"message\") public void logMethodInvokeResult(JoinPoint joinPoint,Object message)&#123; logger.info(\"---After method &#123;&#125; invoke, result: &#123;&#125;---\", joinPoint.getSignature().toShortString(), joinPoint.getArgs()); &#125; @AfterThrowing(pointcut = \"poincut()\",throwing = \"exception\") public void logMethodInvokeException(JoinPoint joinPoint,Exception exception)&#123; logger.info(\"---method &#123;&#125; invoke exception: &#123;&#125;---\", joinPoint.getSignature().toShortString(), exception.getMessage()); &#125;&#125; 2.2.2 Service1234567891011121314151617@Servicepublic class LogServiceImpl implements LogService &#123; private Logger logger = LoggerFactory.getLogger(getClass()); private Random random = new Random(System.currentTimeMillis()); @Override public int LogMethod(String param) &#123; logger.info(\"---LogService: logMethod invoked, param: &#123;&#125;---\", param); return random.nextInt(); &#125; @Override public void exceptionMethod() throws Exception &#123; logger.info(\"---LogService: exceptionMethod invoked---\"); throw new Exception(\"Something bad happened!\"); &#125;&#125; 123456789@Servicepublic class NormalServiceImpl implements NormalService &#123; private Logger logger = LoggerFactory.getLogger(getClass()); @Override public void normalMethod() &#123; logger.info(\"---NormalService: someMethod invoked---\"); &#125;&#125; 2.2.3 测试123456789101112131415161718192021@RunWith(SpringRunner.class)@SpringBootTestpublic class LogAdviseTest &#123; @Autowired private LogService logService; @Autowired private NormalService normalService; @Test @PostConstruct public void testLogAdvise()&#123; logService.LogMethod(\"LogMethod Test!\"); try &#123; logService.exceptionMethod(); &#125;catch (Exception e)&#123; &#125; normalService.normalMethod(); &#125;&#125; 2.3 方法耗时统计需求: 为服务中的每个方法调用进行调用耗时记录. 将方法调用的时间戳, 方法名, 调用耗时上报到监控平台2.3.1 aspect 实现12345678910111213141516171819202122232425262728293031323334353637383940@Component@Aspectpublic class ExpiredAopAdviseDefine &#123; private Logger logger = LoggerFactory.getLogger(getClass()); @Pointcut(\"within(com.example.service.impl.ExpiredServiceImpl)\") public void pointcut() &#123; &#125; @Around(\"pointcut()\") public Object methodInvokeExpiredTime( ProceedingJoinPoint proceedingJoinPoint) &#123; try &#123; // StopWatch 任务执行时间监视器 StopWatch stopWatch = new StopWatch(); // 开始 stopWatch.start(); Object proceed = proceedingJoinPoint.proceed(); // 结束 stopWatch.stop(); // 上报到监控平台 reportToMonitorSystem( proceedingJoinPoint.getSignature().toShortString(), stopWatch.getTotalTimeMillis()); return proceed; &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); return null; &#125; &#125; public void reportToMonitorSystem(String methodName, long expiredTime) &#123; logger.info(\"---method &#123;&#125; invoked, expired time: &#123;&#125; ms---\", methodName, expiredTime); &#125;&#125; 2.3.2 Service123456789101112131415161718@Servicepublic class ExpiredServiceImpl implements ExpiredService &#123; private Logger logger = LoggerFactory.getLogger(getClass()); private Random random = new Random(System.currentTimeMillis()); @Override public void expiredTimeMethod() &#123; logger.info(\"---SomeService: someMethod invoked---\"); try &#123; //模拟耗时任务 Thread.sleep(random.nextInt(500)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2.3.3 测试123456789101112@RunWith(SpringRunner.class)@SpringBootTestpublic class ExpiredAdviseTest &#123; @Autowired private ExpiredService expiredService; @Test @PostConstruct public void testExpiredTime() &#123; expiredService.expiredTimeMethod(); &#125;&#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/tags/Spring/"},{"name":"AOP","slug":"AOP","permalink":"https://imokkkk.github.io/tags/AOP/"},{"name":"面向切面编程","slug":"面向切面编程","permalink":"https://imokkkk.github.io/tags/面向切面编程/"}]},{"title":"Sping Cloud Alibaba","slug":"Sping-Cloud-Alibaba","date":"2021-05-08T13:19:59.157Z","updated":"2021-03-17T14:58:25.266Z","comments":true,"path":"SpringCloudAlibaba/","link":"","permalink":"https://imokkkk.github.io/SpringCloudAlibaba/","excerpt":"1.主要功能 服务限流降级 服务注册与发现 分布式配置管理 消息驱动能力 分布式事务","text":"1.主要功能 服务限流降级 服务注册与发现 分布式配置管理 消息驱动能力 分布式事务 Spring Cloud Alibaba使用@GlobalTransactional注解，高效并且零入侵地解决分布式事务问题。 2.主要组件 SentinelSentinel把流量作为切入点，从流量控制、熔断降级、系统负载等多个维度保障服务的稳定性。Sentinel主要分为两部分，客户端在我们的程序中集成，控制台基于Spring Boot开发，打包后直接运行。利用Sertinel流量控制功能可以对网关、服务进行过载保护，另一个核心功能是熔断降级，与Hystrix一致。 NacosNacos是一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。主要功能有注册中心和配置中心。Nacos可以代替Eureka和Apollo。 RocketMQ分布式消息系统，基于高可用分布式集群技术，提供低延时、高可靠的消息发布与订阅服务。使用消息队列可以让服务之间更加解耦，还可以进行流量削峰，还可以用事务消息来实现分布式事务。 DubboJava RPC框架，Spring Cloud Alibaba体系中，服务之间的通信可以使用Dubbo进行远程调用。Rest的优势：通用性强、无语言限制、调试方便，但一般都是JSON格式，报文较大，Dubbo是二进制传输，性能更好。Spring Cloud Alibaba中可以将Dubbo和Feign结合使用，即服务可以暴露Dubbo协议，也可以暴露Rest协议，调用方选择对应的协议进行调用，对于性能要求高的使用Dubbo，其它的使用Rest。 Seata分布式事务解决方案，提供了AT、TCC等事务模式。 3.技术选型推荐 服务注册与发现：Nacos 服务熔断限流：Sentinel 服务通信调用：Feign 配置中心：Nacos 服务网关：Spring Cloud Gateway 分布式事务：Seata 消息队列：RocketMQ 调用链监控：Sleuth+Zipkin 分布式任务调度：XXL-JOB","categories":[{"name":"微服务","slug":"微服务","permalink":"https://imokkkk.github.io/categories/微服务/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://imokkkk.github.io/tags/微服务/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://imokkkk.github.io/tags/Spring-Cloud/"}]},{"title":"Sping Cloud组件扩展","slug":"Sping Cloud组件扩展","date":"2021-05-08T13:19:59.156Z","updated":"2021-03-17T14:13:35.253Z","comments":true,"path":"SpringCloud_02/","link":"","permalink":"https://imokkkk.github.io/SpringCloud_02/","excerpt":"1.Apollo统一管理配置信息，增强配置管理的服务能力。使用配置中心管理配置后，可以将配置信息从项目转移到配置中心，一般一个项目会有一个唯一的标识ID，通过这个ID从配置中心获取对应的配置内容。","text":"1.Apollo统一管理配置信息，增强配置管理的服务能力。使用配置中心管理配置后，可以将配置信息从项目转移到配置中心，一般一个项目会有一个唯一的标识ID，通过这个ID从配置中心获取对应的配置内容。 拉取项目在启动的时候通过配置中心拉取配置信息。 推送在配置中心修改配置后，可以实时地推送给客户端进行更新。 解决的问题：每个节点都要重启、格式不规范、容易被错改、没有历史记录、安全性不高 1.1 主要功能 统一管理不同环境、不同集群的配置 配置修改实时生效，即热发布功能 版本发布管理 灰度发布 权限管理、发布审核、操作审计 提供Java和.Net原生客户端，轻松集成、操作简单 提供开放平台API 部署简单 Apollo 和 Spring Cloud Config 对比 1.2 概念介绍 应用应用指项目，标识用appId来指定，Spring Boot项目中建议直接配置在application.yml中。 环境Apollo客户端在运行时除了需要知道项目当前的身份标识，还需要知道当前项目对应的环境，从而根据环境去配置中心获取对应的配置。可以通过Java System Property或配置文件指定，目前支持的环境有Local、DEV、FAT(测试环境)、UAT(集成环境)、PRO(生产环境)。 集群不同的集群可以有不同的配置文件，可以通过Java System Property或配置文件来指定。 命名空间可以用来对配置做分类，不同类型的配置存放在不同的命名空间中，如数据库配置文件、消息队列配置、业务相关配置等。命名空间还可以让多个项目共用一份配置，如Redis集群。 权限控制 1.3 架构设计 Config Service服务于Client对配置的操作，提供配置的查询、更新接口(基于Http long polling) Admin Service服务于后台Portal(Web管理端)，提供配置管理接口 Meta ServerMeta Server是对Eureka的一个封装，提供了HTTP接口获取Admin Service和Config Service的服务信息。部署时和Config Service是在一个JVM进程中的，所以IP、端口和Config Service一致。 Eureka用于提供服务注册和发现，Config Service和Admin Service都会向Eureka注册服务。Eureka在部署时和Config Service在一个JVM进程中，即Config Service包含了Meta Server和Eureka。 Portal后台Web界面管理配置，通过Meta Server获取Admin Service服务列表(IP+Port)进行配置的管理，客户端做负载均衡。 CilentApollo提供的客户端，用于项目中对配置的获取、更新。通过Meta Server获取Config Service服务列表(IP+Port)进行配置的管理，客户端内做负载均衡。 工作流程： 注册、续约、取消，也就是服务注册的操作，Config Service和Admin Service都会注册到Eureka中。 服务发现的逻辑，Client需要指定所有的Config Service，Portal需要知道所有的Admin Service，然后才能发起对应的操作。查找服务列表是通过负载进行转发到Meta Server. Meta Server去Eureka中获取对应的服务列表。 当获取到对应的服务信息后，就可以直接发起远程调用了。 推送设计：在Portal中进行配置的编辑和发布操作后，Portal会调用Admin Service提供的接口进行发布操作。Admin Service收到请求后，发送ReleaseMessage给各个Config Service，通知Config Service配置发生了变化。Config Service收到ReleaseMessage后，通知对应的客户端，基于HTTP长连接实现。 消息设计：ReleaseMessage消息是通过MySQL实现了一个简单的消息队列。Admin Service在配置发布后会往ReleaseMessage表插入一条消息记录，Config Service会启动一个线程定时扫描ReleaseMessage表，去查看是否有新的消息记录。Config Service发现有新的消息记录，那么就会通知所有的消息监听器，消息监听器得到配置发布的信息后，则会通知对应的客户端。 客户端设计：客户端和服务端保持了一个长连接，编译配置的实时更新推送。定时拉取配置是客户端本地的一个定时任务，默认5分钟1次，也可以通过在运行时指定System Property:apollo.refreshInterval来进行覆盖，单位是分钟，采用推送+定时拉取的方式就等于双保险。客户端从Apollo配置中心服务端获取到应用的最新配置后，会保存在内存中。客户端会把从服务取到的配置在本地文件系统中缓存一份，当服务或网络不可用时可以使用本地配置，也就是本地开发模式env=Local。 2.分布式链路跟踪分布式链路跟踪的关键在于如何将请求经过的服务节点都关联起来。 2.1 核心概念 Span基本工作单元，如发送RPC请求是一个新的Span、发送HTTP请求是一个新的Span、内部方法调用也是一个新的Span。 Trace一次分布式调用的链路信息，每次调用链路信息都会在请求入口处生成一个TraceId。 Annotation用于记录事件的信息。在Annotation中会有CS、SR、SS、CR这些信息。 CSClient Sent，客户端发送一个请求，这个Annotation表示Span的开始。 SRServer Received，服务端获得请求并开始处理，用SR的时间减去CS的时间即网络延迟时间。 SSServer Sent，在请求处理完成时将响应发送回客户端，SR-SS，即服务端处理请求所需的时间。 CRClient Recevied，表示Span结束，客户端从服务端收到响应，CR-CS，即全部时间。 2.2 请求追踪过程分解 当一个请求访问SERVICE1时，此时没有Trace和Span，会生成Trace和Span，如图所示生成Trace ID是X，Span ID是A。 接着SERVICE1请求SERVICE2，这是一次远程请求，会生成一个新的Span，Span ID为B，Trace ID不变还是X。Span B处于CS状态，当请求到达SERVICE2后，SERVICE2有内部操作，生成了一个新的Span，Span ID为C，Trace ID不变。 SERVICE2处理完后向SERVICE3发起请求，生成新的Span，Span ID为D，Span D处于CS状态，SERVICE3接收请求后，Span D处于RS状态，同时SERVICE内部操作也会生成新的Span，Span ID为E。 SERVICE3处理完后，需要将结果响应给调用方，此时Span D处于SS状态，当SERVICE2收到响应后，Span D处于CR状态。 一次请求会经过多个服务，会产生多个Span，但Trace ID只有一个。 2.3 Spring Cloud Sleuth 可以添加链路信息到日志中 链路数据可直接上报给Zipkin 内置了很多框架的埋点，如Zuul、Feign、Hystrix 2.4 Zipkin收集数据、查询数据。 CollectorZipkin的数据收集器，进行数据验证、存储。 Storage存储组件，Zipkin默认在内存中存储数据，数据落地的话支持ElasticSearch和MySQL。 SearchZipkin的查询API，用于查找和检索数据，主要使用者为Web UI。 Web UI提供可视化的操作界面，直观的查询链路跟踪数据。 链路跟踪的信息会通过Transport传递给Zipkin的Collector，Transport支持的方式有HTTP和MQ进行传输。 2.5 Sleuth关联整个请求链路日志集成Spring Cloud Sleuth后，会在原始的日志加上一些链路的信息。 application name应用名称，即application.yml里的spring.application.name参数配置的属性。 traceId为请求分配的唯一请求号，用来标识一条请求链路。 spanId基本的工作单元，一个请求可以包含多个步骤，每个步骤有自己的Span ID，一次请求只有一个Trace ID和多个Span Id。 export布尔类型，表示是否将该信息输出到Zipkin进行收集和展示。 2.6 使用技巧 抽样采集数据spring.sleuth.sampler.probability=10请求次数：Zipkin数据条数=10：1 RabbitMQ代替HTTP发送调用链路数据删除配置spring.zipkin.base-url，在启动Zipkin服务时指定RabbitMQ信息： 1java -DRABBIT_ADDRESSES=192.168.10.124:5672 -DRABBIT_USER=admin -DRABBIT_PASSWORD=123456 -jar zipkin.jar ElasticSearch存储调用链数据启动Zipkin的时候指定存储类型为ES，指定ES的URL信息： 1java -DSTORAGE_TYPE=elasticsearch -DES_HOSTS=http://localhost:9200 -jar zipkin.jar 手动埋点检测性能 Hystrix埋点分析 3.微服务安全认证3.1 常用的认证方式 session用户登陆后将信息存储在服务端，客户端通过cookie中的sessionId来标识对应的用户。 缺点： 服务端需要保存每个用户的登录信息，如果用户量非常的，服务端的存储压力也会增大。 多节点时，通过负载均衡器进行转发，session可能会丢失。 解决办法：session复制，Nginx可以设置黏性Cookie来保证一个用户的请求只访问同一个节点；session集中存储，如存储在Redis中。 HTTP Basic AuthenticationHTTP基本认证，客户端会在请求头中增加Authorization，Authorization是用户名和密码Base64加密后的内容，服务端获取Authorization Header中的用户名与密码进行验证。 Token与HTTP Basic Authentication类似，与session不同，session只是一个key，会话信息存储在服务端。而Token中会存储用户的信息，然后通过加密算法进行加密，只有服务端才能解密，服务端拿到Token后进行解密获取用户信息。 3.2 JWT认证简介：JWT(JSON Web Token)如用户登录时，基本思路就是用户提供用户名和密码给认证服务器，服务器验证用户提交信息的合法性；如果验证成功，会产生并返回一个Token，用户可以使用这个Token访问服务器上受保护的资源。 JWT由三部分构成：头部(Header)、消息体(Payload)、签名(Signature) 1token = encodeBase64(header) + '.' + encodeBase64(payload) + '.' + encodeBase64(signature) 头部信息：令牌类型、签名算法 1&#123; \"alg\": \"HS256\", \"typ\": \"JWT\" &#125; 消息体：应用需要的信息，如用户的id 1&#123;\"id\": \"1234567890\", \"name\": \"John Doe\"&#125; 签名：用来判断消息在传递的路径上是否被篡改 1HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret) JWT认证流程：客户端需要调用服务端提供的认证接口来获取 Token。获取 Token 的流程如图所示，客户端会首先发起一个认证的请求到网关，网关会将请求转发到后端的用户服务中，在用户服务中验证身份后，就会根据用户的信息生成一个 Token 返回给客户端，这样客户端就获取了后面请求的通行证。然后，客户端会将获取的 Token 存储起来，在下次请求时带上这个 Token，一般会将 Token 放入请求头中进行传递。当请求到达网关后，会在网关中对 Token 进行校验，如果校验成功，则将该请求转发到后端的服务中，在转发时会将 Token 解析出的用户信息也一并带过去，这样在后端的服务中就不用再解析一遍 Token 获取的用户信息，这个操作统一在网关进行的。如果校验失败，那么就直接返回对应的结果给客户端，不会将请求进行转发。 在网关中，验证过滤器会对 /oauth/token 这个认证 API 进行放行，不进行验证。用户信息的全局传递扩展：不需要加参数，直接通过请求头进行传递，在服务内部通过ThreadLocal进行上下文传递。主要流程：从网关传递到后端服务，后端服务接受数据后存储到ThreadLocal中，服务会调用其它服务，如果用Feign调用可以利用Feign的拦截器传递数据，如果用RestTemplate的拦截器传递数据也一样。 3.3 Token的使用Token注销Token的有效期存储在Token本身中，只有解析出Token的信息，才能获取到Token的有效时间，不能修改。Token的有效期越短，安全性越高。还可以在用户退出登录时，进行Token的注销操作，如将注销的Token放入Redis中进行一层过滤，即在网关中验证Token有效性时先从Redis中判断Token是否存在，如果存在，直接拦截。Token放入Redis的过期时间一般会设置为Token剩余的有效时间。 使用建议 设置较短(合理)的过期时间 注销的Token及时清除(放入Redis中做一层过滤) 监控Token的使用频率 核心功能敏感操作可以使用动态验证(验证码) 网络环境、浏览器信息等识别 加密密钥支持动态修改 加密密钥支持动态修改 3.4 内部服务之间的认证 IP白名单如用户服务只能某些IP或IP段访问，IP白名单可以采用配置中心来存储，具备实时刷新的能力。 内部同样使用Token进行验证服务在启动时就可以在统一的认证服务中申请Token，申请需要的认证信息可以放在配置中心。这样服务启动时就有了能够访问其他服务的Token，在调用时带上Token，被调用的服务中进行Token的校验。对于Token的失效更新： 在请求时如果返回的Token已经失效，那么可以重新获取Token后再发起调用，这种在并发量大时需要加锁，不然会发生同时申请多个Token的情况。 定时更新，如Token有效期1个小时，那么定时任务可以50分钟更新一次。","categories":[{"name":"微服务","slug":"微服务","permalink":"https://imokkkk.github.io/categories/微服务/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://imokkkk.github.io/tags/微服务/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://imokkkk.github.io/tags/Spring-Cloud/"}]},{"title":"Sping Cloud相关组件了解","slug":"Sping Cloud相关组件了解","date":"2021-05-08T13:19:59.154Z","updated":"2021-03-15T02:09:38.855Z","comments":true,"path":"SpringCloud_01/","link":"","permalink":"https://imokkkk.github.io/SpringCloud_01/","excerpt":"1.Eureka如果需要实现完整的服务注册与服务发现的功能，我们需要有注册中心来统一存储和管理服务信息，应用程序需要将自身的信息注册到注册中心，也就是服务提供者和服务消费者的信息。整个过程中包含的操作有注册、拉取、心跳、剔除等动作。","text":"1.Eureka如果需要实现完整的服务注册与服务发现的功能，我们需要有注册中心来统一存储和管理服务信息，应用程序需要将自身的信息注册到注册中心，也就是服务提供者和服务消费者的信息。整个过程中包含的操作有注册、拉取、心跳、剔除等动作。 注册中心：用来集中存储管理服务信息。 服务提供者：通过API供其他方调用服务。 服务消费者：需要调用其他方的API获取服务。 项目启动后Eureka Client会向Eureka Server发送请求，进行注册，并将自身信息发送给Eureka Server。注册成功后，每隔一定的时间，Eureka Client会向Eureka Server发送心跳来续约服务，汇报健康状态。如果客户端长时间没有续约，那么Eureka Server大约在90秒内从服务器注册表中清除客户端的信息。应用程序停止时Eureka Client会通知Eureka Server移除相关信息，信息移除成功后，对应的客户端会更新服务的信息，这样就不会调用已经下线的服务，但操作具有延迟，有可能会调到已经失效的服务，所以在客户端会开启失败重试功能来避免这个问题。Eureka Serve集群保证高可用，Eureka Server没有集成其它第三方存储，而是存储在内存中。所以Eureka Server之间会将注册信息复制到集群中的Eureka Serve的所有节点。这样数据才是共享状态，任何的Eureka Client都可以在任何一个Eureka Server节点查找到注册信息。 1.1 Eureka注册表Eureka的注册信息是存储在ConcurrentHashMap中的，Map的key是服务名称，value是一个Map。value的Map的key是服务实例的ID，value是Lease类，Lease中存储了实例的注册时间、上线时间等信息，还有具体的实例信息，如IP、端口、健康检查的地址等信息。 Eureka将注册的服务信息存储在内存中的原因：性能高；部署简单，不需要依赖于第三方存储。劣势：扩容难度高，每个Eureka Server都全量的存储一份注册表，假如存储空间不够了，需要扩容，那么所有的Eureka Server节点都必须扩容，必须采用的内存配置。 Eureka核心操作主要有注册、续约、下线、移除，接口为com.netflix.eureka.lease.LeaseManager，这些操作都是针对注册表的操作，也就是Map的操作。 1.2 自我保护机制自我保护机制是为了避免因网络分区故障而导致服务不可用的问题。自我保护机制带来的问题：若服务提供者B真的下线了，由于Eureka Serve自我保护机制打开，不会移除任务信息，当服务消费者对服务提供者B进行调用时，就会出错。出现某些有问题的实例没能及时移除掉的情况，服务消费者可以通过Ribbon来进行重试，保证调用能够成功。 自我保护开启的条件：AbstractInstanceRegistry中有两个字段，numberOfRenewsPerMinThreshold(期望最小每分钟能够续租的次数)、expectedNumberOfClientsSendingRenews(期望的服务实例数)。假如有10个实例，每个实例每分钟续约2次，那么10x2x0.85=17，即每分钟至少有17次续约才是正常的。 1.3 健康检查Eureka Client会定时发送心跳给Eureka Server来证明自己是否处于健康的状态。但某些场景下，服务处于存活状态，却已经不能对外提供服务，如数据库出问题了，但心跳正常，客户端在请求时还会请求到这个出问题的实例。可以在项目中集成Actuator，统一管理应用的健康状态，将这个状态反馈给Eureka Server。 2.Ribbon2.1 负载均衡负载均衡是一种基础的网络服务，它的核心原理是按照指定的负载均衡算法，将请求分配到后端服务集群上，从而为系统提供并行处理和高可用的能力。 集中式负载均衡：在消费者和提供者中间使用独立的代理方式进行负载，有硬件的负载均衡器，如F5，也有软件，如Nginx。客户端不需要关心对应服务实例的信息，只需要与负载均衡器进行交互，服务实例扩容或者缩容，客户端不需要修改任何代码。 客户端负载均衡：需要自己维护服务实例信息，然后通过某些负载均衡算法，从实例中选取一个实例，直接进行访问。区别：对服务实例信息的维护。集中式负载均衡的信息是集中进行维护的，如Nginx，都会在配置文件中进行指定。客户端负载均衡的信息是在客户端本地进行维护的，可以手动配置，最常见的是从注册中心拉取。 2.2 Ribbon使用方式： 原生API Ribbon+RestTemplate Ribbon+Feign通过给加了@LoadBalanced的RestTemplate添加拦截器，拦截器中通过Ribbon选取服务实例，然后将请求地址中的服务名称替换成Ribbon选取服务实例的IP和端口。 2.3 负载均衡策略 内置负载均衡策略 RoundRobinRule：轮询算法 RandomRule：随机算法 BestAvailableRule：选择一个最小的并发请求server，如果有A、B两个实例，当A有4个请求正在处理中，B有2个，下次请求会选择B，适用于服务所在机器配置相同的情况。 WeightedResponseTimeRule：根据请求的响应时间计算权重，如果响应时间越长，那么对应的权重越低，权重越低的服务器，被选择的可能性就越低。 自定义负载均衡算法 实现IRule接口或继承AbstractLoadBanlancerRule 实现choose方法 指定Ribbon的算法类使用场景： 定制与业务更匹配的策略。 灰度发布 多版本隔离 故障隔离 2.4 饥饿加载模式Ribbon在进行客户端负载均衡时并不是启动时就加载上下文，而是第一次请求时才去创建的，因此第一次调用会比较慢，有可能会引起调用超时。可以指定Ribbon客户端的名称，在启动时加载这些子应用程序上下文。 初始化后进行了缓存操作，getContext()方法中，如果在contexts中不存在才会创建，创建时会用synchronized加锁，并进行二次判断，防止并发下出现创建多次的问题，最后进行增加操作。如果有的话就直接从contexts获取返回。contexts就是一个ConcurrentHashMap。 3.Hystrix3.1 服务雪崩微服务架构下，会存在服务之间相互依赖调用的情况，当某个服务不可用时，很容易因为服务之间的依赖关系使故障扩大，甚至造成整个系统不可用，这种现象称为服务雪崩效应。 产生原因： 服务提供者代码的Bug问题，由于某些代码导致CPU飙升，将资源耗尽等；服务器出现问题，磁盘出问题，导致数据读写特别慢，拉高了响应时间；慢SQL语句问题；请求量太大，超出系统本身的承受能力。 服务消费者同步调用等待结果导致资源耗尽；自己既是服务消费者也是服务提供者。 解决方案： 服务提供者代码Bug问题：测试、Code Review等方式；慢SQL问题：数据库性能优化；服务器硬件故障问题：加大运维粒度，通过监控等手段提前预防；请求量超出承受能力：扩容或限流 服务消费者资源隔离、快速失败。 3.2 容错实现设计原则： 封装请求：将用户的操作进行统一封装，目的在于进行统一控制。 资源隔离：将对应的资源按照指定的类型进行隔离，如线程池和信号量。 失败回退：备用方案，当请求失败后，Hystrix会让用户自定义备用方案。 断路器：决定了请求是否需要真正的执行，如果断路器打开，那么所有的请求都将失败，执行回退逻辑。如果断路器关闭，那么请求将正常执行。 指标监控：对请求的生命周期进行监控。 工作原理： 构建一个HystrixCommand或者HystrixObservableCommand对象，将请求包装到Command对象中。 执行构建好的命令。 判断当前请求是否有缓存，如果有就直接返回缓存的内容。 判断断路器是否打开，如果打开，跳到第8步，获取fallback方法，执行fallback逻辑。如果没有打开，执行第5步。 如果是线程池隔离模式，判断线程池队列的容量；如果是信号量隔离模式，会判断信号量的值是否已经被使用完。如果线程池和信号量都已经满了，同样请求不被执行，直接跳到第8步。 执行HystrixObservableCommand.construct()或HystrixCommand.run()方法，正在执行的请求逻辑就封装在construct()或run()方法中。 请求过程中，若出现异常或者超时，会直接到第8步，执行成功就返回结果。执行结果会将数据上报给断路器，断路器会根据上报的数据来判断断路器是否打开 fallback 3.3 Hystrix使用 HystrixCommand注解方式 在Feign中使用 在Zuul中使用 Hystrix配置： 3.4 Hystrix隔离机制 线程池隔离：当用户请求A服务后，A服务需要调用其它服务，这个时候可以为不同的服务创建独立的线程池，假如A需要调用B和C，那么可以创建两个独立的线程池，将调用B服务的线程池丢入到一个线程池，将调用C服务的线程丢入另一个线程池，这样起隔离效果，就算其中某个线程池请求满了，无法处理请求了，对另一个线程池页没有影响。使用线程隔离。需要调整好线程池参数，否则和信号量一样，并发量大的时候性能上不去。设置最大线程数，默认为10，队列大小决定了能够堆积多少请求，但请求不能一直堆积，所有还需要设置一个阈值来进行拒绝。 信号量隔离：信号量就算一个计数器，如初始化是100，那么每次请求过来时信号量就会减1，当信号量计数为0时，请求就会被拒绝，等之前的请求处理完成后，信号量就会加1。起到了限流的作用，信号量隔离是在请求主线程中执行的。 线程池隔离的特点是Command运行在独立的线程池中，可以支持超时，是单独的线程，支持异步。信号量隔离运行在调用的主线程中，不支持超时，只能同步调用。 3.5 使用技巧 配置可以对接配置中心进行动态调整 回退逻辑中可以手动埋点或者通过输出日志进行告警 使用线程池隔离模式再用ThreadLocal会有坑被隔离的方法会包装成一个Command丢入到独立的线程中执行，这个时候就是从A线程切换到了B线程，ThreadLocal的数据就会丢失。 网关中尽量用信号隔离 插件机制可以实现很多扩展 Hystrix各种超时配置方式 commandKey、groupKey、threadPoolKey的使用在使用HystrixCommand注解时，会配置commandKey、groupKey、threadPoolKey。commandKey表示封装的command的名称，可以给指定的commandKey进行参数的设置。groupKey是将一组command进行分组，如果没有设置threadPoolKey的话，那么线程池的名称会用groupKey。threadPollKey是线程池的名称，多个command的threadPoolKey相同，那么会使用同一个线程池。 4.FeignFeign是一个声明式的REST客户端，Feign提供了HTTP请求的模板，通过编写简单的接口和插入注解，就可以定义好Http请求的参数、格式、地址等信息。Feign会完全代理HTTP请求，Spring Cloud对Feign进行了封装，使其支持SpringMVC标准注解和HttpMessageConverters。Feign可以与Eureka和Ribbon组合使用以支持负载均衡，与Hystrix组合使用，支持熔断回退。 4.1 重要组件 Contract 契约组件Contract允许用户自定义契约去解析注解信息，如在Spring Cloud中使用Feign，可以使用SpringMVC的注解来定义Feign的客户端。 Encoder 编码组件通过该组件可以将请求信息采用指定的编码方式来进行编码后传输。 Decoder 编码组件 Decoder将相应数据解码成对象。 ErrorDecoder 异常解码器当被调用方发生异常后，可以在ErrorDecoder中将响应的数据转换成具体的异常返回给调用方，适合内部服务之间调用，但不想通过指定的字段来判断是否成功的场景，直接用自定义异常代替。 Logger 日志记录Logger组件负责Feign中记录日志的，可以指定Logger的级别及自定义日志的输出。 Client 请求执行组件Client是负责HTTP请求执行的组件，Feign将请求信息封装好后会交由Client来执行，Feign中默认的Client是通过JDK的HttpURLConnection发起请求的，每次发起请求的适合，都会建立新的HttpURLConnection链接，性能很差。可以扩展该接口，使用Apache HttpClient等基于连接池的高性能HTTP客户端。 Retryer 重试组件Retryer是负责重试的组件，Feign内置了重试器，当HTTP请求出现IO异常时，Feign会限定最大重试次数来进行重试操作。 InvocationHandlerFactory 代理IncocationHandlerFactory采用JDK的动态代理方式生成代理对象，定义的Feign接口，当调用这个接口中定义的方法时，实际上是去调用远程的HTTP API，这里用了动态代理的方式，当调用某个方法时，会进入代理中正在的去调用远程HTTP API。 RequestInterceptor 请求拦截器可以为Feign添加多个拦截器，在请求执行前设置一些扩展的参数信息。 QueryMapEncoder 参数查询QueryMapEncoder是针对实体类参数查询的编码器，可以基于QueryMapEncoder将实体类生成对应的查询参数。 4.2 Feign执行过程定义对应的接口类，在接口类上使用Feign自带的注解来标识HTTP的参数信息，当调用接口对应的方法时，Feign内部会基于面向接口的动态代理方式生成实现类，将请求调用委托到动态代理实现类，负责动态代理的组件是InvocationHandlerFactory。根据Contract规则，解析接口类的注解信息，翻译成Feign内部能识别的信息。Spring Cloud OpenFeign中就扩展了SpringMVCContract。MethodHandler在执行的时候会生成Request对象，在构建Request对象的时候会为其设置拦截器，交由Client执行前记录一些日志，Client执行完成后也记录一些日志，然后使Decoder进行相应结果的解码操作，并返回结果。 4.3 使用技巧 继承特性将API的定义提取出来封装成一个单独的接口，给API的实现方和调用方共用。 拦截器添加自己的拦截器来实现某些场景下的需求，实现RequestInterceptor接口，在apply方法中编写自己的逻辑。 GET请求多参数传递一般超过3个以上的参数会封装在一个实体类中，在Spring Cloud Open Feign中要支持对象接收多个参数，需要增加@SpringQueryMap注解。 日志配置Feign日志级别： NONE：不输出日志 BASIC：只输出请求方法的URL和响应的状态码及执行的时间 HEADERS：将BASIC和请求头信息输出 FULL：会输出全部完整的请求信息 异常解码器 5.ZuulAPI网关是对外提供服务的一个入口，并且隐藏了内部架构的实现。可以为我们管理大量的API接口，负责对接用户、协议适配、安全认证、路由转发、流量限制、日志监控、防止爬虫、灰度发布等功能。 动态路由将客户端的请求路由到后端不同的服务上，如果没有网关去做统一的路由，那么客户端就要关注后端N个服务。 请求监控对整个系统的请求进行监控，详细的记录请求响应日志，可以实时的统计当前系统的访问量及监控状态。 认证鉴权统一对访问请求做认证，拒绝非法请求，保护后端的服务。 压力测试动态的将测试请求转发到后端服务的集群中，还可以识别测试流量和真实流量，用来做一些特殊的处理。 灰度发布当需要发布新版本时，通过测试请求对1.1版本的服务进行测试，若没发现问题，可以将正常的请求转发过来，若有问题，不影响用户使用的1.0版本。 5.1 过滤器过滤器可以对请求或响应结果进行处理，Zuul支持动态加载、编译、运行这些过滤器。 pre过滤器：可以在请求被路由器之前调用。适用于身份认证的场景，认证通过后再继续执行下个流程。 route过滤器：在路由请求时被调用。适用于灰度发布的场景，在将要路由的时候可以做一些自定义的逻辑。 post过滤器：在route和error过滤器之后被调用。将请求路由到达具体的服务之后执行，适用于添加响应头，记录响应日志等应用场景。 error过滤器：处理请求发生错误时被调用。在执行过程中发送错误时会进入error过滤器，可以用来统一记录错误信息。 自定义过滤器：继承ZuulFilter，然后重写ZuulFilter的四个方法shouldFilter方法决定了是否执行该过滤器，true为执行，false不执行，可以利用配置中心实现动态的开启或关闭过滤器。filterType方法是要返回过滤器的类型，可选择为pre、route、post、error四种类型。过滤器可以有多个，先后顺序可以通过filterOrder来指定过滤器的执行顺序，数字越小，优先级越高。业务逻辑写在run方法中。 5.2 请求生命周期请求先进入pre过滤器，在pre过滤器执行完后，进入routing过滤器，开始路由到具体的服务中，路由完成后，接着到了post过滤器，然后将请求结果返回给客户端。如果过程中出现异常，则会进入error过滤器。源码对应ZuulServlet。 5.3 Zuul容错与回退5.4 使用技巧 内置端点当@EnableZuulProxy与Spring Boot Actuator配合使用时，Zuul会暴露一个路由管理端点/routes。借助这个端点，可以直观的查看及管理Zuul的路由。还有一个/filters端点可以查看Zuul中所有过滤器的信息。 文件上传通过Zuul上传文件，超过1M的文件会上传失败，配置max-file-size和max-request-size，Zuul中需要配置，最终接受文件的服务也要配置。或在网关请求地址前面上加上/zuul，可以绕过Spring DispatcherServlet上传大文件。Zuul服务不用再配置，但接收文件的服务还是要配置文件上传大小。在上传大文件时，时间较长，可以设置Ribbon的ConnectTimeOut和ReadTimeOut。如果Zuul的Hystrix隔离模式为线程的话需要设置Hystrix的超时时间。 请求响应输出 Zuul Debug 跨域配置 关闭Zuul全局路由转发配置zuul.ignored-service=*关闭路由转发，配置zuui.ignoredPatterns忽略不想暴露的API。 动态过滤器定期扫描存放Groovy Filter文件的目录，如果发现有新Groovy Filter文件或者Groovy Filter源码有改动，那么就会对Groovy文件进行编译加载。首先在项目中增加Groovy的依赖，然后在项目启动后设置Groovy的动态加载任务，定时动态加载指定目录的Groovy文件。","categories":[{"name":"微服务","slug":"微服务","permalink":"https://imokkkk.github.io/categories/微服务/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://imokkkk.github.io/tags/微服务/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://imokkkk.github.io/tags/Spring-Cloud/"}]},{"title":"Redis五种数据类型的底层结构","slug":"Redis五种数据类型的底层结构","date":"2021-05-08T13:19:59.153Z","updated":"2021-04-24T15:10:14.121Z","comments":true,"path":"redisfive/","link":"","permalink":"https://imokkkk.github.io/redisfive/","excerpt":"RedisObject: Redis 中只有一个 K，一个 V。其中 K 绝对是字符串对象，而 V 可以是 String、List、Hash、Set、ZSet 任意一种。","text":"RedisObject: Redis 中只有一个 K，一个 V。其中 K 绝对是字符串对象，而 V 可以是 String、List、Hash、Set、ZSet 任意一种。 123456789101112typedef struct redisObject &#123; // 类型 string list set hash zset等 4bit unsigned type:4; // 编码方式 4bit unsigned encoding:4; // LRU 时间 24bit unsigned lru:LRU_BITS; // 引用计数 4byte int refcount; // 指向对象的指针 8byte 指针指向具体的数据，如set test hello，ptr指向的就是存储hello的字符串。 void *ptr;&#125; robj; encoding： 1234567891011/** 对象编码*/#define REDIS_ENCODING_RAW 0 // 编码为字符串#define REDIS_ENCODING_INT 1 // 编码为整数#define REDIS_ENCODING_HT 2 // 编码为哈希表#define REDIS_ENCODING_ZIPMAP 3 // 编码为 zipmap(2.6 后不再使用)#define REDIS_ENCODING_LINKEDLIST 4 // 编码为双端链表#define REDIS_ENCODING_ZIPLIST 5 // 编码为压缩列表#define REDIS_ENCODING_INTSET 6 // 编码为整数集合#define REDIS_ENCODING_SKIPLIST 7 // 编码为跳跃表 lru 记录对象最后一次被命令程序访问的时间，通过lru时间和当前时间可以计算某个对象的空转时间；利用object idletiem命令可以显示空转时间(单位为秒)，且不会改变该对象的lru。 refcount 该对象被引用的次数，目前共享对象只支持整数值的字符串对象。 1.String三种编码方式： int：存储的字符串全是数字 embstr：存储的字符串长度小于44个字符 raw：存储的字符串长度大于44个字符 embstr类型，它的数据指针和SDS对象在内存地址是连在一起的；但对于raw类型，二者在内存地址不是连续的。 SDS封装char[]，一个SDS最大512M 12345struct sdshdr&#123; unsigned int len; // 标记char[]的长度 unsigned int free; //标记char[]中未使用的元素个数 char buf[]; // 存放元素的坑&#125; Redis底层对SDS做的优化 预空间分配 SDS长度(len的值)小于1MB，那么程序将分配和len属性同样大小的未使用空间，此时 free和len属性值相同。假如SDS的len将变成15字节，则程序也会分配15字节的未使用空间，SDS的buf数组的实际长度变为15+15+1=31字节(额外一个字节用户保存空字符串) SDS长度(len的值)大于等于1MB，程序会分配1MB的未使用空间。如进行修改之后，SDS的len变成30MB，那么它的实际长度为30MB+1MB+1byte 惰性释放空间 当执行sdstrim(截取字符串)之后，SDS不会立即释放多出来的空间，如果下次在进行字符串拼接操作，且拼接的没有刚才释放的大，就会使用刚才的空间，不需要再重新申请空间。 二进制安全 C语言通过是否存在空字符\\0来判断是否已经是字符串的结尾。某些情况下(如使用空格进行分割一段字符串、或图片、视频等二进制文件)时，就会出现问题。SDS通过len字段判断，因此具备二进制安全性。 2.ListquickList(快速列表，是zipList压缩列表和linkedList双端链表的组合)，最大长度2^32-1，自测两端插入和弹出，并可以获得指定位置(或范围)的元素，可以充当数组、队列和栈。 lpush+lpop 先进后出的栈 lpush+rpop 先进先出的队列 lpush+ltrim 有限集合 lpush+Brpop 消息队列 linkedList 1234567891011121314151617181920212223//定义链表节点的结构体 typedef struct listNode &#123; //前置节点 struct listNode *prev; //后置节点 struct listNode *next; //节点的值 void *value; &#125;typedef struct list&#123; //表头节点 listNode *head; //表尾节点 listNode *tail; //链表包含节点的数量 unsigned long len; //节点复制函数，用于链表转移复制时对节点value拷贝的实现，一般情况下使用等号，某些特殊情况下给这个函数赋值NULL即表示使用等号进行节点转移 vode *(*dup) (void *ptr); //节点释放函数，用于释放一个节点所占用的内存空间，默认赋值NULL，即使用Redis自带的zfree函数进行内存空间释放 vode *(*free) (void *ptr); //节点对比函数，用于对比两个链表节点的value是否相等，相等返回1，不相等返回0 vode *(*match) (void *ptr,void *key);&#125; 获取前置节点、后置节点、表头节点和表尾节点的复杂度都是O(1)，获取节点数量也是O(1)。与双端链表相比，压缩列表可以节省空间，但进行修改或增删操作时，复杂度较高；因此节点数量较少时，可以使用压缩列表，但节点数量较多时，还是使用双端链表。 zipList 123456789101112131415161718192021typedf struct ziplist&lt;T&gt;&#123; //压缩列表占用字符数 int32 zlbytes; //最后一个元素距离起始位置的偏移量，用于快速定位最后一个节点 int32 zltail_offset; //元素个数 int16 zllength; //元素内容 T[] entries; //结束位 0xFF int8 zlend;&#125;ziplist typede struct entry&#123; //前一个entry的长度，倒序遍历可以通过这个参数定位到上一个entry的位置 int&lt;var&gt; prelen; //元素类型编码 int&lt;var&gt; encoding; //元素内容 optional byte[] content;&#125;entry zltail_offset这个参数可以快速定位到最后一个entry节点的位置，然后开始倒序遍历，即ziplist支持双向遍历。zipList遍历的时候，先根据zlbytes和zltail_offset定位到最后一个entry的位置，然后根据根据entry的prelen时确定前一个entry的位置。 zipList相比linkedList少了pre和next两个指针16个字节(64位系统1个指针就是8个字节)，linkedList每个节点内存都是单独分配，家具内存碎片化，zipList是由连续的内存组成的。 连锁更新 entry的prelen字段：前一个节点的长度小于254个字节时，prelen长度为1字节；前一个节点的长度大于254个字节时，prelen长度为5字节 假设现在有一组压缩列表，长度都在250~253字节之间，突然新增一个entry节点，这个entry节点长度大于等于254字节。由于新的entry节点大于等于254字节，这个entry节点的prelen为5个字节，随后会导致其余的所有entry节点的prelen增大为5字节；同样地，删除操作也会导致出现连锁更新这种情况。 zipList与linkedList的区别 当列表中元素的长度较小或数量较少时，通常采用zipList，当列表中元素长度较大或者数量较多时，使用linkedList 双向链表linkedList便于在列表的两端进行push和pop，插入节点复杂度很低，但内存开销比较大。(额外保存prev和next指针；内存碎片) zipList存储在一块连续的内存上，所以存储效率高，但插入和删除操作需要频繁的申请和释放内存。 3.2版本之后List使用quickList代替了zipList和linkedList，是zipList和linkedList的混合体。它将linkedList按段切分，每一段使用zipList来紧凑存储，多个zipList之间使用双向指针串接起来。 quickList内部默认单个zipList长度为8k字节，即redis.conf中list-max-ziplist-size的值为-2，超过这个阈值就会重新生成一个zipList来存储数据。当list-max-ziplist-size为正数n时，表示每个quicklist节点上的ziplist最多包含n个数据项。 压缩深度 quickList可以使用LZF算法对zipList进一步压缩，压缩后的zipList结构为 123456typedf struct ziplist_compressed&#123; //元素个数 int32 size; //元素内容 byte[] compressed_data&#125; 此时quicList为： redis.conf中list-compress-depth表示一个quickList两端不被压缩的节点的个数(指quickList双向链表节点个数，而不是zipList里数据项个数)，quickList默认压缩深度为0，即不开启压缩；当list-compress-depth为1，表示quickList的两端各有1个节点不进行压缩，中间节点进行压缩；当list-compress-dept为2，表示quickList的首尾各有2个节点不进行压缩，中间节点进行压缩；由此类推，对于quickList来说，首尾两个节点永远不会被压缩。 3.Hash dict 底层可以是zipList或hashtable(字典也叫哈希表)，hash中元素数量小于512个且所有键值对的键和值字符串长度都小于64字节时，才会使用zipList。 1234567typedf struct dict&#123; dictType *type;//类型特定函数，包括一些自定义函数，这些函数使得key和value能够存储 void *private;//私有数据 dictht ht[2];//两张hash表 int rehashidx;//rehash索引，字典没有进行rehash时，此值为-1 unsigned long iterators; //正在迭代的迭代器数量&#125;dict; type和private这两个属性是为了实现字典多态而设置的，当字典中存放着不同类型的值，对于的一些复制、比较函数也不一样。 rehashidx，这是一个辅助变量，用于记录rehash过程的进度，以及是否正在进行rehash等信息，等于-1时，表示dict此时没有rehash过程。 iterators，记录此时dict有几个迭代器正在进行遍历过程。 dictht dict本质上是对dicht的一个简单封装 123456typedf struct dictht&#123; dictEntry **table;//存储数据的dicEntry类型的数组 二维 unsigned long size;//数组的大小 unsigned long sizemask;//哈希表的大小的掩码，用于计算索引值，总是等于size-1 unsigned long used;//// 哈希表中中元素个数&#125;dictht; dicthtEntry 12345678910typedf struct dictEntry&#123; void *key;//键 union&#123; void val; unit64_t u64; int64_t s64; double d; &#125;v;//值 struct dictEntry *next；//指向下一个节点的指针&#125;dictEntry; 扩容与缩容 渐进式hash 假设当前数据在dictht[0]中，那么首先未dictht[1]分配足够的空间，如果是扩容，则dictht[1]的大小按照扩容规则进行扩容，如果是缩减，则dictht[1]的大小按照缩减规则进行缩减。 在字典dict中维护一个变量，rehashidx=0，表示rehash正式开始。 rehash进行期间，每次对字典执行添加、删除、查找或者更新操作时，还会顺带将dictht[0]哈希表在rehashidx索引上的所有键值对rehash到dichth[1]，当一次rehash工作完成之后，程序将rehashidx属性的值+1。同时在serverCron中调用rehash相关函数，在1ms的时间内，进行rehash处理，每次仅处理少量的转移任务(100个元素)。 当dichth[0]的所有键值对都会被rehash至dichth[1]，这时程序将rehashidx属性的值设为-1，表示rehash操作已完成。 每次对字典执行增删改查才会触发rehash，万一某段时间没有任何命令请求命令呢？ Redis有一个定时器，会定时判断rehash是否完成，如果没有完成，则继续hash。 如果是添加操作，会将新的数据直接添加到dichth[1]上，而对于删除、更改、查询操作，会直接在dictht[0]上进行，当dictht[0]上查询不到时，会接着去dictht[1]上查找，如果再找不到，则表明不存在该K-V值。 优点：采用分而治之的思想，将rehash操作分散到每一次到hash表的操作上及定时函数上，避免了集中式hash带来的性能压力。 缺点：在rehash过程中，需要保存两个hash表，对内存的占用比较大，如果在Redis服务器本来内存满了的时候，突然进行rehash会造成大量的key被抛弃。 4.Set无序且存储元素不重复。当value是整数时，且数据量不大时使用inset存储，其他情况用字典dict来存储。 inset 12345678typedf struct inset&#123; uint32_t encoding;//编码方式 有三种 默认 INSET_ENC_INT16 会根据插入数据的大小选择不一样的类型来存储 uint32_t length;//集合元素个数 int8_t contents[];//实际存储元素的数组，元素类型并不一定是ini8_t类型，柔性数组不占intset结构体大小，并且数组中的元素从小到大排列&#125;inset;#define INTSET_ENC_INT16 (sizeof(int16_t)) //16位，2个字节，表示范围-32,768~32,767#define INTSET_ENC_INT32 (sizeof(int32_t)) //32位，4个字节，表示范围-2,147,483,648~2,147,483,647#define INTSET_ENC_INT64 (sizeof(int64_t)) //64位，8个字节，表示范围-9,223,372,036,854,775,808~9,223,372,036,854,775,807 inset升级过程 了解旧的存储格式，计算出目前已有元素占用大小，计算规则是length*encoding，如4 * 16=64 确定新的编码格式，当原有的编码格式不能存储下新增的数据时，此时就要选择新的合适的编码格式 根据新的编码格式计算出需要新增的内存大小，然后从尾部将数据插入 根据新的编码格式重置之前的值，此时contents存在两种编码格式的值，需要统一，从插入新数据的位置开始，从后向前将之前的数据按照新的编码格式进行移动和设置。从后往前是为了防止数据被覆盖。 优点：根据存入的数据选择合适的编码方式，且只在必要的时候进行升级操作，节省内存。 缺点：耗费系统资源，不支持降级。 5.SortedSet常用作排行榜等功能，以用户id为value，关注事件或者分数作为score进行排序。zipList和skipList两种不同的实现： zipList [score,value]键值对数量少于128个 每个元素的长度小于64字节 skipList 不满足以上两个条件时使用跳表、组合了hash和skipList，hash用来存储value到score的映射，这样就可以在O(1)时间内找到value对应的分数；skipList按照从小到大的顺序存储分数；skipList每个元素的值都是[score,value]对。 skipList 空间换时间 跳表一个节点最高可以达到64层，一个跳表中最多可以存储2^64个元素。跳表中，每个节点都是一个skipListNode，每个跳表的节点也都会维护一个score值，这个值在跳表中是按照从小到大的顺序排列好的。 12345678910typedf struct zskiplist&#123; //头节点 struct zskiplistNode *header; //尾节点 struct zskiplistNode *tail; //跳表中元素个数 unsigned long length; //目前表内节点的最大层数 int level;&#125;zskiplist; 123456789typedf struct zskiplistNode&#123; sds ele;// 具体的数据 每个节点所保存的数据是唯一的，但节点的分数可以是一样的。两个相同分数的节点是按照元素的字典序进行排列的； double score;// 分数 从小到大排序 struct zskiplistNode *backward;//后退指针，用于从表尾向表头遍历，每个节点只有一个，即每次只能后退一步 struct zskiplistLevel&#123; struct zskiplistNode *forward;//前进指针forward，指向下一个节点 unsigned int span;//跨度span用来计算当前节点在跳表中的一个排名 &#125;level[];//层级数组 最大32&#125;zskiplistNode;","categories":[{"name":"Redis","slug":"Redis","permalink":"https://imokkkk.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://imokkkk.github.io/tags/Redis/"}]},{"title":"Redis","slug":"Redis","date":"2021-05-08T13:19:59.151Z","updated":"2020-04-29T02:44:38.032Z","comments":true,"path":"27273/","link":"","permalink":"https://imokkkk.github.io/27273/","excerpt":"1.Redis概述redis是一款高性能的NOSQL系列的非关系型数据库 1.1 什么是NoSQL​ NoSQL(NoSQL = Not Only SQL)，意即“不仅仅是SQL”，是一项全新的数据库理念，泛指非关系型的数据库。​ 随着互联网web2.0网站的兴起，传统的关系数据库在应付web2.0网站，特别是超大规模和高并发的SNS类型的web2.0纯动态网站已经显得力不从心，暴露了很多难以克服的问题，而非关系型的数据库则由于其本身的特点得到了非常迅速的发展。NoSQL数据库的产生就是为了解决大规模数据集和多重数据种类带来的挑战，尤其是大数据应用难题。 1.1.1 NOSQL和关系型数据库比较","text":"1.Redis概述redis是一款高性能的NOSQL系列的非关系型数据库 1.1 什么是NoSQL​ NoSQL(NoSQL = Not Only SQL)，意即“不仅仅是SQL”，是一项全新的数据库理念，泛指非关系型的数据库。​ 随着互联网web2.0网站的兴起，传统的关系数据库在应付web2.0网站，特别是超大规模和高并发的SNS类型的web2.0纯动态网站已经显得力不从心，暴露了很多难以克服的问题，而非关系型的数据库则由于其本身的特点得到了非常迅速的发展。NoSQL数据库的产生就是为了解决大规模数据集和多重数据种类带来的挑战，尤其是大数据应用难题。 1.1.1 NOSQL和关系型数据库比较 优点： (1) 成本：nosql数据库简单易部署，基本都是开源软件，不需要像使用oracle那样花费大量成本购买使用，相比关系型数据库价格便宜。 (2) 查询速度：nosql数据库将数据存储于缓存之中，关系型数据库将数据存储在硬盘中，自然查询速度远不及nosql数据库。 (3) 存储数据的格式：nosql的存储格式是key,value形式、文档形式、图片形式等等，所以可以存储基础类型以及对象或者是集合等各种格式，而数据库则只支持基础类型。 (4) 扩展性：关系型数据库有类似join这样的多表查询机制的限制导致扩展很艰难。 缺点： (1) 维护的工具和资料有限，因为nosql是属于新的技术，不能和关系型数据库10几年的技术同日而语。 (2) 不提供对sql的支持，如果不支持sql这样的工业标准，将产生一定用户的学习和使用成本。 (3) 不提供关系型数据库对事务的处理。 1.1.2 非关系型数据库的优势​ (1) 性能NoSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。​ (2) 可扩展性同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。 1.1.3 关系型数据库的优势​ (1) 复杂查询可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。​ (2) 事务支持使得对于安全性能很高的数据访问要求得以实现。对于这两类数据库，对方的优势就是自己的弱势，反之亦然。 1.1.4 总结​ 关系型数据库与NoSQL数据库并非对立而是互补的关系，即通常情况下使用关系型数据库，在适合使用NoSQL的时候使用NoSQL数据库，让NoSQL数据库对关系型数据库的不足进行弥补。​ 一般会将数据存储在关系型数据库中，在nosql数据库中备份存储关系型数据库的数据 1.2 主流的产品常见的关系型数据库: Oracle、DB2、MySQL、[SQL Server](https://baike.baidu.com/item/Microsoft SQL Server/2947866) 常见的非关系型数据库: Redis、MongodDB、HBase、Neo4J (1) 键值(Key-Value)存储数据库 相关产品： Tokyo Cabinet/Tyrant、Redis、Voldemort、Berkeley DB 典型应用： 内容缓存，主要用于处理大量数据的高访问负载。 数据模型： 一系列键值对 优势： 快速查询 劣势： 存储的数据缺少结构化 (2) 列存储数据库 相关产品：Cassandra, HBase, Riak 典型应用：分布式的文件系统 数据模型：以列簇式存储，将同一列数据存在一起 优势：查找速度快，可扩展性强，更容易进行分布式扩展 劣势：功能相对局限 (3) 文档型数据库 相关产品：CouchDB、MongoDB 典型应用：Web应用（与Key-Value类似，Value是结构化的） 数据模型： 一系列键值对 优势：数据结构要求不严格 劣势： 查询性能不高，而且缺乏统一的查询语法 (4) 图形(Graph)数据库 相关数据库：Neo4J、InfoGrid、Infinite Graph 典型应用：社交网络 数据模型：图结构 优势：利用图结构相关算法。 劣势：需要对整个图做计算才能得出结果，不容易做分布式的集群方案。 1.3 什么是Redis​ Redis是用C语言开发的一个开源的高性能键值对（key-value）数据库，官方提供测试数据，50个并发执行100000个请求,读的速度是110000次/s,写的速度是81000次/s ，且Redis通过提供多种键值数据类型来适应不同场景下的存储需求，目前为止Redis支持的键值数据类型如下：​ (1) 字符串类型 string​ (2) 哈希类型 hash​ (3) 列表类型 list​ (4) 集合类型 set​ (5) 有序集合类型 sortedset redis的应用场景 缓存（数据查询、短连接、新闻内容、商品内容等等） 聊天室的在线好友列表 任务队列（秒杀、抢购、12306等等） 应用排行榜 网站访问统计 数据过期处理（可以精确到毫秒 分布式集群架构中的session分离 2.常用命令操作2.1 redis的数据结构redis存储的是：key, value格式的数据，其中key都是字符串，value有5种不同的数据结构 value的数据结构： (1) 字符串类型 string (2) 哈希类型 hash ： map格式 (3) 列表类型 list ： linkedlist格式。支持重复元素 (4) 集合类型 set ： 不允许重复元素 (5) 有序集合类型 sortedset：不允许重复元素，且元素有顺序 字符串类型 string 123456789存储： set key value 127.0.0.1:6379&gt; set username zhangsan OK获取： get key 127.0.0.1:6379&gt; get username \"zhangsan\"删除： del key 127.0.0.1:6379&gt; del age (integer) 1 哈希类型 hash 1234567891011121314151617181920存储： hset key field value 127.0.0.1:6379&gt; hset myhash username lisi (integer) 1 127.0.0.1:6379&gt; hset myhash password 123 (integer) 1获取： hget key field: 获取指定的field对应的值 127.0.0.1:6379&gt; hget myhash username \"lisi\" hgetall key：获取所有的field和value 127.0.0.1:6379&gt; hgetall myhash 1) \"username\" 2) \"lisi\" 3) \"password\" 4) \"123\"删除： hdel key field 127.0.0.1:6379&gt; hdel myhash username (integer) 1 列表类型 list: 可以添加一个元素到列表的头部（左边）或者尾部（右边） 12345678910111213141516171819202122添加： lpush key value: 将元素加入列表左表 rpush key value：将元素加入列表右边 127.0.0.1:6379&gt; lpush myList a (integer) 1 127.0.0.1:6379&gt; lpush myList b (integer) 2 127.0.0.1:6379&gt; rpush myList c (integer) 3获取： lrange key start end ：范围获取 127.0.0.1:6379&gt; lrange myList 0 -1 (1) \"b\" (2) \"a\" (3) \"c\"删除： lpop key： 删除列表最左边的元素，并将元素返回 rpop key： 删除列表最右边的元素，并将元素返回 集合类型 set : 不允许重复元素 12345678910111213存储：sadd key value 127.0.0.1:6379&gt; sadd myset a (integer) 1 127.0.0.1:6379&gt; sadd myset a (integer) 0获取：smembers key:获取set集合中所有元素 127.0.0.1:6379&gt; smembers myset (1) \"a\"删除：srem key value:删除set集合中的某个元素 127.0.0.1:6379&gt; srem myset a (integer) 1 有序集合类型 sortedset：不允许重复元素，且元素有顺序.每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 123456789101112131415161718192021222324存储：zadd key score value 127.0.0.1:6379&gt; zadd mysort 60 zhangsan (integer) 1 127.0.0.1:6379&gt; zadd mysort 50 lisi (integer) 1 127.0.0.1:6379&gt; zadd mysort 80 wangwu (integer) 1获取：zrange key start end [withscores] 127.0.0.1:6379&gt; zrange mysort 0 -1 (1) \"lisi\" (2) \"zhangsan\" (3) \"wangwu\" 127.0.0.1:6379&gt; zrange mysort 0 -1 withscores (1) \"zhangsan\" (2) \"60\" (3) \"wangwu\" (4) \"80\" (5) \"lisi\" (6) \"500\"删除：zrem key value 127.0.0.1:6379&gt; zrem mysort lisi (integer) 1 2.2 通用命令1231. keys * : 查询所有的键2. type key ： 获取键对应的value的类型3. del key：删除指定的key value 3.Java客户端 Jedis3.1 Jedis操作各种redis中的数据结构123456//1. 获取连接 Jedis jedis = new Jedis(\"localhost\",6379); //2. 操作 jedis.set(\"username\",\"zhangsan\"); //3. 关闭连接 jedis.close(); 字符串类型 string 123456789101112//1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 \"localhost\",6379端口 //2. 操作 //存储 jedis.set(\"username\",\"zhangsan\"); //获取 String username = jedis.get(\"username\"); System.out.println(username); //可以使用setex()方法存储可以指定过期时间的 key value jedis.setex(\"activecode\",20,\"hehe\");//将activecode：hehe键值对存入redis，并且20秒后自动删除该键值对 //3. 关闭连接 jedis.close(); 哈希类型 hash: map格式 12345678910111213141516171819202122//1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 \"localhost\",6379端口 //2. 操作 // 存储hash jedis.hset(\"user\",\"name\",\"lisi\"); jedis.hset(\"user\",\"age\",\"23\"); jedis.hset(\"user\",\"gender\",\"female\"); // 获取hash String name = jedis.hget(\"user\", \"name\"); System.out.println(name); // 获取hash的所有map中的数据 Map&lt;String, String&gt; user = jedis.hgetAll(\"user\"); // keyset Set&lt;String&gt; keySet = user.keySet(); for (String key : keySet) &#123; //获取value String value = user.get(key); System.out.println(key + \":\" + value); &#125; //3. 关闭连接 jedis.close(); 列表类型 list: linkedlist格式 支持重复元素 ​ lpush / rpush​ lpop / rpop​ lrange start end : 范围获取 123456789101112131415161718192021222324//1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 \"localhost\",6379端口 //2. 操作 // list 存储 jedis.lpush(\"mylist\",\"a\",\"b\",\"c\");//从左边存 jedis.rpush(\"mylist\",\"a\",\"b\",\"c\");//从右边存 // list 范围获取 List&lt;String&gt; mylist = jedis.lrange(\"mylist\", 0, -1); System.out.println(mylist); // list 弹出 String element1 = jedis.lpop(\"mylist\");//c System.out.println(element1); String element2 = jedis.rpop(\"mylist\");//c System.out.println(element2); // list 范围获取 List&lt;String&gt; mylist2 = jedis.lrange(\"mylist\", 0, -1); System.out.println(mylist2); //3. 关闭连接 jedis.close(); 集合类型 set: 不允许重复元素 12345678910//1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 \"localhost\",6379端口 //2. 操作 // set 存储 jedis.sadd(\"myset\",\"java\",\"php\",\"c++\"); // set 获取 Set&lt;String&gt; myset = jedis.smembers(\"myset\"); System.out.println(myset); //3. 关闭连接 jedis.close(); 有序集合类型 sortedset: 不允许重复元素，且元素有顺序 123456789101112//1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 \"localhost\",6379端口 //2. 操作 // sortedset 存储 jedis.zadd(\"mysortedset\",3,\"亚瑟\"); jedis.zadd(\"mysortedset\",30,\"后裔\"); jedis.zadd(\"mysortedset\",55,\"孙悟空\"); // sortedset 获取 Set&lt;String&gt; mysortedset = jedis.zrange(\"mysortedset\", 0, -1); System.out.println(mysortedset); //3. 关闭连接 jedis.close(); 3.2 jedis连接池: JedisPool* 使用： 1. 创建JedisPool连接池对象 2. 调用方法 getResource()方法获取Jedis连接1234567891011121314//0.创建一个配置对象 JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(50); config.setMaxIdle(10); //1.创建Jedis连接池对象 JedisPool jedisPool = new JedisPool(config,\"localhost\",6379); //2.获取连接 Jedis jedis = jedisPool.getResource(); //3. 使用 jedis.set(\"hehe\",\"heihei\"); //4. 关闭 归还到连接池中 jedis.close(); 连接池工具类 12345678910111213141516171819202122232425262728public class JedisPoolUtils &#123; private static JedisPool jedisPool; static&#123; //读取配置文件 InputStream is = JedisPoolUtils.class.getClassLoader().getResourceAsStream(\"jedis.properties\"); //创建Properties对象 Properties pro = new Properties(); //关联文件 try &#123; pro.load(is); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; //获取数据，设置到JedisPoolConfig中 JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(Integer.parseInt(pro.getProperty(\"maxTotal\"))); config.setMaxIdle(Integer.parseInt(pro.getProperty(\"maxIdle\"))); //初始化JedisPool jedisPool = new JedisPool(config,pro.getProperty(\"host\"),Integer.parseInt(pro.getProperty(\"port\"))); &#125; /** * 获取连接方法 */ public static Jedis getJedis()&#123; return jedisPool.getResource(); &#125;&#125;","categories":[],"tags":[{"name":"tool","slug":"tool","permalink":"https://imokkkk.github.io/tags/tool/"}]},{"title":"MyBatis是如何防止SQL注入的","slug":"MyBatis是如何防止SQL注入的","date":"2021-05-08T13:19:59.141Z","updated":"2020-04-29T02:44:37.934Z","comments":true,"path":"16852/","link":"","permalink":"https://imokkkk.github.io/16852/","excerpt":"1.MyBatis概述​ MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs(Plain Ordinary Java Object,普通的 Java对象)映射成数据库中的记录。 2.SQL 注入攻击概述","text":"1.MyBatis概述​ MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs(Plain Ordinary Java Object,普通的 Java对象)映射成数据库中的记录。 2.SQL 注入攻击概述 ​ SQL注入攻击，简称SQL攻击或注入攻击，是发生于应用程序之数据库层的安全漏洞。简而言之，是在输入的字符串之中注入SQL指令，在设计不良的程序当中忽略了检查，那么这些注入进去的指令就会被数据库服务器误认为是正常的SQL指令而运行，因此遭到破坏或是入侵。 ​ 最常见的就是我们在应用程序中使用字符串联结方式组合 SQL 指令，有心之人就会写一些特殊的符号，恶意篡改原本的 SQL 语法的作用，达到注入攻击的目的。 举个例子: 比如验证用户登录需要 username 和 password，编写的 SQL 语句如下： 1select * from user where (name = '\"+ username +\"') and (pw = '\"+ password +\"'); username 和 password 字段被恶意填入 1username = \"1' OR '1'='1\"; 与 1password = \"1' OR '1'='1\"; 将导致原本的 SQL 字符串被填为： 1select * from user where (name = '1' or '1'='1') and (pw = '1' or '1'='1'); 实际上运行的 SQL 语句将变成： 1select * from user; 也就是不再需要 username 和 password 账密即达到登录的目的，结果不言而喻。 3.MyBatis 解决 SQL 注入问题#{}和${}的区别是什么？ (1）mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值。 （2）mybatis在处理${}时，就是把${}替换成变量的值。 （3）使用#{}可以有效的防止SQL注入，提高系统安全性。原因在于：预编译机制。预编译完成之后，SQL的结构已经固定，即便用户输入非法参数，也不会对SQL的结构产生影响，从而避免了潜在的安全风险。 （4）预编译是提前对SQL语句进行预编译，而其后注入的参数将不会再进行SQL编译。我们知道，SQL注入是发生在编译的过程中，因为恶意注入了某些特殊字符，最后被编译成了恶意的执行操作。而预编译机制则可以很好的防止SQL注入。 MyBatis框架作为一款半自动化的持久层框架，其SQL语句都要我们自己手动编写，这个时候当然需要防止SQL注入。其实，MyBatis的SQL是一个具有“输入+输出”的功能，类似于函数的结构，参考上面的两个例子。其中，parameterType表示了输入的参数类型，resultType表示了输出的参数类型。回应上文，如果我们想防止SQL注入，理所当然地要在输入参数上下功夫。上面代码中使用#的即输入参数在SQL中拼接的部分，传入参数后，打印出执行的SQL语句，会看到SQL是这样的： 1select id, username, password, role from user where username=? and password=? 不管输入什么参数，打印出的SQL都是这样的。这是因为MyBatis启用了预编译功能，在SQL执行前，会先将上面的SQL发送给数据库进行编译；执行时，直接使用编译好的SQL，替换占位符“?”就可以了。因为SQL注入只能对编译过程起作用，所以这样的方式就很好地避免了SQL注入的问题。 底层实现原理 ​ MyBatis是如何做到SQL预编译的呢？其实在框架底层，是JDBC中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。这种“准备好”的方式不仅能提高安全性，而且在多次执行同一个SQL时，能够提高效率。原因是SQL已编译好，再次执行时无需再编译。 在使用MyBatis框架时，有以下场景极易产生SQL注入 SQL语句中的一些部分，例如order by字段、表名等，是无法使用预编译语句的。这种场景极易产生SQL注入。推荐开发在Java层面做映射，设置一个字段/表名数组，仅允许用户传入索引值。这样保证传入的字段或者表名都在白名单里面(手动过滤，添加白名单)。 like参数注入。使用如下SQL语句可防止SQL注入 1like concat('%',#&#123;title&#125;, '%')， in之后参数的SQL注入。使用如下SQL语句可防止SQL注入 1234id in&lt;foreach collection=\"ids\" item=\"item\" open=\"(\"separator=\",\" close=\")\"&gt;#&#123;item&#125; &lt;/foreach&gt;","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://imokkkk.github.io/tags/面试/"}]},{"title":"JVM&垃圾回收","slug":"JVM&垃圾回收","date":"2021-05-08T13:19:59.138Z","updated":"2021-04-07T14:48:19.865Z","comments":true,"path":"jvmAndgc/","link":"","permalink":"https://imokkkk.github.io/jvmAndgc/","excerpt":"1.JRE、JDK JRE：Java的运行时环境，JVM的标准加上实现的一大堆基础类库。 JDK：包含JRE，还提供了一些小工具，如Javac、Java、Jar。 2.JVM","text":"1.JRE、JDK JRE：Java的运行时环境，JVM的标准加上实现的一大堆基础类库。 JDK：包含JRE，还提供了一些小工具，如Javac、Java、Jar。 2.JVM 3.类的加载过程 加载将外部的.class文件加载到方法区。 验证不能将任何的.class文件都加载，不符合规范的将抛出java.lang.VerifyError错误。(如低版本的JVM无法加载一些高版本的类库) 准备为一些类变量分配内存，并将其初始化为默认值。此时，实例对象还没有分配内存，所以这些动作是在方法区上进行的。 123456789101112131415161718code-snippet 1： public class A &#123; //类变量 static int a ; public static void main(String[] args) &#123; System.out.println(a); &#125; &#125;a:0code-snippet 2： public class A &#123; public static void main(String[] args) &#123; //局部变量 int a ; System.out.println(a); &#125; &#125;编译错误 类变量有两次赋初始值的过程：1.准备阶段，赋予初始值(也可以是指定值) 2.初始化阶段，赋予程序员指定的值 局部变量不存在准备阶段，如果没有赋予初始值就不能使用 解析将符号引用替换成直接引用符号引用：一种定义，可以是任何字面上的含义直接引用：直接指向目标的指针、相对变量，直接引用的对象都存在于内存中 类或接口的解析 类方法的解析 接口方法的解析 字段解析 初始化初始化成员变量 123456789101112public class A &#123; static int a = 0 ; static &#123; a = 1; b = 1; &#125; static int b = 0; public static void main(String[] args) &#123; System.out.println(a); //a:1 System.out.println(b); //b:0 &#125; &#125; static语句块只能访问到定义在static语句块之前的变量 JVM会保证在子类的初始化方法执行之前，父类的初始化方法已经执行完毕 类初始化顺序 父类静态变量、静态代码块(只有第一次加载类时执行) 子类静态变量、静态代码块(只有第一次加载类时执行) 父类非静态代码块 父类构造器 子类非静态代码块 子类构造器 4.clinit和init123456789101112131415161718192021222324252627public class A &#123; static &#123; System.out.println(\"1\"); &#125; public A()&#123; System.out.println(\"2\"); &#125; &#125; public class B extends A &#123; static&#123; System.out.println(\"a\"); &#125; public B()&#123; System.out.println(\"b\"); &#125; public static void main(String[] args)&#123; A ab = new B(); ab = new B(); &#125; &#125;// 1// a// 2// b// 2// b static字段和static代码块属于类，在类的初始化阶段就被执行，类的信息存放在方法区，同一个类加载器只有一份，所以上面的static只会执行一次，对应clinit方法。 对象初始化，在new一个新对象时，会调用构造方法来初始化对象的属性，对应init，每次新建对象都会执行。 5.双亲委派机制除了顶层的启动类加载器以外，其余的类加载器，在加载之前，都会委派给其父加载器进行加载。这样一层层向上传递，直到祖先们都无法胜任，它才会真正的加载。 6.引用级别 强引用当内存空间不足时，JVM抛出OutOfMemoryError。即使程序异常终止，这种对象也不会被回收，是最普通最强硬的一种存在，只有在和GC Roots断绝关系时才会被消灭掉。 软引用用于维护一些可有可无的对象。在内存足够时，软引用对象不会被回收，内存不足时，系统会回收软引用对象。如果回收了软引用对象仍然没有足够的内存，才会抛出内存溢出异常。软引用可以和引用队列联合使用，如果软引用的对象被垃圾回收，JVM就会把这个软引用加入到与之关联的引用队列中。 弱引用垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象，它拥有更短的生命周期。 虚引用如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。虚引用主要用来跟踪对象被垃圾回收的活动。虚引用必须和引用队列联合使用，当垃圾回收准备回收一个对象时，如果发现它还有虚引用，就会在回收对象之前，把这个虚引用加入到与之关联的引用队列中。 7.典型OOM场景除了程序计数器，其它区域都有可能会发生OOM，但最常见的还是发生在堆上。 内存容量太小了，需要扩容，或者需要调整堆的空间 错误的引用的方式，发生了内存泄漏。如线程池里的线程，在复用的情况下忘记清理ThreadLocal的内容。 接口没有进行范围校验，外部传参超出范围。比如数据库查询时的每页条数等。 对堆外内存无限制的使用。这种情况更加严重，会造成操作系统内存耗尽。 8.垃圾回收算法 标记清除 标记：从根集合扫描，对存活的对象进行标记 清除：从堆内粗从头到尾进行线性遍历，回收不可达对象内存 标记整理 标记：从根集合扫描，对存活的对象进行标记 整理：移动所有存活的对象，按照内存地址排序，内存地址以后的内存全部回收。 复制算法分对象面和空闲面，对象在对象面创建，回收时，存活的对象被从对象面复制到空闲面，后将对象面所有对象清除。 分代收集把死的快的对象所占区域，叫作年轻代。其他活的长的对象所占的区域，叫作老年代。 年轻代：使用复制算法年轻代分为：一个伊甸园空间(Eden)，两个幸存者空间(Survivor)当年轻代中的Eden区分配满的时候，就会触发年轻代的GC(Minoe GC)。 在Eden区执行了第一次GC之后，存活的对象会移动到其中一个Survivor区(from) Eden再次GC，这时会采用复制算法，将Eden和from区一起清理。存活的对象会被复制到to区，接下来清空from区就可以了。 Eden：from：to = 8：1：1 -XX:SurvivorRatio 默认为8 TLAB：JVM默认给每个线程开辟一个buffer区域来加速对象分配，这个buffer就放在Eden区。对象的分配优先在TLAB上分配，但通常TLAB很小，所以对象比较大时，会在Eden的共享区域进行分配。 老年代：使用标记清除、标记整理算法 9.对象如何进入老年代 提升每当发生一次Minor GC，存活下的对象年龄会加1，达到阈值(-XX:+MaxTenuringThreshold，最大值为15)，就会提升到老年代。 分配担保每次存活的对象，都会放入其中一个幸存区，默认比例为10%。但无法保证每次存活的对象小于10%，当Survivor空间不够，就需要依赖其他内存(老年代)进行分配担保。 大对象直接在老年代分配超过某个大小的对象将直接在老年代分配，-XX:PretenureSizeThreshold进行配置，默认为0。 动态对象年龄判定有的垃圾回收算法，并不需要年龄达到15，会使用一些动态的计算方法，如幸存区中相同年龄对象大小的和大于幸存区的一半，大于或等于age的对象将直接进入老年代 10.垃圾回收器 年轻代垃圾收集器 Serial垃圾收集器处理GC的只有一条线程，并且在垃圾回收过程中暂停一切用户线程。 ParNew垃圾收集器Serial的多线程版本，多条GC线程并行的进行垃圾清理，清理过程中依然要停止用户线程。 Parallel Scavenge垃圾收集器另一个多线程版本的垃圾回收器。与ParNew的主要区别： Parallel Scacenge：追求CPU吞吐量，适合没有交互的后台计算。弱交互强计算。 ParNew：追求降低用户停顿时间，适合交互式应用。强交互弱计算。 老年代垃圾收集器 Serial Old垃圾收集器与年轻代的Serial垃圾收集器对应，都是单线程版本，同样适用客户端使用。年轻代的Serial，使用复制算法，老年代的Serial Old，使用标记-整理算法。 Parallel OldParallel Scavenge的老年代版本，追求CPU吞吐量。 CMS垃圾收集器垃圾收集时用户线程和GC线程可以并发执行。 11.CMS回收过程Minor GC：发生在年轻代的GC Major GC：发生在老年代的GC Full GC：全堆垃圾回收，如Metaspace区引起年轻代和老年代的回收 CMS(主要并发标记清除收集器)，年轻代使用复制算法，老年代使用标记-清除算法 初始标记初始标记阶段，只标记直接关联GC root的对象，不用向下追溯。最耗时的就是tracing阶段，极大地缩短了初始标记阶段，所以该过程时间较短，这个过程是STW的。 并发标记在初始标记的基础上，进行并发标记。tracing，标记所有可达的对象。这个阶段会比较久，但却可以和用户线程并行。 有些对象，从新生代晋升到了老年代 有些对象，直接分配到了老年代 老年代或者新生代的对象引用发生了变化 并发预清理不需要STW，目的是缩短重新标记的时间。这个时候，老年代中被标记为dirty的卡页中的对象，就会被重新标记，然后清除掉dirty的状态。由于这个阶段也是可以并发的，有可能还会有处于dirty状态的卡页。 并发可取消的预清理重新标记阶段是STW的，所以会有很多次预清理动作。可以在满足某些条件时，可以终止，如迭代次数、有用工作量、消耗的系统时间等。 意图：避免回扫年轻代的大量对象；当满足重新标记时，自动退出。 重新标记CMS尝试在年轻代尽可能空的情况下运行重新标记，以免接连多次发生STW。这是CMS垃圾回收阶段的第二次STW阶段，目标是完成老年代中所有存活对象的标记。 并发清理用户线程被重新激活，目标时删掉不可达对象。 由于CMS并发清理阶段用户线程还在运行中，CMS无法在当次GC中处理它们，只好留在下一处GC时再清理掉，这一部分垃圾称为”浮动垃圾”。 并发重置与用户线程并发执行，重置CMS算法相关的内部数据，为下一次GC循环做准备。 12.内存碎片 CMS执行过程中，用户线程还在运行，如果老年代空间快满了，才开始回收，用户线程可能会产生”Concurrent Mode Failure”的错误，这时会临时启用Serial Old收集器来重新进行老年代垃圾收集，这样STW会很久。 CMS对老年代回收的时候，并没有内存的整理阶段。程序长时间运行后，碎片太多，如果你申请一个稍大的对象，就会引起分配失败。(1) UseCMSCompactAtFullCollection(默认开启)，在进行Full GC时，进行碎片整理。内存碎片整理是无法并发的，STW时间较长。(2)CMSFullGCsBeforeCompation(默认为0)，每隔多少次不压缩的Full GC后，执行一次带压缩的Full GC。 13.CMS优势： 低延迟，尤其对大堆来说。大部分垃圾回收过程并发执行。 劣势： 内存碎片问题。Full GC的整理阶段，会造成较长时间的停顿。 需要预留空间，用来收集”浮动垃圾”。 使用更多的CPU资源。 14.G1 G1也是有Eden区和Survivor区的概念的，但内存上不是连续的。小区域(Region)的大小是固定的，名字叫做小队区，小队区可以是Eden，也可以是Survivor，还可以是Old。Region大小（-XX:G1HeapRegionSize=M)一致，为1M-32M字节间的一个2的幂指数。如果对象太大，大小超过Region 50%的对象，将会分配在Humongous Region。垃圾最多的小堆区，会被优先收集。-XX:MaxGCPauseMills=10 15.卡表与RSet 卡表老年代被分成众多的卡页(一般是2的次幂)，卡表就是用于标记卡页状态的一个集合，每个卡表对应一个卡页。如果年轻代有对象分配，而且老年代有对象指向这个新对象，那么这个老年代对象所对应内存的卡页，就会标识为dirty，卡表只需要很小的存储空间就可以保留这些状态。垃圾回收时，就可以先读卡表，进行快速判断。 RSetRSet是一个空间换时间的数据结构。卡表是一种points-out(我引用了谁对象)的结构，而RSet是一种points-into(谁引用了我的对象)的结构。RSet类似一个Hash，key是引用的Region地址，value是引用它的对象的卡页集合 16.G1回收过程 年轻代回收JVM启动时，G1会先准备好Eden区，程序在运行时不断创建到Eden区，当所有的Eden区满了，启动一次年轻代垃圾回收过程。年轻代是一个STW过程，它的跨代引用使用RSet来追溯，会一次回收掉年轻代的所有Region。 并发标记当整个堆内存使用达到一定比例(-XX:InitatingHeapOccupancyPercent 默认45%)，启动并发标记阶段。为混合回收提供标记服务，类似CMS的垃圾回收过程。 混合回收通过并发标记阶段，已经统计了老年代的垃圾占比，在Minor GC之后，如果占比达到阈值(-XX:G1HeapWastePercent 默认是堆大小的5%，该参数可以调整Mixed GC的频率)，下次就会触发混合回收。参数G1MixedGCCountTarget：一次并发标记之后，最多执行Mixed GC的次数。","categories":[{"name":"JVM","slug":"JVM","permalink":"https://imokkkk.github.io/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://imokkkk.github.io/tags/JVM/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://imokkkk.github.io/tags/垃圾回收/"},{"name":"GC","slug":"GC","permalink":"https://imokkkk.github.io/tags/GC/"}]},{"title":"Java并发编程","slug":"Java并发编程","date":"2021-05-08T13:19:59.135Z","updated":"2021-06-13T15:00:06.936Z","comments":true,"path":"multithreading/","link":"","permalink":"https://imokkkk.github.io/multithreading/","excerpt":"1.多线程基础1.1 实现多线程的方法 实现Runnable接口 123456public class Test_01 implements Runnable&#123; @Override public void run() &#123; System.out.println(\"实现Runnable接口实现多线程\"); &#125;&#125;","text":"1.多线程基础1.1 实现多线程的方法 实现Runnable接口 123456public class Test_01 implements Runnable&#123; @Override public void run() &#123; System.out.println(\"实现Runnable接口实现多线程\"); &#125;&#125; 123456public class Test_02 extends Thread &#123; @Override public void run() &#123; System.out.println(\"继承Thread类实现多线程\"); &#125;&#125; 线程池创建线程 1234567891011121314151617181920212223242526static class DefaultThreadFactory implements ThreadFactory &#123; private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() &#123; SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = \"pool-\" + poolNumber.getAndIncrement() + \"-thread-\"; &#125; public Thread newThread(Runnable r) &#123; Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; &#125;&#125; 对于线程池而言，本质上是通过线程工厂创建线程的，默认采用DefaultThreadFactory，它会给线程池创建的线程设置一些默认值，如：线程的名字、是否守护线程，以及线程的优先级等。但无论怎么设置这些这些属性，最终还是通过new Thread()创建线程的，只不过这里的构造函数传入的参数要多一些，本质还是通过new Thread()实现的。 实现有返回值的Callable创建线程 12345678910111213141516public class Test_03 implements Callable &#123; @Override public Integer call() throws Exception &#123; int i = new Random().nextInt(); System.out.println(Thread.currentThread().getName() +\" : \"+ i); return i; &#125; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newFixedThreadPool(10); for (int i = 0; i &lt; 100; i++) &#123; Future&lt;Integer&gt; future = executorService.submit(new Test_03()); &#125; executorService.shutdown(); &#125;&#125; Runnable创建线程是无返回值的，而Callable和与之相关的Future、FutureTask，它们可以把线程执行的结果作为返回值返回。 …… 实现线程只有一种方式 启动线程需要调用start()方法，而start方法最终会调用run()方法，分析run()方法 123456@Overridepublic void run() &#123; if (target != null) &#123; target.run(); &#125;&#125; target实际上就是一个Runnable，即使用Runnable接口实现线程时传给Thread类的对象。第二种，继承Thread方式，继承Thread之后，会把run()方法重写，最终还是会调用thread.start()方法启动线程，而start()方法最终也会调用这个已经被重写的run()方法来执行任务。创建线程本质就是构造一个Thread类，不同点在于实现线程运行内容的方式不同，可以通过实现Runnable接口，或继承Thread类重写run()方法。 1.2 实现Runnable接口比继承Thread类实现线程更好？ Java不支持多继承，一旦继承了Thread类，就无法再继承其它类，限制了代码的可扩展性。 Runnable里只有一个run()方法，定义了需要执行的内容，实现了Runnable与Thread类的解耦，Thread类负责线程启动和属性设置，权责分明。 1.3 如何正确停止线程？对于Java而言，最正确的停止线程的方式是使用interrupt，但interrupt仅仅起到通知被停止线程的作用，而对于被停止的线程而言，它拥有完全的自主权，即可以选择立即停止，也可以一段时间后停止，也可以不停止。Java希望程序间可以相互通知、相互协作的管理线程，如果贸然停止线程可能会造成一些安全性问题，为了避免造成问题就需要给对方一定的时间来整理收尾工作。 123while (!Thread.currentThread().isInterrupted() &amp;&amp; more work to do) &#123; do more work&#125; 一旦调用某个线程的interrupt后，该线程的中断标记位就会被设置成true，每个线程都有这样的标记位，当线程执行时应定期检查这个标记位。上面代码可以看到，while循环判断语句中，先通过Thread.currentThread().isInterrupt()判断是否被中断，随后检查是否还有工作要做。 1.4 sleep期间能否感受到中断？如果sleep、wait等可以让线程进入阻塞的方法使线程休眠了，而处于休眠中的线程被中断，线程是可以感受到中断信号的，并会抛出InterruptedException，同时清除中短信号，将中断标记位设为false。 处理方式： 方法签名抛异常，run()强制try/catch 123void subTask() throws InterruptedException &#123; Thread.sleep(1000);&#125; 要求每一个方法的调用方有义务去处理异常。调用方要不使用try/catch并在catch中正确处理异常，要不将异常声明到方法签名中。如果每层逻辑都遵守规范，便可以将中断信号传递到顶层，最终让run()方法可以捕获到异常。而对于run()方法而言，它本身没有抛出checkedException的能力，只能通过try/catch来处理异常。层层传递异常保障了异常不会被遗漏，而对于run()方法，就可以根据不同的业务逻辑来进行相应的处理。 再次中断 12345678private void reInterrupt() &#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); e.printStackTrace(); &#125;&#125; 在catch语句中再次中断线程。如果线程在休眠期间被中断，那么会自动清除中断信号。如果这时手动添加中断信号，中断信号依然可以被捕捉到。 1.5 为什么用volatile标记位的停止方法是错误的？stop()会直接把线程停止，会导致出现数据完整性等问题。suspend()和resume()并不会释放锁，就开始进入休眠，但此时有可能仍持有锁，容易导致死锁问题。 volatile修饰标记位适用的场景 123456789101112131415161718192021222324252627public class VolatileCanStop implements Runnable &#123; private volatile boolean canceled = false; @Override public void run() &#123; int num = 0; while (!canceled &amp;&amp; num &lt; 1000000) &#123; if (num % 10 == 0) &#123; System.out.println(Thread.currentThread().getName() + \":\" + num + \"是10的倍数\"); &#125; num++; try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; VolatileCanStop canStop = new VolatileCanStop(); Thread thread = new Thread(canStop); thread.start(); Thread.sleep(3000); canStop.canceled = true; &#125;&#125; 启动线程，经过3s，把volatile修饰的标记位设置为true，那么下一次while循环中判断出canceled的值为true，就跳出while循环，线程停止。 volatile修饰标记位不适用的场景 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class VolatileCanNotStop &#123; public static void main(String[] args) throws InterruptedException &#123; ArrayBlockingQueue storage = new ArrayBlockingQueue(8); Producer producer = new Producer(storage); Thread producerThread = new Thread(producer); producerThread.start(); Thread.sleep(500); Consumer consumer = new Consumer(storage); while (consumer.needMoreNums()) &#123; System.out.println(consumer.storage.take() + \"被消费了\"); Thread.sleep(100); &#125; System.out.println(\"消费者不需要更多数据了。\"); // 一旦消费不需要更多数据了，我们应该让生产者也停下来，但是实际情况却停不下来 producer.canceled = true; System.out.println(producer.canceled); &#125;&#125;class Producer implements Runnable &#123; public volatile boolean canceled = false; BlockingQueue storage; public Producer(BlockingQueue storage) &#123; this.storage = storage; &#125; @Override public void run() &#123; int num = 0; try &#123; while (!canceled &amp;&amp; num &lt;= 10000) &#123; if (num % 50 == 0) &#123; storage.put(num); System.out.println(Thread.currentThread().getName() + \":\" + num + \"是50的倍数，被放到仓库中了\"); &#125; num++; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(\"生产者结束运行\"); &#125; &#125;&#125;class Consumer &#123; BlockingQueue storage; public Consumer(BlockingQueue storage) &#123; this.storage = storage; &#125; public boolean needMoreNums() &#123; if (Math.random() &gt; 0.97) &#123; return false; &#125; return true; &#125;&#125; 线程被长时间阻塞的情况，就无法及时感受中断：尽管已经把canceled的标记位设置为true，但生产者仍然没有被停止，是因为生产者在执行storage.put(num)时发生阻塞，在它被叫醒之前是没有办法进入下次循环判断canceled的值的，这种情况下volatile没有办法让生产者停下来的，如果用interrupt语句来中断，即使生产者处于阻塞状态，仍然能够感受到中断信号，并做相应处理。 1.6 线程是如何在6种状态之间转换的？线程的6种状态 New(新建) Runnable(可运行) Blocked(被阻塞) Waiting(等待) Timed Waiting(计时等待) Terminated(被终止) New 新建New表示线程被创建但尚未启动的状态：new Thread()新建一个线程时，如果线程没有开始运行start()方法，所以也没有开始执行run()方法里面的代码，此时它的状态就是New。一旦线程调用了start()，就变成Runnable。 Runnable 可运行Java中的Runnable状态对应操作系统线程状态中的两种状态，分别是Running和Ready，即Java中处于Runnable状态的线程有可能正在执行，也有可能没有正在执行，正在等待被分配CPU资源。所以，如果一个正在运行的线程是Runnable状态，当它运行到任务的一半时，执行该线程的CPU被调度去做其他事情，导致该线程暂时不运行，它的状态仍为Runnable，因为它有可能随时被调度回来继续执行任务。 Blocked 被阻塞从Runnable状态进入Blocked状态只有一种可能，就是进入synchronized保护的代码块/方法时没有抢到monitor锁，Blocked仅仅针对synchronized monitor锁。 Waiting 等待线程进入Waiting 没有设置Timeout参数的Object.wait()方法 没有设置Timeout参数的Thread.join()方法 LockSupport.park()方法 Blocked与Waiting的区别是Blocked在等待其它线程释放monitor锁，而Waiting则是在等待某个条件，比如join的线程执行完毕，或者是notify()/notifyAll()。 Timed Waiting 限期等待Waiting和Time Waiting区别：有没有时间限制，Timed Waiting会等待超时，由系统自动唤醒，或者在超时前被唤醒信号唤醒。 线程进入Timed Waiting 设置了时间参数的Thread.sleep(long millis)方法 设置了时间参数的Object.wait(long timeout)方法 设置了时间参数的Thread.join(long millis)方法 设置了时间参数的LockSupport.parkNanos(long nanos)方法和LockSupport.parkUntil(long deadline)方法 Blocked—&gt;Runnable：线程获取monitor锁 Waiting—&gt;Runnable：执行了LockSupport.unpark()，或join的线程运行结束，或者被中断。 Waiting—&gt;Blocked：其它线程调用notify()或notifyAll()，因为唤醒Waiting线程的线程如果调用notify()或notifyAll()，必须首先持有该monitor锁，所以处于Waiting状态的线程被唤醒时拿不到该锁，就会进入Blocked状态，直到执行notify()/notifyAll()的唤醒线程执行完毕并释放monitor锁，才可能轮到它去抢夺这把锁，抢到就会从Blocked状态回到Runnable状态。 TimedWaiting类似，但如果它的超时时间到了且能直接获取到锁/join的线程运行结束/被中断/调用了LockSupport.unpark()，会直接恢复到Runnable状态。 Terminated线程进入Terminated run()方法执行完毕，线程正常退出。 出现一个没有捕获的异常，终止了run()方法，最终导致意外终止。 Tips 线程的状态是按照箭头方向走的，如线程从New不可以进入Blocked，它需要经历Runnable。 线程的生命周期不可逆：一旦进入Runnable就不能回到New状态；一旦被终止就不可能有任何状态的变化。所以一个线程只有一次New和Terminated状态，只有处于中间状态才可以相互转换。 1.7 为什么wait必须在synchronized保护的同步代码中使用？1.8 为什么wait/notify/notifyAll方法被定义在Object类中，而sleep定义在Thread类中？ Java中每个对象都有一把称之为monitor监视器的锁，由于每个对象都可以上锁，这就要求在对象头中有一个用来保存锁信息的位置。这个锁是对象级别的，而非线程级别的，wait/notify/notifyAll也都是锁级别的操作，它们的锁属于对象，所以把它们定义在Object类，因为Object类是所有对象的父类。 如果把wait/notify/notifyAll方法定义在Thread类中，会带有很大的局限性，如一个线程可能持有多个锁。如何明确当前线程等待的是哪把锁呢？既然是让当前线程去等待某个对象的锁，自然应该通过操作对象来实现。 1.9 wait/notify和sleep方法的异同相同点 都可以让线程阻塞 都可以响应interrupt中断：在等待的过程中如果收到中断信号，都可以进行响应，并抛出InterruptedException 不同点 wait方法必须在synchronized保护的代码中使用，而sleep方法并没这个要求。 在同步代码块中执行sleep方法，并不会释放monitor锁，但执行wait方法时会主动释放monitor锁。 sleep方法必须定义一个时间，时间到期后会主动会恢复，而对于没有参数的wait方法而言，意味着永久等待，直到被中断或被唤醒才能恢复，它并不主动恢复。 wait/notify是Object类的方法，而sleep是Thread类的方法。 2.线程安全如果某个对象是线程安全的，即使用时就不需要考虑方法间的协调问题。 2.1 3种典型的线程安全问题 运行结果错误 12345678910111213141516171819202122public class WrongResult &#123; volatile static int i; public static void main(String[] args) throws InterruptedException &#123; Runnable r = new Runnable() &#123; @Override public void run() &#123; for (int j = 0; j &lt; 10000; j++) &#123; i++; &#125; &#125; &#125;; Thread thread1 = new Thread(r); thread1.start(); Thread thread2 = new Thread(r); thread2.start(); thread1.join(); thread2.join(); System.out.println(i); &#125;&#125; i++并不是一个原子操作 发布或初始化导致线程安全问题 12345678910111213141516171819202122232425public class WrongInit &#123; private Map&lt;Integer, String&gt; students; public WrongInit() &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; students = new HashMap&lt;&gt;(); students.put(1, \"王小美\"); students.put(2, \"钱二宝\"); students.put(3, \"周三\"); students.put(4, \"赵四\"); &#125; &#125;).start(); &#125; public Map&lt;Integer, String&gt; getStudents() &#123; return students; &#125; public static void main(String[] args) &#123; WrongInit wrongInit = new WrongInit(); System.out.println(wrongInit.getStudents().get(1)); &#125;&#125; students 这个成员变量是在构造函数中新建的线程中进行的初始化和赋值操作，而线程的启动需要一定的时间，但是我们的 main 函数并没有进行等待就直接获取数据，导致 getStudents 获取的结果为 null，这就是在错误的时间或地点发布或初始化造成的线程安全问题。 活跃性问题 分别为死锁、活锁和饥饿 死锁：两个线程之间相互等待对方资源，但同时又互不相让，都想自己先执行，如代码所示。 12345678910111213141516171819202122232425262728293031323334353637383940public class MayDeadLock &#123; Object lock1 = new Object(); Object lock2 = new Object(); public void thread1() throws InterruptedException &#123; synchronized (lock1)&#123; Thread.sleep(500); synchronized (lock2)&#123; System.out.println(\"线程1成功拿到两把锁\"); &#125; &#125; &#125; public void thread2() throws InterruptedException &#123; synchronized (lock2)&#123; Thread.sleep(500); synchronized (lock1)&#123; System.out.println(\"线程2成功拿到两把锁\"); &#125; &#125; &#125; public static void main(String[] args) &#123; MayDeadLock deadLock = new MayDeadLock(); new Thread(new Runnable() &#123; @SneakyThrows @Override public void run() &#123; deadLock.thread1(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @SneakyThrows @Override public void run() &#123; deadLock.thread2(); &#125; &#125;).start(); &#125;&#125; 活锁：与死锁类似，不过活锁是活的，因为正在运行的线程并没有阻塞，它始终在运行，缺一直得不到结果。假设有一个消息队列里放着需要被处理的消息，而某个消息由于自身的错误无法被正确处理，同时队列的重试机制会把它放在队列头进行优先重试处理。 饥饿：线程需要某些资源始终得不到，尤其是CPU资源，就会导致线程一直不能运行。 在Java中有1-10的线程优先级，1最低，10最高。如果某个线程的优先级为1，该线程就有可能始终分配不到CPU资源，而导致长时间无法运行。 或者是某个线程始终持有某个文件的锁，其他线程想要修改文件必须先获取锁，这时想要修改文件的线程就会陷入饥饿。2.2 需要额外注意线程安全的场景 访问共享变量和资源 如访问共享对象的属性、访问static静态变量、访问共享的缓存等。 依赖时序的操作 123if (map.containsKey(key)) &#123; map.remove(obj)&#125; 不同数据之间存在绑定关系 不同的数据之间是成组出现的，存在着相互对应或绑定的关系，最典型的就是IP和端口号。 对方没有声明自己是线程安全的 2.3 为什么多线程会带来性能问题单线程是独立工作的，不需要与其他线程进行交互，但多线程之间则需要调度以及协作，调度与协作就会带来性能开销从而产生性能问题。 调度开销 上下文切换：线程数往往大于CPU核心数，操作系统会按照一定的调度算法，给每个线程分配时间片。而在进行调度时就会引起上下文切换，上下文切换会挂起当前正在执行的线程并保存当前的状态，然后寻找下一处即将恢复执行的代码，唤醒下一个线程。 缓存失效：进行了线程调度，切换到其他线程，CPU就会去执行不同的代码，原有的缓存就很有可能失效了，需要重新缓存新的数据。 给被调度到的线程设置最小执行时间，即只有执行完这段时间后，才可能进行下一次的调度，由此减少上下文切换的次数。 协作开销 为了避免共享数据错乱、保证线程安全，就有可能禁止编译器和CPU对其进行重排序等优化，也可能出于同步的目的，反复把线程工作内存的数据flush到主内存，然后再从主内存refresh到其他线程的工作内存中。 2.4 使用线程池的好处 线程池可以解决线程生命周期的系统开销问题，线程池里的线程可以复用，消除了线程创建带来的延迟，从而提高响应速度。 线程池可以统筹内存和CPU的使用，避免资源的使用不当。 线程池可以统一管理资源。 2.5 线程池各参数的含义 线程池的特点： 线程池希望保持较少的线程数，只有在负载变的很大时才增加线程。 线程池只有在任务队列满时才会创建多于corePoolSize的线程，如果使用的是无界队列(如LinkedBlockingQueue)，线程数不会超过corePoolSize。 设置corePoolSize和maxPoolSize为相同的值，可以创建固定大小的线程池。 2.6 线程池有哪几种拒绝策略？ AbortPolicy：拒绝任务时直接抛出一个类型为RejectedExecutionException的RuntimeException，可以感知到任务被拒绝了，可以根据业务逻辑选择重试或放弃提交等。 DiscardPolicy：当新任务被提交后直接被丢弃掉，不会有任何通知。 DiscardOldestOlicy：丢弃任务队列的头节点，通常是存活时间最长的任务，也不会有任何通知。 CallerRunsPolicy：把任务交给提交任务的线程执行，即谁提交任务，谁就负责执行任务。 提交的任务不会被丢弃 提交任务的线程负责执行任务，提交任务的线程被占用，不会再提交新的任务，线程池中的线程也可以利用这段时间执行掉一部分任务，相当于是给了线程池一定的缓冲期。 2.7 有哪6种常见的线程池？什么是Java8的ForkJoinPool? FixedThreadPool 核心线程数和最大线程数是一样的，可以看作是固定线程数的线程池，没有可用的线程的时候，任务会放在队列中等待，任务的长度无限制(LinkedBlockingQueue) CachedThreadPool 线程数几乎可以无限增加(Integer.MAX_VALUE，2^31-1)，该线程池的线程数量不固定，不够使用时自动增加，闲置时自动回收。队列为SynchronousQueue，队列容量为0，实际不存储任务，只对任务进行中转和传递。 ScheduledThreadPool 支持定时或周期的执行任务。 1234567ScheduledExecutorService service = Executors.newScheduledThreadPool(10);//延迟指定时间后执行一次任务，10秒执行一次service.schedule(new Task(), 10, TimeUnit.SECONDS);//以固定的频率执行任务service.scheduleAtFixedRate(new Task(), 10, 10, TimeUnit.SECONDS);//与第二种类似，不过scheduledAtFixedRate以开始时间为起点，时间到就开始第二次，而scheduledWithFixedDelay以任务结束时间为下一次循环的时间起点开始计算service.scheduleWithFixedDelay(new Task(), 10, 10, TimeUnit.SECONDS); SingleThreadExecutor 原理与FixedThreadPool一样，线程只有一个，如果线程在执行过程中发生异常，线程池也会重新创建一个线程来执行后续的任务。适合用于任务需要按被提交的顺序依次执行的场景。 SingleThreadScheduledExecutor 于ScheduledThreadPool类似，如源码所示：只是将ScheduledThreadPool的核心线程数设置为1 1new ScheduledThreadPoolExecutor(1) ForkJoinPool 2.8 线程池常用的阻塞队列 LinkedBlockingQueue 对于FixedThreadPool和SingleThreadExector，它们使用的是容量为Integer.MAX_VALUE的LinkedBlockingQueue，可以任务是无界队列。 SynchronousQueue 对于CachedThreadPool，最大线程数为Integer.MAX_VALUE，所以不需要任务队列来存储任务，一旦有任务提交就直接转发给线程或创建新线程来执行。 DelayedWorkQueue 对于ScheduledThreadPool和SingleThreadScheduledExecutor，DelayedWorkQueue内部元素并不是按照放入的时间排序，而是按照延迟的时间长短对任务进行排序，内部采用的是”堆”的数据结构。 2.9 为什么不应该自动创建线程池？ FixedThreadPool、SingleThreadPool 使用的队列是没有上限的LinkedBlockingQueue，如果处理任务过慢，队列中堆积的任务会越来越多，占用大量内存，导致OOM。 CachedThreadPool 不限制线程的数量，任务特别多时，有可能会创建非常多的线程，最终导致超过了操作系统的上限而无法创建线程，或导致内存不足。 ScheduledThreadPool、SingleThreadScheduledExecutor DelayedWorkQueue也是一个无界队列。 2.10 合适的线程数是多少？ CPU密集型任务 如加密、解密、压缩、计算等大量耗费CPU资源的任务，线程数为CPU核心数的1-2倍。 耗时IO型任务 如数据库、文件的读写、网络通信等并不消耗CPU资源的任务，线程数=CPU核心数*(1+平均等待时间/平均工工作时间) 线程的平均工作时间所占比例越高，就需要越少的线程。线程的平均等待时间所占比例越高，就需要越多的线程。 2.11 如何正确关闭线程？ shutdown() 安全的关闭的一个线程池，调用shutdown()之后，如果还有新任务被提交，线程池会根据拒绝策略直接拒绝后续提交的任务，执行完正在执行的任务和队列中等待的任务后关闭。 isShutdown() 判断线程是否已经开始了关闭工作，即是否执行了shutdown()或shutdownNow() isTerminated() 检测线程池是否真正”终结”了，即线程池已关闭，同时线程池中的所有任务都执行完毕了。 awaitTermination() 判断线程池状态，如给awaitTermination方法传入的参数为10秒，那么它会陷入10秒等待，直到 等待期间(包括进入等待之前)，线程池已关闭并所有任务都执行完毕，相当于线程池”终结”了，方法便返回true。 等待超时时间到后，线程池始终未”终结”，返回false。 等待期间线程被中断，方法抛出InterruptedException异常。 即调用awaitTermination方法后当前线程池会尝试等待一定指定的时间，如果在等待时间内，线程池已关闭并任务都执行完毕，方法返回true，否则返回false。 shutdownNow() 立刻关闭，执行shutdownNow()方法之后，首先会给线程池中的线程发送interrupt中断信号，尝试中断这些任务的执行，然后会将等待的所有任务转移到一个List中并返回。 3.各种各样的”锁”3.1 你知道哪几种锁？分别有什么特点？ 偏向锁/轻量级锁/重量级锁 特指synchronized锁的状态，通过在对象头中的mark word来表明锁的状态。 偏向锁 如果，这把锁一直不存在竞争，就没必要上锁，只需打个标记就行。对象被初始化，还没有线程来获取它的锁时，那么它就是可偏向的，当有第一个线程来访问它并尝试获取锁的时候，它就将这个线程记录下来，以后如果尝试获取锁的线程正是偏向锁的拥有者，就可以直接获取锁，开销很小，性能最好。 轻量级锁 synchronized中的代码是被多个线程交替执行的，并不存在实际的竞争、或只有短时间的竞争，用CAS就可以解决。轻量级锁是指当锁原来是偏向锁时，被另一个线程访问，说明存在竞争，那么偏向锁就会升级为轻量级锁，线程会通过自旋的形式获取锁，而不会陷入阻塞。 重量级锁 重量级锁是互斥锁，它是利用操作系统的同步机制实现的，开销相对较大。当多个线程直接实际竞争，且锁竞争时间长的时候，锁就会膨胀为重量级锁。重量级锁会让其它申请缺拿不到锁的线程进入到阻塞状态。 偏向锁性能最好，可以避免执行CAS操作。而轻量级锁利用自旋和CAS避免了重量级锁带来的线程阻塞和唤醒，性能中等。重量级锁则会把获取不到锁的线程阻塞，性能最差。 可重入锁/不可重入锁 可重入锁指的是线程当前已经持有这把锁了，能在不释放这个锁的情况下，再次获取这把锁。不可重入锁指的是虽然当前持有了这把锁，但如果想再次获取此锁，也必须先要释放锁后才能再次尝试获取。 共享锁/独占锁 共享锁指同一把锁可以被多个线程同时获得，而独占锁指这个锁只能同时被一个线程获得。如读写锁中的读锁是共享锁，而写锁是独占锁。 公平锁/非公平锁 公平锁 如果线程现在拿不到这把锁，那么线程都会进入等待，开始排队，在等待队列等待时间长的线程会优先拿到这把锁，先来先得。 非公平锁 在一定情况下，忽略掉已经在排队的线程，发生插队现象。 悲观锁/乐观锁 悲观锁 在获取资源之前，必须先拿到锁，以便达到”独占”的状态。 乐观锁 并不要求在获取资源前拿到锁，也不会锁住资源，利用CAS理念，在不独占资源的情况下，完成对资源的修改。 自旋锁/非自旋锁 自旋锁 如果线程现在拿不到锁，并不直接陷入阻塞或者释放CPU资源，而是开始利用循环，不停的尝试获取锁。 非自旋锁 拿不到锁就直接放弃，或者进行其它的处理逻辑，如阻塞、排队等。 可中断锁/不可中断锁 synchronized关键字修饰的锁代表的是不可中断锁，一旦线程申请了锁，就没有回头路，只能等拿到锁以后才能进行其它的逻辑处理。 ReentrantLock是一种典型的可中断锁，如使用lockInterruptibly方法在获取锁的过程中，突然不想获取了，可以在中断之后去做其它的事。 3.2 悲观锁与乐观锁 悲观锁 为了确保结果的正确性，会在每次获取并修改数据时，都把数据锁住，让其他线程无法访问。 线程A拿到了锁，并且正在操作同步资源，那么此时线程B就必须进行等待。 当线程A执行完毕后，CPU才会唤醒正在等待这把锁的线程B再次尝试获取锁 如果线程B获取到了锁，才可以对同步资源进行自己的操作。 乐观锁 认为自己在操作资源的时候不会有其他线程干扰，所以并不会锁住被操作对象。为了确保数据正确性，在更新之前，会去对比在修改数据期间，数据有没有被其他线程修改过。 例子： 悲观锁：synchronized关键字和Lock接口 以Lock接口为例，如Lock的实现类ReentrantLock，类中的lock()等方法就是执行加锁，而unlock()方法就是执行解锁()。处理资源之前必须要先加锁并拿到锁，等到处理完之后再解开锁。 乐观锁：原子类 如AtomicInteger在更新数据时，多个线程可以同时操作同一个原子变量。 两种锁各自的使用场景： 悲观锁适合于并发写入多、临界区代码复杂、竞争激烈等场景，此时悲观锁可以避免大量的无用的反复尝试等消耗。 乐观锁适用于读取多，修改少的场景，也适合虽然读写都很多，但是并发不激烈的场景。 3.3 synchronized背后的monitor锁获取和释放monitor锁的时机：线程在进入synchronized保护的代码块之前，会自动获取锁；并且无论是正常退出，还是抛出异常退出，在退出的时候都会自动释放锁。 查看反汇编命令：javac SynTest.java javap -verbose SynTest.class 同步代码块 123456789101112 ......3: monitorenter 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: ldc #3 // String lagou 9: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 12: aload_1 13: monitorexit 14: goto 22 17: astore_2 18: aload_1 19: monitorexit ...... monitorenter可以理解为加锁，monitorexit理解为释放锁，每个对象维护着一个记录着被锁次数的计数器。未锁定的对象的该计数器未0。 monitorenter 如果该monitor的计数为0，则线程获得该monitor并将其计数设置为1，该线程就是这个monitor的所有者。 如果线程已经拥有了这个monitor，则它将重新进入，并且累加计数。 如果其他线程已经拥有了这个monitor，那么这个线程就会被阻塞，直到这个monitor的计数器变为0，代表这个monitor已经被释放了，于是当前这个线程就会再次尝试获取这个monitor。 monitorexit 作用：将monitor的计数器减1，直到减为0为止。代表这个monitor已经被释放了，已经没有任何线程拥有它了，也就代表着解锁。其他正在等待这个monitor的线程，此时可以再次尝试获取这个monitor的所有权。 同步方法 123456 public synchronized void synMethod(); descriptor: ()V flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=0, locals=1, args_size=1...... 被synchronized修饰的方法会有一个ACC_SYNCHRONIZED标志，当某个线程要访问某个方法时，会首先检查方法是否有ACC_SYNCHRONIZED标志，如果有则需要先获得monitor锁，方法执行之后再释放monitor锁。3.4 synchronized与Lock 相同点 synchronized和Lock都是用来保护资源线程安全的 都可以保证可见性 synchronized和ReentrantLock(Lock的一个实现类)都拥有可重入的特点 不同点 用法区别 synchronized关键字可以加在方法上，不需要指定锁对象(此时的锁对象为this)；也可以修饰同步代码块并且自定义monitor对象。而Lock锁对象必须显示的开始加锁lock()和解锁unlock()，并且一般会在finally块中确保用unlock()来解锁，以防止发生死锁。 加解锁顺序不同 对于Lock而言如果有多把Lock锁，Lock可以不完全按照加锁的反序解锁 12345lock1.lock();lock2.lock();...lock1.unlock();lock2.unlock(); synchronized解锁的顺序和加锁的顺序必须完全相反，obj2先解锁，obj1后解锁。 12345synchronized(obj1)&#123; synchronized(obj2)&#123; ... &#125;&#125; synchronized锁不够灵活 一旦synchronized锁已经被某个线程获得了，此时其他线程如果还想获得，那么它只能被阻塞，直到持有锁的线程运行完毕或发生异常从而释放这个锁。Lock类在等待锁的过程中，如果使用的时lockInterruptibly方法，如果等待时间太长，可以中断退出，也可以使用tryLock()等方法尝试获取锁，如果获取不到可以执行其他逻辑。 synchronized锁只能同时被一个线程拥有，但Lock锁没有这个限制。 如在读写锁中的读锁，是可以被多个线程同时拥有的，但synchronized不行。 原理区别 synchronized是内置锁，由JVM实现获取锁和解锁，还分为偏向锁、轻量级锁、重量级锁。Lock根据实现不同，原理也不同，如ReentrantLock内部是通过AQS来获取和释放锁的。 是否可以设置公平/非公平 ReentrantLock可以根据需求来设置公平或非公平，synchronized则不能设置。 如何选择： 最好既不使用Lock也不使用synchronized，尽量使用java.util.concurrent包中的机制。 尽量使用synchronized，避免忘记在finally里忘记unlock。 需要Lock的特殊功能时，如尝试获取锁、可中断、超时功能等，才使用Lock。 3.5 Lock的常用方法 lock() 在线程获取锁时如果锁已被其他线程获取 12345678Lock lock = ...;lock.lock();try&#123; //获取到了被本锁保护的资源，处理任务 //捕获异常&#125;finally&#123; lock.unlock(); //释放锁&#125; tryLock() 用来尝试获取锁，如果当前锁没有被其他线程占用，则获取成功，返回true，否则返回false，代表获取锁失败，可以根据是否能获取到锁来决定后续程序行为。 12345678910Lock lock = ...;if(lock.tryLock()) &#123; try&#123; //处理任务 &#125;finally&#123; lock.unlock(); //释放锁 &#125; &#125;else &#123; //如果不能获取锁，则做其他事情&#125; tryLock(long time, TimeUnit unit) 和tryLock()类似，tryLock(long time, TimeUnit unit)会有一个超时时间，在拿不到锁时会等待一定的时间，时间期限结束后，还获取不到锁，就会返回false，如果在最开始或等待期间内获取到锁就返回true。 lockInterruptibly() 除非当前线程在获取锁期间被中断，否则会一直尝试获取直到获取到为止。相当于超时时间无限长的tryLock(long time, TimeUnit unit)。 123456789101112 public void lockInterruptibly() &#123; try &#123; lock.lockInterruptibly(); try &#123; System.out.println(\"操作资源\"); &#125; finally &#123; lock.unlock(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; unlock() 用于解锁，对ReentrantLock而言，执行unlock()的时候，内部会把锁的”被持有计数器”减1，直到减到0就代表当前这把锁已经完全释放了，如果减1后计数器不为0，说明这把锁之前被”重入”了，那么锁并没有真正释放，仅仅是减少了持有的次数。 3.6 公平锁与非公平锁公平锁：按照线程请求顺序来分配锁 非公平锁：不完全按照请求的顺序，在一定情况下，可以允许插队。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class FairAndUnfair &#123; static class PrintQueue &#123; private final Lock queueLock = new ReentrantLock(false);//false:非公平锁 true:公平锁 默认false public void printJob(Object document) &#123; queueLock.lock(); try &#123; Long duration = (long)(Math.random() * 10000); System.out.printf(\"%s: PrintQueue: Printing a Job during %d seconds\\n\", Thread.currentThread().getName(), (duration / 1000)); Thread.sleep(duration); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; queueLock.unlock(); &#125; queueLock.lock(); try &#123; Long duration = (long)(Math.random() * 10000); System.out.printf(\"%s: PrintQueue: Printing a Job during %d seconds\\n\", Thread.currentThread().getName(), (duration / 1000)); Thread.sleep(duration); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; queueLock.unlock(); &#125; &#125; &#125; static class Job implements Runnable &#123; private PrintQueue printQueue; public Job(PrintQueue printQueue) &#123; this.printQueue = printQueue; &#125; @Override public void run() &#123; System.out.printf(\"%s: Going to print a job\\n\", Thread.currentThread().getName()); printQueue.printJob(new Object()); System.out.println(); System.out.printf(\"%s: The document has been printed\\n\", Thread.currentThread().getName()); &#125; &#125; public static void main(String[] args) &#123; PrintQueue printQueue = new PrintQueue(); Thread thread[] = new Thread[10]; for (int i = 0; i &lt; 10; i++) &#123; thread[i] = new Thread(new Job(printQueue), \"Thread \" + i); &#125; for (int i = 0; i &lt; 10; i++) &#123; thread[i].start(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 非公平情况下，存在抢锁”插队”现象，如Thread 0 在释放锁后又能优先获取到锁，虽然此时在等待队列中已经有Thread 1~Thread 9在排队了。 各自的优缺点 源码分析 ReentrantLock中包含一个Sync类，这个类继承自AQS(AbstractQueuedSynchronizer) 1234public class ReentrantLock implements Lock, java.io.Serializable &#123;private static final long serialVersionUID = 7373984872572414699L;/** Synchronizer providing all implementation mechanics */private final Sync sync; Sync有公平锁FairSync和非公平锁NonfairSync两个子类 12static final class NonfairSync extends Sync &#123;...&#125;static final class FairSync extends Sync &#123;...&#125; 公平锁与非公平获取锁的lock()方法唯一区别就在于公平锁在获取锁时多了一个限制条件：hasQueuedPredecessors()为false，这个方法就是在判断在等待队列中是否已经有线程在排队了。公平锁，一旦有线程在排队，当前线程就不再尝试获取锁了；对于非公平锁，无论是否有线程在排队，都会尝试获取一下锁，获取不到的话，再去排队。 tryLock()，一旦有线程释放了锁，那么正在tryLock的线程就能获取到锁，即使设置的是公平锁模式，即使在它之前已经有其他正在等待队列中等待的线程，即tryLock可以插队。调用的是nonfairTryAcquire()，表明是不公平的，和锁本身是否公平锁无关。 123public boolean tryLock() &#123; return sync.nonfairTryAcquire(1);&#125; 3.7 读写锁保证多个线程同时读的效率，同时可以保证有写入操作时的线程安全。 读写锁的获取规则 如果一个线程已经占用了读锁，则此时其他线程如果要申请读锁，可以申请成功。 如果有一个线程已经占用了读锁，则此时其他线程如果要申请写锁，则申请写锁的线程会一直等待释放读锁，因为读写不能同时操作。 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或读锁，都必须等待之前的线程释放锁，因为读写、写写不能同时操作。 要么是一个或多个线程同时有读锁，要么是一个线程有写锁，但是两者不会同时出现。即读读共享，其他都互斥。 1234567891011121314151617181920212223242526272829303132333435363738public class ReadWriteLockDemo &#123; private static final ReentrantReadWriteLock reentrantReadWriteLock = new ReentrantReadWriteLock(false); private static final ReentrantReadWriteLock.ReadLock readLock = reentrantReadWriteLock.readLock(); private static final ReentrantReadWriteLock.WriteLock writeLock = reentrantReadWriteLock.writeLock(); private static void read() &#123; readLock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \"得到读锁，正在读取\"); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(Thread.currentThread().getName() + \"释放读锁\"); readLock.unlock(); &#125; &#125; private static void write() &#123; writeLock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \"得到写锁，正在写入\"); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(Thread.currentThread().getName() + \"释放写锁\"); writeLock.unlock(); &#125; &#125; public static void main(String[] args) &#123; new Thread(() -&gt; read()).start(); new Thread(() -&gt; read()).start(); new Thread(() -&gt; write()).start(); new Thread(() -&gt; write()).start(); &#125;&#125; 运行结果： 12345678Thread-0得到读锁，正在读取Thread-1得到读锁，正在读取Thread-0释放读锁Thread-1释放读锁Thread-2得到写锁，正在写入Thread-2释放写锁Thread-3得到写锁，正在写入Thread-3释放写锁 读写锁适用于读多写少的情况 3.8 读锁应该插队么？什么是读写锁的升降级？ 公平锁 只要等待队列中有线程在等待，即hasQueueedPredecessors()返回true的时候，那么write和reader都会block，即不允许插队。 非公平锁 1234567final boolean writerShouldBlock() &#123; return false; // writers can always barge&#125;final boolean readerShouldBlock() &#123; return apparentlyFirstQueuedIsExclusive();&#125; 写锁：随时可以插队 读锁： 允许插队 有可能导致需要拿到写锁的线程会陷入”饥饿”状态，它将在长时间内得不到执行。 不允许插队 即使是非公平锁，只要等待队列的头结点是尝试获取写锁的线程，那么读锁依然不能插队，目的是避免”饥饿”。 1234567891011121314151617181920212223242526272829303132333435363738public class ReadLockJumpQueue &#123; private static final ReentrantReadWriteLock reentrantReadWriteLock = new ReentrantReadWriteLock(); private static final ReentrantReadWriteLock.ReadLock readLock = reentrantReadWriteLock.readLock(); private static final ReentrantReadWriteLock.WriteLock writeLock = reentrantReadWriteLock.writeLock(); private static void read() &#123; readLock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \"得到读锁，正在读取\"); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(Thread.currentThread().getName() + \"释放读锁\"); readLock.unlock(); &#125; &#125; private static void write() &#123; writeLock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \"得到写锁，正在写入\"); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(Thread.currentThread().getName() + \"释放写锁\"); writeLock.unlock(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; new Thread(() -&gt; read(), \"Thread-2\").start(); new Thread(() -&gt; read(), \"Thread-4\").start(); new Thread(() -&gt; write(), \"Thread-3\").start(); new Thread(() -&gt; read(), \"Thread-5\").start(); &#125;&#125; 运行结果： 12345678Thread-2得到读锁，正在读取Thread-4得到读锁，正在读取Thread-2释放读锁Thread-4释放读锁Thread-3得到写锁，正在写入Thread-3释放写锁Thread-5得到读锁，正在读取Thread-5释放读锁 锁的升降级 1234567891011121314151617181920212223242526272829303132public class CachedData &#123; Object data; volatile boolean cacheValid; final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); void processCachedData() &#123; rwl.readLock().lock(); if (!cacheValid) &#123; //在获取写锁之前，必须首先释放读锁。 rwl.readLock().unlock(); rwl.writeLock().lock(); try &#123; //这里需要再次判断数据的有效性,因为在我们释放读锁和获取写锁的空隙之内，可能有其他线程修改了数据。 if (!cacheValid) &#123; data = new Object(); cacheValid = true; &#125; //在不释放写锁的情况下，直接获取读锁，这就是读写锁的降级。 rwl.readLock().lock(); &#125; finally &#123; //释放了写锁，但是依然持有读锁 rwl.writeLock().unlock(); &#125; &#125; try &#123; System.out.println(data); &#125; finally &#123; //释放读锁 rwl.readLock().unlock(); &#125; &#125;&#125; 只有一处修改数据的代码，后面都是读取，如果一直使用写锁的话，就不能让多个线程同时来读取了，这个时候利用锁的降级，可以提高整体性能。 支持锁的降级，不支持升级 ReentrantReadWriteLock不支持读锁升级到写锁。 不可能有读锁和写锁同时持有的情况，升级写锁的过程中，需要等到所有的读锁都释放才能升级。另一种特殊情况，线程A、B都想升级到写锁，对于A而言，它需要等待其他线程(包括B)释放读锁，而线程B也是如此，则会发生死锁。 3.9 自旋锁 非自旋锁和自旋锁最大的区别，如果它遇到拿不到锁的情况，它会把线程阻塞，直到被唤醒；而自旋锁会不停地尝试。 自旋锁的好处 自旋锁用循环去不停地尝试获取锁，让线程始终处于Runnable状态，节省了线程切换带来的开销。 自己实现可重入的自旋锁： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class ReentrantSpinLock &#123; private AtomicReference&lt;Thread&gt; owner = new AtomicReference&lt;&gt;(); // 重入次数 private int count = 0; public void lock() &#123; Thread currentThread = Thread.currentThread(); if (currentThread == owner.get()) &#123; ++count; return; &#125; // 自旋获取锁 while (!owner.compareAndSet(null, currentThread)) &#123; System.out.println(\"自旋了！\"); &#125; &#125; public void unlock() &#123; Thread currentThread = Thread.currentThread(); // 只有持有锁的线程才能解锁 if (currentThread == owner.get()) &#123; if (count &gt; 0) &#123; --count; &#125; else &#123; // 此处无需CAS操作，因为没有竞争，因为只有线程持有者才能解锁 owner.set(null); &#125; &#125; &#125; public static void main(String[] args) &#123; ReentrantSpinLock spinLock = new ReentrantSpinLock(); Runnable runnable = new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + \"开始尝试获取自旋锁\"); spinLock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \"获取到了自旋锁\"); Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; spinLock.unlock(); System.out.println(Thread.currentThread().getName() + \"释放了了自旋锁\"); &#125; &#125; &#125;; Thread thread1 = new Thread(runnable); Thread thread2 = new Thread(runnable); thread1.start(); thread2.start(); &#125;&#125; 运行结果： 123456789101112Thread-1开始尝试获取自旋锁Thread-0开始尝试获取自旋锁Thread-1获取到了自旋锁自旋了！自旋了！自旋了！......自旋了！自旋了！Thread-0获取到了自旋锁Thread-1释放了了自旋锁Thread-0释放了了自旋锁 缺点 虽然避免了线程切换的开销，但带来了新的开销，因为它需要不停地去尝试获取锁。 适用场景 自旋锁适用于并发度不是特别高，以及临界区比较短小的情况，这样可以避免线程切换来提高效率。可是如果临界区很大，线程一旦拿到锁，很久才会释放的话，那就不适合自旋锁，因为自旋会一直占用CPU却无法拿到锁，白白消耗资源。 3.10 JVM对锁的优化 自适应的自旋锁 自旋的缺点在于如果自旋时间过长，那么性能开销很大，浪费CPU资源。自适应意味着自旋的时间不再固定，而是根据最近自旋尝试的成功率、失败率，以及当前锁的拥有者的状态等多种因素来共同决定。如：最近尝试自旋获取某一把锁成功了，那么下次可能还会继续使用自旋，并且允许自旋更长时间；但如果最近自旋获取某一把锁失败了，那么可能会省略掉自旋的过程，以便减少无用的自旋，提高效率。 锁消除 123456@Overridepublic synchronized StringBuffer append(Object obj) &#123; toStringCache = null; super.append(String.valueOf(obj)); return this;&#125; 这个方法是被synchronized修饰的同步方法，因为它可能会被多个线程同时使用。但在大多数情况下，它只会在一个线程内使用，如果编译器能确定这个StringBuffer只会在一个线程内使用，那么编译器便会做出优化，把synchronized消除，省去加锁和解锁，以便增加整体的效率。 锁粗化 如果释放了锁，紧接着什么都没做，又重新获取锁，如： 12345678public void lockCoarsening() &#123; synchronized (this) &#123; //do something &#125; synchronized (this) &#123; //do something &#125;&#125; 可以把同步区域扩大，即最开始加一次锁，并且在最后直接解锁，减少性能开销。 如果在循环中也这样做，会导致其他线程长时间无法获得锁。锁粗化的功能默认打开，用-XX:-EliminateLocks可以关闭该功能。 12345for (int i = 0; i &lt; 1000; i++) &#123; synchronized (this) &#123; //do something &#125;&#125; 偏向锁/轻量级锁/重量级锁 这三种锁是特指synchronized锁的状态的，通过对象头中的mark word来表明锁的状态。 偏向锁 这把锁自始至终不存在竞争，那么没必要上锁，只要打个标记就行了。一个对象被初始化后，如果还没有任何线程来获取它的锁，它就是可偏向的，当第一个线程来访问它尝试获取锁的时候，它就记录下来这个线程，如果后面尝试获取锁的线程正是这个偏向锁的拥有者，就可以直接获取锁，开销小。 轻量级锁 synchronized中的代码块是被多个线程交替执行的，也就是不存在实际的竞争，或者只有短时间的竞争，用CAS就可以解决。轻量级锁指当锁原来是偏向锁的时候，被另一线程所访问，说明存在竞争，那么偏向锁升级为轻量级锁，线程会通过自旋的方式尝试获取锁，不会阻塞。 重量级锁 当多个线程直接有实际竞争，并且锁竞争时间比较长的时候，此时偏向锁和轻量级锁都不能满足需求，锁就会膨胀为重量级锁，会让其他申请却拿不到锁的线程进入阻塞状态。 3.10 HashMap为什么是线程不安全的？ 扩容期间取出的值不准确 HashMap扩容期间，会新建一个新的空数组，并用旧的项填充到这个新的数组中。如果这个填充的过程中，如果有线程取值，很可能会取到null值。 同时put碰撞导致数据丢失 如果有多个线程同时put，而且恰好两个put的key是一样的，它们发生了碰撞，也就是根据hash值计算出来的bucket位置一样，并且两个线程又同时判断该位置是空的，可以写入，所以这两个线程的两个不同的value便会添加到数组的同一位置，就丢失了一个数据。 可见性问题 线程1给某个key放入了一个新值，那么线程2在获取对应的key的值的时候，它的可见性是无法保证的。 死循环造成CPU100% 在扩容的时候，也就是内部新建新的HashMap的时候，扩容的逻辑会反转散列桶中的节点顺序，当多个线程同时进行扩容的时候，如果两个线程同时反转的话，便可能形成一个循环，并且这种循环是链表的循环，相当于A节点指向B节点，B节点又指回A节点，在下一次想要获取该key所对应的value的时候，便会在遍历链表的时候发生永远无法遍历结束的情况。 3.11 为什么Map桶中超过8个才转为红黑树？最开始的Map是空的，因为里面没有任何元素，往里放元素时会计算hash值，计算之后，第1个个value会占用一个桶(也称为槽点)位置，后续经过计算键值key计算hash值得到插入的数组索引i相同，那么会使用链表的形式往后延长，俗称拉链法。当链表长度大于或等于阈值(默认为8)，且数组长度大于或等于MIN_TREEIFY_CAPACITY(默认64)时，就会把链表转为红黑树。当红黑树的节点小于或等于6个以后，又会恢复为链表形态。 链表查找时间复杂度：O(n) 红黑树查找时间复杂度：O(log(n)) 单个TreeNode需要占用的空间大约是Node的两倍 时间与空间的平衡 如果hash计算结果离散的好，各个值都均匀分配，很少出现链表很长的情况。在理想情况下，链表长度符合泊松分布，各个长度的命中概率依次递减，当长度为8时，概率仅为0.00000006，小于千万分之一概率，通常情况下并不会发生链表向红黑树的转换。 链表长度为8转为红黑树的设计，为了防止自定义实现了不好的hash算法导致链表长度过长，从而导致查询效率低。 3.12 Hashtable与ConcurrentHashMap的区别 出现版本不同 Hashtable在JDK1.0就存在了，并在JDK1.2实现了Map接口；ConcurrentHashMap在JDK1.5中才出现。 实现线程安全的方式不同 Hashtable通过synchronized关键字实现线程安全；ConcurrentHashMap利用了CAS+synchronized+Node(volatile)。 性能不同 随着线程数量的增加，Hashtable性能会急剧下降，每一次修改会锁住整个对象，而其他线程在此期间不能操作，还会带来额外的上下文切换；ConcurrentHashMap只会对一部分上锁而不是全部都上锁。 迭代时的修改不同 Hashtable(包括HashMap)不允许在迭代期间修改内容，否则会抛出ConcurrentModificationException异常，ConcurrentHashMap不会。 3.13 CopyOnWriteArrayListArrayList LinkedList 线程安全：Vector Collections.synchronized() Vector内部使用synchronized来保证线程安全，并且锁的粒度比较大，都是方法级别的锁，在并发高的时候，很容易发生竞争，并发效率相对较低。 适用场景： 读操作可以尽可能的快，而写即使慢一些也没关系 读多写少 读写规则： 读写锁的思想是：读读共享，其他都互斥，因为读操作不会修改原有的数据，因此并发读不会有安全问题；而写操作发生时，不允许读和写操作加入。CopyOnWriteArrayList读取是完全不用加锁的，并且写入也不会阻塞读取操作，也就是说可以在写入的同时进行读取，只有写入和写入之间需要进行同步，也就是不允许多个写入同时发生，但可以在写入时允许读取发生。 特点： CopyOnWrite 当容器需要被修改的时候，不直接修改当前容器，而是先将当前容器进行Copy，复制出一个新容器，然后修改新的容器，完成修改之后，再将容器的引用指向新的容器。读写分离的思想，读和写使用不同的容器。 迭代期间允许修改集合内容 ArrayList源码里的ListItr的next()方法中有一个checkForComodification()方法： 1234final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException();&#125; modCount是保存修改次数，每次调用add、remove时都会增加，expectedComodification是迭代器的变量，创建迭代器时会初始化并记录当时的modCount，后面迭代期间如果发现modCount和expectedModCount不一致，就会抛出异常。CopyOnWriteArrayList的迭代器在迭代时，迭代器使用的依然是原数组，只不过迭代器的内容可能已经过时了。CopyOnWrite的迭代器一旦被建立，如果往之前的CopyOnWriteArrayList对象中去新增元素，在迭代器中既不会显示出元素的变更情况，同时也不会报错。 缺点： 内存占用问题 在元素较多或者复杂的情况下，复制的开销很大 数据一致性问题 由于CopyOnWrite容器的修改是先修改副本，所以这次修改对于其他线程来说，并不是实时能看到的，只有在修改完之后才能体现出来。 源码分析： 数据结构 1234567891011121314151617181920212223/** 可重入锁对象 */final transient ReentrantLock lock = new ReentrantLock();/** CopyOnWriteArrayList底层由数组实现，volatile修饰，保证数组的可见性 */private transient volatile Object[] array;/*** 得到数组*/final Object[] getArray() &#123; return array;&#125;/*** 设置数组*/final void setArray(Object[] a) &#123; array = a;&#125; /*** 初始化CopyOnWriteArrayList相当于初始化数组*/public CopyOnWriteArrayList() &#123; setArray(new Object[0]);&#125; add()方法 12345678910111213141516171819public boolean add(E e) &#123; // 加锁 final ReentrantLock lock = this.lock; lock.lock(); try &#123; // 得到原数组的长度和元素 Object[] elements = getArray(); int len = elements.length; // 复制出一个新数组 Object[] newElements = Arrays.copyOf(elements, len + 1); // 添加时，将新元素添加到新数组中 newElements[len] = e; // 将volatile Object[] array 的指向替换成新数组 setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; 在添加的时候首先上锁，并复制一个新数组，增加操作在新数组上完成，然后将array指向到新数组，最后解锁。上面的步骤实现了CopyOnWrite的思想：写操作是在原来容器的拷贝上进行的，并且在读取数据的时候不会锁住list。如果对容器拷贝操作的过程中有新的读线程进来，那么读到的还是旧的数据，因为那个时候对象的引用还没有被更改。 迭代器 COWIterator 类 1234private COWIterator(Object[] elements, int initialCursor) &#123; cursor = initialCursor; snapshot = elements;&#125; snapshot：数组的快照，即创建迭代器那个时刻的数组情况 cursor：迭代器的游标 迭代器在被构建的时候，会把当时的elements赋值给snapshot，而之后的迭代器所有的操作都基于snapshot数组进行的，比如： 12345public E next() &#123; if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++];&#125; 可以看到，返回的内容是snapshot对象，所以，后续就算原数组被修改，这样snapshot既不会感知到，也不会受影响，执行迭代操作不需要加锁，也不会因此抛出异常。迭代器返回的结果，和创建迭代器的时候内容一致。 4.阻塞队列4.1 什么是阻塞队列？BlockingQueue，是一个接口，继承了Queue接口，是队列的一种，是线程安全的。 主要并发队列关系图 阻塞队列典型代表就是BlockingQueue接口的实现类，分别是ArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue、DelayQueue、PriorityBlockingQueue和LinkedTransferQueue。非阻塞队列的典型代表是ConcurrentLinkedQueue，这个类不会让线程阻塞，利用CAS保证线程安全。 Deque为双端队列，它从头和尾都能添加和删除元素；而普通的Queue只能从一端进入，另一端出去。 特点 阻塞功能使得生产者和消费者两端的能力得以平衡，当有任何一端速度过快时，阻塞队列便会把过快的速度给降下来。 take方法 获取并移除队列的头结点，在队列里有数据时可以正常移除，一旦执行take方法的时候，队列无数据，则阻塞，直到队列有数据。 put方法 put方法插入元素时，如果队列已满，那么就无法继续插入，则阻塞，直到队列有了空闲空间。 是否有界(容量有多大)无界队列意味着里面可以容纳非常多的元素，如LinkedBlockingQueue的上限是Integer.MAX_VALUE，约为2^31。有些阻塞队列是有界的，如ArrayBlockingQueue如果容量满了，也不会扩容，所以一旦满了，就无法再往里面放数据了。 4.2 阻塞队列常用方法第一组：无法正常执行的情况下抛出异常；第二组：在无法正常执行的情况下不抛出异常，但会用返回值提示运行失败；第三组：在遇到特殊情况时让线程阻塞，等到可以运行再继续执行。 带有超时时间的offer和poll 1offer(E e, long timeout, TimeUnit unit) 插入不成功时会等待指定的超时时间，时间到了依然没有插入成功，就会返回false 1poll(long timeout, TimeUnit unit) 如果移除时，如果队列是空的就会进行等待，超时时间到了，如果队列中依然没有元素可供移除，则会返回null为提示 4.3 几种常见的阻塞队列 ArrayBlockingQueue 有界队列，其内部是用数组存储元素的，利用ReentrantLock实现线程安全，在创建它的时候就需要指定它的容量，之后不可以再扩容了，可以在构造函数中指定是否公平。 非公平：存在插队的可能；公平：等待最长时间的线程会被优先处理 LinkedBlockingQueue 内部用链表实现，不指定容量时默认为Integer.MAX_VALUE，被称为无界队列。 SynchronousQueue 容量为0，所以没有地方来暂存元素，导致每次取数据都要先阻塞，直到有数据被放入；同理，每次放数据也会阻塞，直到有消费者来取。Synchronous的容量不是1而是0，它不需要去持有元素，它所做的就是直接传递。 PriorityBlockingQueue 支持优先级的无界阻塞队列，可以通过自定义类实现compareTo()方法来指定元素排序规则，或者初始化时通过构造器参数Comparator来指定排序规则。同时插入的对象必须是可比较大小的，即Comparable的，否则会抛出ClassCastException。 DelayQueue 具有”延迟”的功能，可以设定让队列中的任务延迟多久之后执行，如”30 分钟后未付款自动取消订单”。它是无界队列，放入的元素必须实现Delayed接口，而Delayed接口又继承了Comparable接口，拥有了比较和排序的能力。元素会根据延迟时间的长短放到队列的不同位置，越靠近头队列代表越早过期。 4.4 阻塞队列和非阻塞队列的并发安全原理 ArrayBlockingQueue 12345678// 用于存放元素的数组final Object[] items;// 下一次读取操作的位置int takeIndex;// 下一次写入操作的位置int putIndex;// 队列中的元素数量int count; 1234// 以下3个是控制并发用的工具final ReentrantLock lock;private final Condition notEmpty;private final Condition notFull; 这三个变量非常关键，第一个是ReentrantLock，下面两个Condition是由ReentrantLock产生出来的。读操作和写操作都需要先获取到ReentrantLock独占锁才能进行下一步操作。进行读操作时如果队列为空，线程就会进入到读线程专属的noEmpty的Condition的队列中去排队，等待写线程写入新的元素；同理如果队列已满，写操作的线程会进入到写线程专属的notFull队列中去排队，等待读线程将队列元素移除并腾出空间。 put方法： 123456789101112public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == items.length) notFull.await(); enqueue(e); &#125; finally &#123; lock.unlock(); &#125;&#125; LinkedBlockingQueue的内部有两把锁，分别锁住队列的头和尾，比共用一把锁的效率高。 非阻塞队列ConcurrentLinkedQueue offer方法： 12345678910111213141516171819202122232425262728public boolean offer(E e) &#123; checkNotNull(e); final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); for (Node&lt;E&gt; t = tail, p = t;;) &#123; Node&lt;E&gt; q = p.next; if (q == null) &#123; // p is last node if (p.casNext(null, newNode)) &#123; // Successful CAS is the linearization point // for e to become an element of this queue, // and for newNode to become \"live\". if (p != t) // hop two nodes at a time casTail(t, newNode); // Failure is OK. return true; &#125; // Lost CAS race to another thread; re-read next &#125; else if (p == q) // We have fallen off list. If tail is unchanged, it // will also be off-list, in which case we need to // jump to head, from which all live nodes are always // reachable. Else the new tail is a better bet. p = (t != (t = tail)) ? t : head; else // Check for tail updates after two hops. p = (p != t &amp;&amp; t != (t = tail)) ? t : q; &#125;&#125; 整个是以一个大的for循环，p.casNext()方法 123boolean casNext(Node&lt;E&gt; cmp, Node&lt;E&gt; val) &#123; return UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val);&#125; 这里运用了UNSAFE.compareAndSwapObject方法来完成CAS操作，而compareAndSwapObject是一个native方法，最终会利用CPU的CAS指令保证其不可中断。非阻塞队列ConcurrentLinkedQueue使用CAS非阻塞算法+不停重试，来实现线程安全，适合用在不需要阻塞功能，且并发不是特别剧烈的场景。 4.5 如何选择合适的阻塞队列？ 线程池对于阻塞队列的选择 从以下5个角度考虑，来选择合适的阻塞队列。 功能 是否需要阻塞队列来排序，如优先级排序、优先执行等。 容量 是否需要有存储要求，还是只需要”直接传递”。 能否扩容 业务可能有高峰期、低谷期，如果需要动态扩容，就不能选择ArrayBlockingQueue。 内存结构 如ArrayBlockingQueue的内部结构是”数组”的形式，LinkedBlockingQueue的内部是链表实现的，ArrayBlockingQueue没有链表所需要的”节点”，空间链表利用率更高。 性能 如LinkedBlockingQueue拥有两把锁，操作粒度更细，并发程度高的时候，相对于只有一把锁的ArrayBlockingQueue性能会更好。SynchronousQueue性能往往优于其他实现，因为它只需要”直接传递”，而不需要存储的过程。 5.原子类5.1 原子类如何利用CAS保证线程安全？原子类的作用和锁有类似之处，都是为了保证并发情况下线程安全。 粒度更细：原子变量可以把竞争范围缩小到变量级别，通常情况下，锁的粒度都要大于原子变量的粒度。 效率更高：除高度竞争的情况下，原子类的效率通常比使用同步互斥锁的效率更高，因为原子类利用了CAS操作，不会阻塞线程。 6类原子类纵览 类型 具体类 特点 Atomic* 基本类型原子类 AtomicInteger、AtomicLong、AtomicBoolean Atomic*Array 数组类型原子类 AtomicIntegerArray(整形数组原子类)、AtomicLongArray(长整形数组原子类)、AtomicReferenceArray(引用类型数组原子类) Atomic*Reference 引用类型原子类 AtomicReference、AtomicStampedReference(对AtomicReference的升级，在此基础上还加了时间戳，用于解决CAS的ABA问题)、AtomicMarkableReference(和AtomicReference类似，多了一个绑定的布尔值，可以用于表示该对象已删除等场景) AtomicInteger可以让一个整数保证原子形，AtomicReference可以让一个对象保证原子性。 Atomic*FieldUpdater升级类型原子类 AtomicIntegerFieldUpdater(原子更新整形的更新器)、AtomicLongFieldUpdater(原子更新长整形的更新器)、AtomicReferenceFieldUpdater(原子更新引用的更新器) 可以把已经声明的变量进行升级，使其拥有CAS操作的能力。 Adder累加器 LongAdder、DoubleAdder Accumulator积累器 LongAccumulator、DoubleAccumulator 12345678910111213141516171819202122232425262728293031public class AtomicIntegerFieldUpdaterDemo implements Runnable &#123; public static class Score &#123; volatile int score; &#125; static Score math; static Score computer; static AtomicIntegerFieldUpdater&lt;Score&gt; scoreUpdater = AtomicIntegerFieldUpdater.newUpdater(Score.class, \"score\"); @Override public void run() &#123; for (int i = 0; i &lt; 10000; i++) &#123; computer.score++; scoreUpdater.getAndIncrement(math); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; math = new Score(); computer = new Score(); AtomicIntegerFieldUpdaterDemo updaterDemo = new AtomicIntegerFieldUpdaterDemo(); Thread thread1 = new Thread(updaterDemo); Thread thread2 = new Thread(updaterDemo); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(\"普通变量的结果：\"+ computer.score); System.out.println(\"升级后的结果：\"+ math.score); &#125;&#125; 以AtomicInteger为例，分析其如何利用CAS实现原子操作？ getAndAdd()方法 1234//JDK 1.8实现public final int getAndAdd(int delta) &#123; return unsafe.getAndAddInt(this, valueOffset, delta);&#125; Unsafe类 Unsafe类是CAS的核心类。Java无法直接访问底层操作系统，而需要通过native方法实现。在JDK中有一个Unsafe类，提供了硬件级别的原子操作，可以利用它直接操作内存数据。 1234567891011121314public class AtomicInteger extends Number implements java.io.Serializable &#123; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\"value\")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125; &#125; private volatile int value; public final int get() &#123;return value;&#125; ...&#125; static代码块会在类加载的时候执行，执行时会调用Unsafe的objectFieldOffset方法，从而得到当前这个原子类的value的偏移量(在内存中的偏移地址)，并且赋给valueOffset变量，并且赋值给valueOffset变量，Unsafe根据内存偏移地址获取数据的原值，这样就可以通过Unsafe来实现CAS了。 value是用volatile修饰的，它就是我们原子类存储的值的变量，由于它被volatile修饰，我们就可以保证在多线程之间看到的value是同一份，保证了可见性。 Unsafe的getAndAddInt方法： 1234567public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2);//获取var1中的var2偏移处的值 var1:当前原子类 var2:最开始获取到的offset &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));//var1:object 当前原子类对象 var2:offset 即偏移量，借助它就可以获取到value的数值 var3:expectedValue 代表\"期望值\"，传入的是刚才获取到的var5 var5+var4:newValue 是希望修改的数值，等于之前取到的数值var5+var4，var4是希望原子类所改变的数值，如+1或-1。 return var5;&#125; compareAndSwapInt方法的作用：判断如果现在原子类里的value的值和之前获取到的var5相等的话，那么就把计算出来的var5+var4给更新上去。一旦CAS操作成功，就会退出这个while循环，但也有可能操作失败。如果操作失败就意味着在获取到var之后，并在CAS操作之前，value的数值已经发生变化了，证明有其他线程修改过这个变量。会再次执行循环体里面的代码，重新获取var5，即获取最新的原子变量的数值，并再次利用CAS尝试更新，直到更新成功。 5.2 AtomicInteger在高并发下性能不好，如何解决？为什么？123456789101112131415161718192021222324252627282930public class AtomicLongDemo &#123; static class Task implements Runnable &#123; private final AtomicLong counter; public Task(AtomicLong counter) &#123; this.counter = counter; &#125; @Override public void run() &#123; counter.incrementAndGet(); System.out.println(Thread.currentThread().getName()+\"...\"+counter.get()); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; AtomicLong counter = new AtomicLong(0); ExecutorService poolExecutor = new ThreadPoolExecutor(20, 40, 10, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new ThreadPoolExecutor.AbortPolicy()); StopWatch stopWatch = new StopWatch(); stopWatch.start(); for (int i = 0; i &lt; 100; i++) &#123; poolExecutor.submit(new Task(counter)); &#125; Thread.sleep(2000); System.out.println(\"result:\"+counter.get()); stopWatch.stop(); System.out.println(stopWatch.getTotalTimeMillis()); &#125;&#125; 每一个线程是运行在自己的core中的，并且它们都有一个本地内存是自己独用的。在本地内存下方有两个CPU核心共用的共享内存。对于AtomicLong内部的value属性而言，它是被volatile修饰的，需要保证自身可见性。每次它的数值变化的时候，都需要进行flush到共享内存和refresh到本地内存。 flush和refresh操作耗费了很多资源，而且CAS也会经常失败。 LongAdder LongAdder引入了分段累加的概念，内部一共有两个参数参与计数： base，是一个变量，用在竞争不激烈的情况下，可以直接把来家结果改到base变量上。 Cell[]，是一个数组，一旦竞争激烈，各个线程会分散累加到自己所对应的那个Cell[]数组的某一个对象中，而大家不会共用同一个。 竞争激烈的时候，LongAdder会通过计算出每个线程的hash值来给线程分配到不同的Cell上去，每个Cell相当于是一个独立的计数器，Cell之间并不存在竞争，所以自加过程中，大大减少了flush和refresh，以及降低了冲突的概率。空间换时间。 1234567891011public long sum() &#123; Cell[] as = cells; Cell a; long sum = base; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125; 如何选择 如何仅仅是需要用到加和减操作的场景，那么可以直接使用LongAdder。 如果需要利用CAS比如compareAndSet等操作的话，就需要使用AtomicLong来完成。 5.3 原子类与volatile 线程1和线程2分别在不同的CPU核心，每一个核心都有自己的本地内存，并且在下方也有它们的共享内存。在变量加上volatile关键字，线程1的更改会被flush到共享内存，然后又被refresh到线程2的本地内存，保证了可见性。 但对于value++这种，即使用volatile修饰value也是不能保证线程安全的，无法保证其原子性。此时可以使用原子类。 原子类和volatile的使用场景 通常情况下，volatile可以用来修饰boolean类型的标记位，对于标记位来讲，直接的赋值操作本身就具有原子性，再加上volatile保证了可见性，那么就是线程安全的了。而对于会被多个线程同时操作的计数器counter的场景，即不仅仅是赋值操作，还需要读取当前值，然后在此基础上进行一定的修改，再把它给赋值回去，此时需要使用原子类保证线程安全。 5.4 Adder与Accumlator的区别高并发场景下AtomicLong CAS冲突概率大，会导致经常自旋。而LongAdder引入了分段锁的概念，竞争不激烈的时候，所有线程都是通过CAS对同一个Base变量进行修改，但竞争激烈的时候，LongAdder会把不同线程对应到不同的Cell上进行修改，降低了冲突的概率。 LongAccumulator就是个更通用版本的Adder，提供了自定义的函数操作。 12345678910111213public class LongAccumulatorDemo &#123; public static void main(String[] args) throws InterruptedException &#123; LongAccumulator accumulator = new LongAccumulator((x, y) -&gt; x + y, 0); ExecutorService executorService = new ThreadPoolExecutor(8, 16, 10, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new ThreadPoolExecutor.AbortPolicy()); IntStream.range(1,10).forEach(i-&gt;executorService.submit(()-&gt;&#123; accumulator.accumulate(i); System.out.println(Thread.currentThread().getName()+\"...\"+accumulator.get()); &#125;)); Thread.sleep(2000); System.out.println(accumulator.getThenReset()); &#125;&#125; 自定义函数： 1234LongAccumulator counter = new LongAccumulator((x, y) -&gt; x + y, 0);LongAccumulator result = new LongAccumulator((x, y) -&gt; x * y, 0);LongAccumulator min = new LongAccumulator((x, y) -&gt; Math.min(x, y), 0);LongAccumulator max = new LongAccumulator((x, y) -&gt; Math.max(x, y), 0); 适用场景 需要大量的计算，并且当需要并行计算的时候。 计算的执行顺序并不关键。 6.ThreadLocal6.1 ThreadLocal适用场景 场景1 保存每个线程独享的对象，为每个线程都创建一个副本，这样每个线程都可以修改自己拥有的副本，而不会影响其他线程的副本，确保了线程安全。 这种场景下，每个Thread内都有自己的实例副本，且该副本只能由当前Thread访问到并使用，相当于每个线程内部的本地变量。因为每个线程独享副本，而不是共用的，所以不存在多线程间共享的问题。 这种场景通常用于保存线程不安全的工具类，如SimpleDateFormat。 1234567891011121314151617181920212223242526public class ThreadLocalDemo05 &#123; static ThreadLocal&lt;SimpleDateFormat&gt; formatThreadLocal = new ThreadLocal&lt;SimpleDateFormat&gt;()&#123; @Override protected SimpleDateFormat initialValue() &#123; return new SimpleDateFormat(\"mm:ss\"); &#125; &#125;; public static ExecutorService executorService = new ThreadPoolExecutor(16, 32, 10, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new ThreadPoolExecutor.AbortPolicy()); public String date(int seconds) &#123; Date date = new Date(1000 * seconds); SimpleDateFormat simpleDateFormat = formatThreadLocal.get(); return simpleDateFormat.format(date); &#125; public static void main(String[] args) throws InterruptedException &#123; IntStream.range(1, 1000).forEach(i -&gt; executorService.submit(() -&gt; &#123; String date = new ThreadLocalDemo05().date(i); System.out.println(Thread.currentThread().getName() + \":\" + date); &#125;)); Thread.sleep(2000); executorService.shutdown(); &#125;&#125; 场景2 每个线程内需要独立保存信息，以便其他方法更方便的获取该信息的场景。每个线程获取到的信息可能都是不一样的，前面执行的方法保存了信息之后，后续方法可以通过ThreadLocal直接获取到，避免了传参，类似于全局变量的概念。 每个线程内需要保存类似于全局变量的信息(列如拦截器中获取的用户信息)，可以让不同方法直接使用，避免参数传递的麻烦却不想被多线程共享(因为不同线程获取到的用户信息不一样)。 例如，用ThreadLocal保存一些业务内容(用户权限信息)，这些信息在同一个线程内相同，但在不同的线程使用的业务内容是不相同的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class ThreadLocalDemo07 &#123; public static void main(String[] args) &#123; new Service1().service1(); &#125;&#125;class User &#123; String username; public User() &#123;&#125; public User(String username) &#123; this.username = username; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125;&#125;class userContextHolder &#123; public static ThreadLocal&lt;User&gt; holder = new ThreadLocal&lt;&gt;();&#125;class Service1 &#123; public void service1() &#123; User user = new User(\"张三\"); userContextHolder.holder.set(user); new Service2().service2(); &#125;&#125;class Service2 &#123; public void service2() &#123; User user = userContextHolder.holder.get(); System.out.println(\"Service2拿到用户名：\" + user.getUsername()); new Service3().service3(); &#125;&#125;class Service3 &#123; public void service3() &#123; User user = userContextHolder.holder.get(); System.out.println(\"Service3拿到用户名：\" + user.getUsername()); userContextHolder.holder.remove(); &#125;&#125; 6.2 ThreadLocal是用来解决共享资源的多线程访问的问题吗？不是，虽然ThreadLocal是用于解决多线程情况下的线程安全问题，但其资源并不是共享的，而是每个线程独占的。 如果把放到ThreadLocal中的资源用static修饰，让它变为一个共享资源的话，那么即便使用ThreadLocal，同样有线程安全问题。 ThreadLocal和synchronized是什么关系？ ThreadLocal是通过让每个线程独享自己的副本，避免了资源的竞争。 synchronized主要用于临界资源的分配，在同一时刻限制最多只有一个线程能够访问该资源 相比于ThreadLocal而言，synchronized的效率会更低一些，但花费的内存也更少。但对于ThreadLocal而言，它还有不同的使用场景。比如避免传参。 6.3 ThreadLocal的结构Thread、ThreadLocal及ThreadLocalMap三者之间的关系 每个Thread对象中都持有一个ThreadLocalMap类型的成员变量，这个ThreadLocalMap自身类似一个Map，里面会有一个个key-value形式的，key就是ThreadLocal的引用，value就是希望ThreadLocal存储的内容。 get方法 1234567891011121314151617public T get() &#123; //获取到当前线程 Thread t = Thread.currentThread(); //获取到当前线程内的ThreadLocalMap对象，每个线程内都有一个ThreadLocalMap对象 ThreadLocalMap map = getMap(t); if (map != null) &#123; //获取ThreadLocalMap中的Entry对象并拿到value，每个线程内都有一个ThreadLocalMap对象 ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; &#125; &#125; //如果线程内之前没创建过ThreadLocalMap，就创建 return setInitialValue();&#125; getMap方法 12345 ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125;ThreadLocal.ThreadLocalMap threadLocals = null; set方法 1234567891011public void set(T value) &#123; //获取到当前线程 Thread t = Thread.currentThread(); //获取当前线程内的ThreadLocalMap ThreadLocalMap map = getMap(t); if (map != null) //第一个参数this：当前ThreadLocal的引用，key的类型则是ThreadLocal；第二个参数即为所传入的value map.set(this, value); else createMap(t, value);&#125; ThreadLocalMap类，即Thread.threadLocals 123456789101112131415static class ThreadLocalMap &#123; static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; private static final int INITIAL_CAPACITY = 16; private Entry[] table; ......&#125; 在ThreadLocalMap中会有一个Entry类型的数组，名字叫table。可以理解为一个map，其键值对为： 键，当前的ThreadLocal 值，实际需要存储的变量，比如user用户对象或者simpleDateFormat对象 HashMap在面对hash冲突的时候，采用的是拉链法，它会先把对象hash到一个对应的格子中，如果有冲突就用链表的形式往下链；但ThreadLocalMap采用的是线性探测法，如果发生冲突，并不会用链表的形式往下链，而是会继续寻找下一个空的格子。 6.4 为何每次用完 ThreadLocal 都要调用 remove()？内存泄漏：当某一个对象不再有用的时候，占用的内存却不能被回收。 12345678static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; Entry是extends WeakReference。弱引用的特点：如果这个对象只被弱引用关联，而没有任何强引用关联，那么这个对象就可以被回收，所以弱引用不会阻止GC。 但是这个Entry包含了一个对value的强引用。value=v这行代码就代表了强引用的发生。 Thread Ref → Current Thread → ThreadLocalMap → Entry → Value → 可能泄漏的value实例。 这条链路是随着线程的存在而一直存在，如果线程迟迟不会终止，那么当垃圾回收进行可达性分析的时候，这个value就是可达的，所以不会被回收。但与此同时可能已经完成了业务逻辑处理，不再需要这个value了，此时就发生了内存泄漏。 在执行ThreadLocal的set、remove、rehash等方法时，都会扫描key为null的Entry，如果发现某个Entry的key为null，则代表它所对应的value也没有作用了，所以就会把对应的value设置为null，这样，value对象就可以被正常回收了。但假设ThreadLocal已经不被使用了，那么实际上set、remove、rehash方法也不会被调用。 如何避免内存泄漏 调用 ThreadLocal 的 remove 方法。调用这个方法就可以删除对应的 value 对象，可以避免内存泄漏。 12345public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this);&#125; 7.Future7.1 Callable和Runnable的不同 Runnable的不足 不能返回一个返回值 不能抛出checked Exception 123456789101112131415161718192021222324public class RunThrowException &#123; /** * 普通方法内可以 throw 异常，并在方法签名上声明 throws */ public void normalMethod() throws Exception &#123; throw new IOException(); &#125; Runnable runnable = new Runnable() &#123; /** * run方法上无法声明 throws 异常，且run方法内无法 throw 出 checked Exception，除非使用try catch进行处理 */ @Override public void run() &#123; try &#123; throw new IOException(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; Runnable规定了run()方法的返回类型是void，而且没有声明抛出任何异常。所以，当实现并重写这个方法的时候，既不能改变返回值类型，也不能更改对于异常抛出的描述。 123public interface Runnable &#123; public abstract void run();&#125; Callable接口 call方法已经声明了throws Exception，前面还有一个V泛型的返回值。 123public interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; Callable和Runnable的不同之处 方法名：Callable规定的执行方法是call()，而Runnable规定的执行方法是run() 返回值：Callable的任务执行后有返回值，而Runnable的任务执行后是没有返回值的 抛出异常：call()方法可抛出异常，而run方法是不能抛出检查异常的 和Callable配合使用的Future类，通过Future可以了解任务的执行情况，或者取消任务的执行，还可获取任务的执行结果等。 7.2 Future的主要功能Future的作用 比如当做一定较耗时的任务时，可以把任务放到子线程去执行，再通过Future去控制子线程执行的过程，最后获取到计算结果。通过异步的思想，提高程序的运行效率。 Callable和Future的关系 Callable接口相比于Runnable可以通过Future类的get方法返回结果。因此，Future类相当于一个存储器，它存储了Callable的call方法的任务结果。 12345678910111213public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; get() 获取结果 获取任务执行的结果 当执行get的时候，任务已经执行完毕了。可以立刻返回，获取到任务执行的结果。 任务还未开始或任务正在执行中，调用get时，都会把当前线程阻塞，直到任务完成再把结果返回回来。 任务执行过程中抛出异常，调用get时，就会抛出ExecutionException，且无论执行call方法时里面抛出的异常类型是什么，在执行get方法时所获得的异常都是ExecutionException。 任务被取消了，如果任务被取消，调用get方法时则会抛出CancellationException。 任务超时，调用带延迟参数的get方法之后，如果call方法在规定时间内仍没有完成任务，get方法则会抛出TimeoutException，代表超时了。 123456789101112131415161718192021222324/* *一个Future的使用 */public class OneFuture &#123; static class CallableTask implements Callable&#123; @Override public Object call() throws Exception &#123; Thread.sleep(3000); return new Random().nextInt()+Thread.currentThread().getName(); &#125; &#125; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newFixedThreadPool(10); Future future = executorService.submit(new CallableTask()); try &#123; System.out.println(future.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; executorService.shutdown(); &#125;&#125; isDone() 判断是否执行完毕 判断当前线程是否执行完毕，返回true代表已经执行完毕，返回false则代表还没完成。但这里如果返回true，并不代表这个任务是成功执行的，比如说任务执行到一半抛出了异常，仍然会返回true，所以isDone方法在返回true的时候，不代表这个任务是成功执行的，只代表它执行完毕了。 123456789101112131415161718192021222324252627282930313233public class GetException &#123; static class CallableTask implements Callable&#123; @Override public Object call() throws Exception &#123; throw new IllegalArgumentException(\"Callable抛出异常！\"); &#125; &#125; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newFixedThreadPool(20); Future future = executorService.submit(new CallableTask()); try &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(i); Thread.sleep(500); &#125; System.out.println(future.isDone()); future.get(); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125;01234truejava.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Callable抛出异常！ at java.util.concurrent.FutureTask.report(FutureTask.java:122)...... 这段代码的运行结果证明了： 即便任务抛出异常，isDone方法依然会返回true。 虽然call方法抛出的异常是IllegalArgumentException，但对于get而言，它抛出的异常依然是ExecutionException。 虽然在任务执行一开始就抛出了异常，但真正要等到执行get的时候，才看到了异常。 cancel 取消任务的执行 任务还未执行，任务会被正常取消，未来也不会被执行，返回true。 任务已经完成或被取消过，返回false。 任务正在执行，会根据传入的参数mayInterruptIfRunning，如果传入的参数是true，执行任务的线程会收到一个中断的信号。如果传入的是false，就代表不中断正在运行的任务，同时返回false。 true：明确知道这个任务能够处理中断 false：明确知道这个任务不能处理中断；不知道这个任务是否支持取消(是否能够响应中断)；如果这个任务一旦开始运行，就希望它完全的执行完毕。 isCancelled() 判断是否被取消 用FutureTask创建Future 12345678910111213141516171819202122232425public class FutureTaskDemo &#123; public static void main(String[] args) &#123; Task task = new Task(); FutureTask futureTask = new FutureTask(task); new Thread(futureTask).start(); try &#123; System.out.println(\"task运行结果：\"+futureTask.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class Task implements Callable&#123; @Override public Object call() throws Exception &#123; System.out.println(\"子线程\"+Thread.currentThread().getName()+\"正在计算！\"); int sum = 0; for (int i = 0; i &lt; 100; i++) &#123; sum += i; &#125; return sum; &#125;&#125; 7.3 Future注意点 当for循环批量获取Future的结果时容易block，get方法调用时应该使用timeout限制 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class FutureDemo &#123; static class SlowTask implements Callable &#123; @Override public Object call() throws Exception &#123; Thread.sleep(5000); return \"速度慢的任务\" + Thread.currentThread().getName(); &#125; &#125; static class FastTask implements Callable &#123; @Override public Object call() throws Exception &#123; return \"速度快的任务\" + Thread.currentThread().getName(); &#125; &#125; public static void main(String[] args) &#123; ExecutorService executorService = new ThreadPoolExecutor(10, 10, 10, TimeUnit.MICROSECONDS, new LinkedBlockingDeque&lt;&gt;(), new ThreadPoolExecutor.AbortPolicy()); List&lt;Future&gt; futures = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 4; i++) &#123; Future future; if (i == 0 || i == 1) &#123; future = executorService.submit(new SlowTask()); &#125; else &#123; future = executorService.submit(new FastTask()); &#125; futures.add(future); &#125; for (int i = 0; i &lt; 4; i++) &#123; Future future = futures.get(i); try &#123; String result = (String)future.get(); System.out.println(result); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; executorService.shutdown(); &#125;&#125;速度慢的任务pool-1-thread-1速度慢的任务pool-1-thread-2速度快的任务pool-1-thread-3速度快的任务pool-1-thread-4 第三个任务量比较小，可以很快返回结果，紧接着第四个任务也会返回结果。但由于前两个任务速度很慢，所以get方法执行时，会卡在第一个任务上。所以，即使第三、四个任务很早就得到结果了，但在此使用for循环的方式去获取结果，依然无法及时获取第三、四个任务的结果。直到5秒后，第一个任务出结果了，我们才能获取到，紧接着获取剩下任务的结果。 此时可以使用Future的带超时参数的get(long timeout, TimeUnit unit)方法，如果在限定时间内没能返回结果，即抛出TimeoutException。 Future的生命周期不可后退 Future的生命周期不可后退，一旦完成了任务，它就永久停在了”已完成”的状态，不能重头再来，即不能让一个已经完成计算的Future再次重新执行任务。 Future产生新的线程了吗 Callable和Future本身并不能产生新的线程，它们需要借助其它的比如Thread类或者线程池才能执行任务。例如：在把Callable提交到线程池后，真正执行Callable的其实还是线程池中的线程，而线程池中的线程是由ThreadFactory产生的。 7.4 CountDownLatch、Completable 线程池实现 12345678910111213141516171819202122232425262728293031323334public class ThreadPoolDemo &#123; ExecutorService threadPool = Executors.newFixedThreadPool(3); public static void main(String[] args) throws InterruptedException &#123; ThreadPoolDemo threadPoolDemo = new ThreadPoolDemo(); System.out.println(threadPoolDemo.getPrices()); &#125; private Set&lt;Integer&gt; getPrices() throws InterruptedException &#123; Set&lt;Integer&gt; prices = Collections.synchronizedSet(new HashSet&lt;Integer&gt;()); threadPool.submit(new Task(123, prices)); threadPool.submit(new Task(456, prices)); threadPool.submit(new Task(789, prices)); Thread.sleep(3000); return prices; &#125; private class Task implements Runnable &#123; Integer productId; Set&lt;Integer&gt; prices; public Task(Integer productId, Set&lt;Integer&gt; prices) &#123; this.productId = productId; this.prices = prices; &#125; @Override public void run() &#123; int price=0; try &#123; Thread.sleep((long) (Math.random() * 4000)); price= (int) (Math.random() * 4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; prices.add(price); &#125; &#125;&#125; CountDownLatch 1234567891011121314151617181920212223242526272829303132333435363738394041public class CountDownLatchDemo &#123; ExecutorService threadPool = Executors.newFixedThreadPool(3); public static void main(String[] args) throws InterruptedException &#123; CountDownLatchDemo countDownLatchDemo = new CountDownLatchDemo(); System.out.println(countDownLatchDemo.getPrices()); &#125; private Set&lt;Integer&gt; getPrices() throws InterruptedException &#123; Set&lt;Integer&gt; prices = Collections.synchronizedSet(new HashSet&lt;Integer&gt;()); CountDownLatch countDownLatch = new CountDownLatch(3); threadPool.submit(new Task(123, prices, countDownLatch)); threadPool.submit(new Task(456, prices, countDownLatch)); threadPool.submit(new Task(789, prices, countDownLatch)); countDownLatch.await(3, TimeUnit.SECONDS); return prices; &#125; private class Task implements Runnable &#123; Integer productId; Set&lt;Integer&gt; prices; CountDownLatch countDownLatch; public Task(Integer productId, Set&lt;Integer&gt; prices, CountDownLatch countDownLatch) &#123; this.productId = productId; this.prices = prices; this.countDownLatch = countDownLatch; &#125; @Override public void run() &#123; int price = 0; try &#123; Thread.sleep((long) (Math.random() * 4000)); price = (int) (Math.random() * 4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; prices.add(price); countDownLatch.countDown(); &#125; &#125;&#125; 执行countDownLatch.await(3, TimeUnit.SECONDS)等待时，如果三个任务都非常快速得执行完毕了，那么都已经执行了countDown方法，相当于把计数减1。如果有一个线程没有执行countDown方法，来不及在3秒内执行完毕，那么这个带超时参数的await方法也会在3秒以后，及时的放弃这一次等待，于是就把prices返回了。 CompletableFuture 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class CompletableFutureDemo &#123; private class Task implements Runnable &#123; Integer productId; Set&lt;Integer&gt; prices; public Task(Integer productId, Set&lt;Integer&gt; prices) &#123; this.productId = productId; this.prices = prices; &#125; @Override public void run() &#123; int price = 0; try &#123; Thread.sleep((long)(Math.random() * 4000)); price = (int)(Math.random() * 4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; prices.add(price); &#125; &#125; private Set&lt;Integer&gt; getPrices() &#123; Set&lt;Integer&gt; prices = Collections.synchronizedSet(new HashSet&lt;Integer&gt;()); CompletableFuture&lt;Void&gt; task1 = CompletableFuture.runAsync(new Task(123, prices)); CompletableFuture&lt;Void&gt; task2 = CompletableFuture.runAsync(new Task(456, prices)); CompletableFuture&lt;Void&gt; task3 = CompletableFuture.runAsync(new Task(789, prices)); CompletableFuture&lt;Void&gt; allTasks = CompletableFuture.allOf(task1, task2, task3); try &#123; allTasks.get(3, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; catch (TimeoutException e) &#123; e.printStackTrace(); &#125; return prices; &#125; public static void main(String[] args) &#123; CompletableFutureDemo completableFutureDemo = new CompletableFutureDemo(); System.out.println(completableFutureDemo.getPrices()); &#125;&#125; CompletableFuture的runAsync()方法，这个方法会异步的去执行任务。 8.线程协作8.1 信号量 控制需要限制并发访问量的资源。 使用流程 初始化一个信号量，并传入许可证的数量。public Semaphore(int permits, boolean fair)，传入两个参数，第一个参数是许可证的数量，另一个参数是是否公平，如果为true，代表是公平的策略，会把之前已经在等待的线程放入到队列中，当有新的许可证时，会按照顺序发放；如果为false，则代表非公平策略，也就有可能插队。 在调用慢服务之前，线程调用acquire()或者acquireUninterruptibly()获取许可证。如果此时信号量没有剩余的许可证，那么线程会等在acquire()的这一行代码中，不会进一步执行下面调用服务的方法。 acquire()和acquireUninterruptibly()的区别：是否能够中断。acquire()支持中断，即在获取信号量期间，假如这个线程被中断了，那么它就会跳出acquire()，不再继续尝试获取了，而acquireUninterruptibly()方法是不会中断的。 任务执行完毕之后，调用release()释放许可证。 其他的主要方法 public boolean tryAcquire() 尝试获取许可证，获取不到不会阻塞，可以去做其他事。 public boolean tryAcquire(long timeout, TimeUnit unit) 超时时间到，依然获取不到许可证，认为获取失败，返回false。 availablePermits() 查询可用许可证的数量，返回一个整形的结果。 12345678910111213141516171819202122232425262728293031323334353637public class SemaphoreDemo2 &#123; static Semaphore semaphore = new Semaphore(5); static ThreadLocal&lt;StopWatch&gt; stopWatchThreadLocal = ThreadLocal.withInitial(() -&gt; new StopWatch()); private static class Task implements Runnable &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \"获取到许可证，开始执行任务！\"); StopWatch stopWatch = stopWatchThreadLocal.get(); stopWatch.start(); try &#123; Thread.sleep(3000); stopWatch.stop(); System.out.println(\"慢服务执行完毕，耗时：\" + stopWatch.getTotalTimeMillis() + \"---\" + Thread.currentThread().getName() + \"释放了许可证！\"); semaphore.release(); stopWatchThreadLocal.remove(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; ExecutorService executorService = new ThreadPoolExecutor(50, 50, 5, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(), new ThreadPoolExecutor.AbortPolicy()); for (int i = 0; i &lt; 1000; i++) &#123; executorService.submit(new Task()); &#125; executorService.shutdown(); &#125;&#125; 特殊用法：一次获取或释放多个许可证 semphore.acquire(2) semaphore.release(3) 注意点 获取和释放的许可证数量尽量保持一致 在初始化时可以设置公平性，true会让它更公平，false则会让总的吞吐量更高 信号量是支持跨线程、跨线程池的，并且并不是哪个线程获得的许可证，就必须由这个线程去释放，对于获取和释放许可证的线程是没有要求的。 8.2 CountDownLatch 是如何安排线程执行顺序的？ 主要方法 构造函数 public CountDownLatch(int count){ } count是需要倒数的值 await() 调用await()方法的线程开始等待，直到倒数结束，也就是count值为0的时候才会继续执行。 await(long timeout, TimeUnit unit) 和await()类似，但这里可以设置超时时间，如果超时就不等待了。 countDown() 把数值倒数1，也就是将count值减1，直到减为0时，之前等待的线程会被唤起。 用法 12345678910111213141516171819202122232425262728293031323334public class RunDemo3 &#123; public static void main(String[] args) throws InterruptedException &#123; ExecutorService executorService = new ThreadPoolExecutor(5, 5, 5, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(), new ThreadPoolExecutor.AbortPolicy()); CountDownLatch downLatch1 = new CountDownLatch(5); CountDownLatch downLatch2 = new CountDownLatch(1); for (int i = 0; i &lt; 5; i++) &#123; int finalI = i + 1; Runnable runnable = new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(finalI + \"号运动员准备完毕，等待裁判员的发令枪\"); downLatch2.await(); Thread.sleep((long)(Math.random() * 10000)); System.out.println(finalI + \"号运动员完成了比赛\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; downLatch1.countDown(); &#125; &#125; &#125;; executorService.submit(runnable); &#125; Thread.sleep(5000); System.out.println(\"5秒准备时间已过，发令枪响，比赛开始！\"); downLatch2.countDown(); System.out.println(\"等待5个运动员都跑完....\"); downLatch1.await(); System.out.println(\"所有人都跑完了，比赛结束\"); executorService.shutdown(); &#125;&#125; 注意点 CountDownLatch是不能够重用的，比如已经完成了倒数，不可以在下一次继续去重新倒数。可以考虑使用CyclicBarrier或创建一个新的CountDownLatch实例。 8.3 CyclicBarrier和CountdownLatchCyclicBarrier可以构造出一个集结点，当某一个线程执行await()的时候，它就会到这个集结点开始等待，等待这个栅栏被撤销。直到预定数量的线程都到了这个集结点之后，这个栅栏就会撤销，之前等待的线程就在此刻统一出发，继续去执行剩下的任务。 12345678910111213141516171819202122232425262728293031323334353637public class CyclicBarrierDemo &#123; static class Task implements Runnable &#123; private int id; private CyclicBarrier cyclicBarrier; public Task(int id, CyclicBarrier cyclicBarrier) &#123; this.id = id; this.cyclicBarrier = cyclicBarrier; &#125; @Override public void run() &#123; try &#123; System.out.println(\"同学\" + id + \"现在从大门出发，前往自行车驿站\"); Thread.sleep((long)(Math.random() * 10000)); System.out.println(\"同学\" + id + \"到了自行车驿站，开始等待其他人到达\"); cyclicBarrier.await(); System.out.println(\"同学\" + id + \"开始骑车\"); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(3, new Runnable() &#123; // 当线程达到集结点，执行下一次动作之前，会执行一次这个动作 @Override public void run() &#123; System.out.println(\"凑齐3人了，GO！\"); &#125; &#125;); for (int i = 0; i &lt; 6; i++) &#123; new Thread(new Task(i + 1, cyclicBarrier)).start(); &#125; &#125;&#125; 执行动作barrierAction public CyclicBarrier(int parties, Runnable barrierAction): 当parties线程到达集结点时，继续往下执行前，会执行这一次这个动作。 1234567891011121314151617181920同学1现在从大门出发，前往自行车驿站同学5现在从大门出发，前往自行车驿站同学6现在从大门出发，前往自行车驿站同学4现在从大门出发，前往自行车驿站同学3现在从大门出发，前往自行车驿站同学2现在从大门出发，前往自行车驿站同学5到了自行车驿站，开始等待其他人到达同学2到了自行车驿站，开始等待其他人到达同学6到了自行车驿站，开始等待其他人到达凑齐3人了，GO！同学6开始骑车同学5开始骑车同学2开始骑车同学3到了自行车驿站，开始等待其他人到达同学4到了自行车驿站，开始等待其他人到达同学1到了自行车驿站，开始等待其他人到达凑齐3人了，GO！同学1开始骑车同学3开始骑车同学4开始骑车 CyclicBarrier和CountDownLatch的异同 相同点：都能阻塞一个或一组线程，直到某个预设条件达成，再统一出发。 不同点： 作用对象不同： CyclicBarrier要等固定数量的线程都到达了栅栏位置才能继续执行，而CountDownLatch只需等待数字到0，也就是说CountDownLatch作用于事件，但CyclicBarrier作用于线程；CountDownLatch是在调用了countDown方法之后把数字减1，而CyclicBarrier是在某线程开始等待后把计数减1。 可重用性不同： CountDownLatch在倒数0并且触发门闩打开后，就不能再次使用了，除非新建一个新的实例；而CyclicBarrier可以重复使用。CyclicBarrier还可以随时调用reset方法进行重置，如果重置时有线程已经调用了await方法并开始等待，那么这些线程则会抛出BrokenBarrierException异常。 执行动作不同： CyclicBarrier有执行动作barrierAction，而CountDownLatch没这个功能。 8.4 Condition、object都wait()何notify()的关系假设线程1需要等待某些条件满足后，才能继续运行，如等待某个时间点到达或者等待某些任务处理完毕。此时，就可以执行Condition的await方法，一旦执行了该方法，这个线程就会进入WATTING状态。通常还有另外一个线程2，它去达成对应的条件，直到这个条件达成之后，那么线程2调用signal方法或signalAll方法，代表”条件达成，之前等待这个条件的线程现在可以苏醒了“。这个时候，JVM就会找到等待该Condition的线程，并予以唤醒，线程1在此时就会被唤醒，线程状态又会回到Runnable。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class ConditionDemo &#123; private ReentrantLock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); void task1() throws InterruptedException &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \":条件不满足，开始await\"); condition.await(); System.out.println(Thread.currentThread().getName() + \"条件满足了，开始执行后续的任务\"); &#125; finally &#123; lock.unlock(); &#125; &#125; void task2() throws InterruptedException &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \":需要5秒钟的准备时间\"); Thread.sleep(5000); System.out.println(Thread.currentThread().getName() + \":准备工作完成，唤醒其他的线程\"); condition.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; ConditionDemo conditionDemo = new ConditionDemo(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; conditionDemo.task2(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); conditionDemo.task1(); &#125;&#125;main:条件不满足，开始awaitThread-0:需要5秒钟的准备时间Thread-0:准备工作完成，唤醒其他的线程main条件满足了，开始执行后续的任务 注意点 线程2解锁后，线程1才能获得锁并继续执行 调用signal之后，还需要等待子线程完全退出这个锁，即执行unlock之后，这个主线程才有可能去获取到这把锁，并且当获取锁成功之后才能继续执行后面的任务。 signalAll()和signal()区别 signalAll()会唤醒所有正在等待的线程，而signal()只会唤醒一个线程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/* * Condition实现简易版阻塞队列 */public class MyBlockingQueueForCondition &#123; private Queue queue; private int max = 16; private ReentrantLock lock = new ReentrantLock(); private Condition notEmpty = lock.newCondition(); private Condition notFull = lock.newCondition(); public MyBlockingQueueForCondition(int maxSize) &#123; this.max = maxSize; queue = new LinkedList(); &#125; public void put(Object object) throws InterruptedException &#123; lock.lock(); try &#123; while (queue.size() == max) &#123; notFull.await(); &#125; queue.add(object); notEmpty.signalAll(); &#125; finally &#123; lock.unlock(); &#125; &#125; public Object take() throws InterruptedException &#123; lock.lock(); try &#123; while (queue.size() == 0) &#123; notEmpty.await(); &#125; Object item = queue.remove(); notFull.signalAll(); return item; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;/* * 使用wait/notify来实现简易版阻塞队列 */public class MyBlockingQueueForWaitNotify &#123; private int maxSize; private LinkedList&lt;Object&gt; queue; public MyBlockingQueueForWaitNotify(int maxSize) &#123; this.maxSize = maxSize; queue = new LinkedList&lt;&gt;(); &#125; public synchronized void put(Object object) throws InterruptedException &#123; while (queue.size() == maxSize) &#123; this.wait(); &#125; queue.add(object); this.notifyAll(); &#125; public synchronized Object take() throws InterruptedException &#123; while (queue.size() == 0) &#123; this.wait(); &#125; Object item = queue.remove(); this.notifyAll(); return item; &#125;&#125; Condition把Object的wait/notify/notifyAll转化为了一种相应的对象，其实现的效果基本一样，但是把更复杂的用法，变成了更直观可控的对象方法，是一种升级。await方法会自动释放持有的Lock锁，否则会抛出异常，和Object的wait一样，不需要自己手动释放锁。另外，调用await的时候必须持有锁，否则会抛出异常，这一点和Object的wait一样。 9.Java内存模型9.1 什么是Java内存模型？JVM内存结构 堆 堆是存放类实例和数组的，通常是内存中最大的一块。比如new Object()就会产生一个实例；而数组也是保存在堆上，因为在Java中，数组也是对象。 虚拟机栈 保存局部变量和部分结果，并在方法调用和返回中起作用。 方法区 它存储每个类的结构，例如运行时常量池、字段和方法数据，以及方法和构造函数的代码，包括用于类初始化以及接口初始化的特殊方法。 本地方法栈 与虚拟机栈类似，区别在于虚拟机栈为虚拟机执行的Java方法服务，而本地方法栈则是为Native方法服务。 程序计数器 最小的一块内存区域，它的作用通常是保存当前正在执行的JVM指令地址。 运行时常量池 是方法区的一部分，包含多种常量，范围从编译时已知的数字到必须在运行时解析的方法和字段引用。 为什么需要JMM(Java Memory Model, Java内存模型) 程序最终执行的效果依赖于具体的处理器，而不同的处理器的规则又不一样，需要一个标准，让多线程运行结果可以预期，这个标准就是JMM。 JMM是什么 JMM是规范 JMM是和多线程相关的一组规范，需要各个JVM的实现来遵守JMM规范。因此JMM与处理器、缓存、并发、编译器有关，它解决了CPU多级缓存、处理器优化、质量重排序等导致的结果不可预期的问题。 JMM是工具类和关键字的原理 如volatile、synchronized、Lock等原理都涉及JMM。重排序、原子性、内存可见性。 9.2 什么是指令重排序？为什么要进行重排序？假设我们写了一个Java程序，实际上语句的运行顺序可能可写的代码顺序不一致。编译器、JVM或者CPU都有可能出于优化等目的，对于实际指令执行的顺序进行调整。 重排序的好处：提高处理速度 重排序的3种情况 编译器优化 编译器(包括JVM、JIT编译器等)；重排序并不意味着可以任意排序，它需要保证重排序后，不改变单线程内的语义。 CPU重排序 CPU同样会有优化行为，即使之前的编译器不发生冲排，CPU也可能进行重排。 内存的”重排序” 内存系统不存在真正的重排序，但是内存会带来看上去和重排序一样的效果。由于内存有缓存的存在，在JMM里表现为主内存和本地内存，而主内存和本地内存的内容可能不一致，所以这也会导致程序表现出乱序的行为。 9.3 Java中的原子操作有哪些注意事项？原子操作指一系列操作要么全部发生，要么全部不发生，不会出现执行一半的情况。 Java中的原子操作有哪些 除了long和double之外的基本类型(int、byte、boolean、short、char、float)的读/写操作，都天然的具备原子性 所有引用reference的读/写操作 加了volatile后，所有变量的读/写操作(包含long/double) java.concurrent.Atomic包中的一部分类的一部分方法，比如AtomicInteger的incrementAndGet long和double的原子性 long和double的值需要占用64位的内存空间，而对于64位值的写入，可以分为两个32位的操作进行。因此，本来是一个整体的赋值操作，就可能被拆分为低32位和高32位两个操作。如果在这两个操作之间发生了其他线程对这个值的读操作，就可能会读到一个错误、不完整的值。 JVM的开发者可以自由选择是否把64位的long和double的读写操作作为原子操作去实现，并且规范推荐JVM将其实现为原子操作。 原子操作 + 原子操作 != 原子操作 9.4 什么是内存可见性123456789101112131415161718192021222324252627282930313233343536373839404142434445/* * 内存可见性问题 */public class VisibilityProblem &#123; int a = 10; int b = 20; private synchronized void change() &#123; a = 30; b = a; &#125; private synchronized void print() &#123; System.out.println(\"b=\" + b + \";a=\" + a); &#125; public static void main(String[] args) &#123; while (true) &#123; VisibilityProblem visibilityProblem = new VisibilityProblem(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; visibilityProblem.change(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; visibilityProblem.print(); &#125; &#125;).start(); &#125; &#125;&#125; 第1种情况：假设第1个线程，也就是执行change的线程先运行，并且运行完毕了，然后，第2个线程开始运行，打印出b=30;a=30 第2种情况：与第1种情况相反。因为线程先start，并不代表它真的先执行，所以第2种情况是第2个线程先打印b=20;a=10，然后第1个线程再去进行change 第3种情况：它们几乎同时运行，所以会出现交叉的情况。如第1个线程的change执行到一半，已经把a的值改为30了，而b的值还未来得及修改，此时第2个线程就开始打印，即打印结果为b=20;a=30 第4种情况：发生可见性问题，a的值已经被第1个线程修改了，但是其他线程却看不到，由于a的最新值没能及时同步过来，打印出b=30;a=10 volatile关键字解决可见性问题 synchronized不仅保证了原子性，还保证了可见性 synchronized不仅保证了临界区内最多同时只有一个线程执行操作，同时还保证了在前一个线程释放锁之后，之前所做的所有修改，都能被获得同一个锁的下一个线程所看到，也就是能读取到最新的值。 9.5 主内存与工作内存的关系CPU有多级缓存，导致读的数据过期 为了提高CPU的整体运行效率，减少空闲时间，在CPU和内存之间会有cache层(缓存层)。虽然缓存的容量比内存小，但是缓存的速度却比内存的速度要快得多，其中L1缓存的速度仅次于寄存器的速度。 线程间对于共享变量的可见性问题，并不是由多核引起的，而是由多级缓存引起的。每个核心在获取在获取数据时，都会将数据从内存一层层往上读取，同样，后续对于数据的修改也是先写入到自己的L1缓存中，然后等待时机再逐层往下同步，直到最终刷回内存。 假设core1修改了变量a的值，并写入到了core1的L1缓存里，但是还没来得及继续往下同步，由于core1有它自己的L1缓存，core4是无法直接获取core1的L1缓存的值，那么此时对于core4而言，变量a的值就不是core1修改后的最新的值，core4读取到的可能是一个过期的值，从而引起多线程时的可见性问题发生。 JMM的抽象：主内存和工作内存 每个线程都只能直接接触到工作内存，无法直接操作主内存，而工作内存中所保存的正是主内存的共享变量的副本，主内存和工作内存之间的通信是JMM控制的。 主内存和工作内存的关系 所有的变量都存储在主内存中，同时每个线程拥有自己独立的工作内存，而工作内存中的变量的内容内容是主内存中该变量的拷贝。 线程不能直接读/写主内存中的变量，但可以操作自己工作内存中的变量，然后再同步到主内存中，这样，其他线程就可以看到本次修改。 主内存是由多个线程所共享的，但线程之间不共享各自的工作内存，如果线程间需要通信，则必须借助主内存主内存来完成。 9.6 什么是happens-before规则？Happens-before关系是用来描述可见性相关问题的：如果第一个操作happens-before第二个操作，那么可以认为第一个操作对于第二个操作一定是可见的。 Happens-before的规则 单线程规则 在一个单独的线程中，按照程序代码的顺序，先执行的操作happens-before后执行的操作。 但只要重排序后的结果依然符合happens-before关系，也就是能保持可见性的话，并不会限制重排序的发生。 锁操作规则(synchronized和Lock接口) 如果操作A是解锁，而操作B是对同一个锁的加锁，那么hb(A,B)。 volatile变量规则 对于一个volatile变量的写操作happens-before后面对该变量的读操作。 线程启动规则 Thread对象的start方法happens-before此线程run方法中的每一个操作。 线程join规则 join可以让线程之间等待，假设线程A通过调用threadB.start()启动了一个新线程B，然后调用threadB.join()，那么线程A将一直等待到线程B的run方法结束(不考虑中断等特殊情况)，然会join方法才返回。在join方法返回后，线程A中的所有后续操作都可以看到线程B的run方法执行的所有操作的结果，也就是线程B的run方法里面的操作hanppens-before线程A的join之后的语句。 中断规则 对线程interrupt方法的调用happens-before检测该线程的中断事件。 并发工具类的规则 线程安全的并发容器(如ConcurrentHashMap)在get某个值时一定能看到在此之前发生的put等存入操作的结果。 信号量(Semaphore)会释放许可证，也会获取许可证。释放许可证的操作happens-before获取许可证的操作。 Future：当Future的get方法得到结果的时候，一定可以看到之前任务中所有的操作。 线程池：提交任务的操作happens-before任务的执行。 9.7 volatile的作用是什么？与synchronized有什么异同？volatile是Java中的一个关键字，是一种同步机制。当某个变量是共享变量，且这个变量被volatile修饰，那么在修改了这个变量的值之后，再读取该变量的值时，可以保证获取到的是修改后的最新的值。 相比于synchronized或者Lock，volatile更加轻量，因为使用volatile不会发生上下文切换等开销很大的情况，不会让线程阻塞。 volatile不适用于a++ volatile不适合运用于需要保证原子性的场景。 1234567891011121314151617181920212223242526272829/** * a++ 不适合使用volatile */public class DontVolatile implements Runnable &#123; volatile int a; AtomicInteger realA = new AtomicInteger(); public static void main(String[] args) throws InterruptedException &#123; DontVolatile dontVolatile = new DontVolatile(); Thread thread1 = new Thread(dontVolatile); Thread thread2 = new Thread(dontVolatile); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(dontVolatile.a); System.out.println(dontVolatile.realA); &#125; @Override public void run() &#123; for (int i = 0; i &lt; 1000; i++) &#123; a++; realA.incrementAndGet(); &#125; &#125;&#125;19262000 适用场合1：布尔标记位 第一个例子的操作是a++，这是个复合操作，不具备原子性，而下面这个例子只是把flag设置为true，这样的赋值操作本身就是具备原子性的，所以适合使用volatile。 1234567891011121314151617181920212223242526272829303132/** * 可以使用volatile的场景 布尔标记位 */public class YesVolatile1 implements Runnable &#123; volatile boolean flag = false; AtomicInteger realA = new AtomicInteger(); public static void main(String[] args) throws InterruptedException &#123; YesVolatile1 yesVolatile1 = new YesVolatile1(); Thread thread1 = new Thread(yesVolatile1); Thread thread2 = new Thread(yesVolatile1); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(yesVolatile1.flag); System.out.println(yesVolatile1.realA); &#125; @Override public void run() &#123; for (int i = 0; i &lt; 1000; i++) &#123; realA.incrementAndGet(); setDone(); &#125; &#125; private void setDone() &#123; flag = true; &#125;&#125;true2000 适用场合2：作为触发器，保证其他变量的可见性 1234567891011121314Map configOptions;char[] configText;volatile boolean initialized = false;. . .// In thread AconfigOptions = new HashMap();configText = readConfigFile(fileName);processConfigOptions(configText, configOptions);initialized = true;. . .// In thread Bwhile (!initialized) sleep();// use configOptions happens-before具有传递性，根据happens-before的单线程规则，线程A中configOptions的初始化happens-before对iniialized变量的写入，而线程B中对initialized的读取happens-before对configOptions变量的使用，同时根据happens-before关系的volatile规则，线程A中对initialized的写入为true的操作happens-before线程B中随后对initialized变量的读取。 volatile的作用 保证可见性 对于一个volatile变量的写操作happen-before后面对该变量的读操作，即如果变量被volatile修饰，那么每次修改之后，接下来在读取这个变量的时候一定能读到该变量的最新值。 禁止重排序 as-if-serial：不管怎么重排序，单线程的执行结果不变。多线程情况下的重排序可能会导致严重的线程安全问题。使用volatile关键字可以在一定程度上禁止这种重排序。 volatile和synchronized的关系 相似性：volatile可以看作是一个轻量版的synchronized，如果一个共享变量如果自始至终只被各个线程赋值和读取，而没有其他操作的话，那么就可以用volatile来代替synchronized或者代替原子变量。 不可代替：volatile是不能代替synchronized的，volatile并没有提供原子性和互斥性的。 性能方面：volatile的读写操作都是无锁的，比synchronized性能更好。 9.8 单例模式的双重检查锁模式为什么必须加volatile？单例模式：保证一个类只有一个实例，并且提供一个可以全局访问的入口。 为什么需要使用单例模式？ 为了节省内存、节省计算。 保证结果正确。 方便管理。 有一个私有的Singleton类型的singleton对象；同时构造方法也是私有的，为了防止他人调用构造函数来生成实例；还有一个public的getInstance方法，可通过这个方法获取到单例。 双重检查锁模式的写法 12345678910111213141516171819/** * volatile 双重检查锁 */public class Singleton &#123; private static volatile Singleton singleton; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 进行了两次if(singleton==null)检查，即”双重检查锁”。假设有两个线程同时到达synchronized语句块，那么实例化代码只会由其中先抢到锁的线程执行一次，而后抢到锁的线程会在第二个if判断中发现singleton不为null，所以跳过创建实例的语句。再后面的其他线程再来调用getInstance方法时，只需判断第一次的if(singleton==null)，然后跳过整个if块，直接return实例化后的对象。 为什么需要双重检查？ 如果有两个线程同时调用getInstance方法，由于singleton是空的，因此两个线程可以通过第一重if的检查，然后由于锁的存在，会有一个线程先进入同步语句，并进入第二重检查，而另外一个线程就在外面等待。不过当第一个线程执行完new Singleton()语句后，就会退出synchronized保护的区域，这时如果没有第二重if(singleton==null)判断的话，那么第二个线程也会创建一个实例，破环了单例。 如果去掉第一个检查，那么所有线程都会串行执行，效率低下。 双重检查模式中为什么需要使用volatile关键字？ singleton = new Singleton() 并非是一个原子操作，在JVM中至少做了以下3件事。 给singleton分配内存空间 调用Singleton的构造函数，来初始化singleton 将singleton对象指向分配的内存空间(执行完这步singleton就不是null了) 因为存在指令排序的优化，所以第2，3步的顺序是不能保证的，最终的执行顺序可能是1-2-3，也有可能是1-3-2。 如果是1-3-2： 使用volatile之后，相当于是表明了该字段的更新可能是在其他线程中发生的，在JDK5及后续版本所使用的JMM中，在使用了volatile后，会在一定程度禁止相关语句的重排序。 10.CAS10.1 什么是CAS?CAS(Compare And Swap)，是一种思想，为了保证并发安全，可以使用互斥锁，而CAS的特点就是避免使用互斥锁，当多个线程同时使用CAS更新同一个变量时，只有其中一个线程能够操作成功，而其他线程都会更新失败。不过和同步互斥锁不同的是，更新失败的线程并不会被阻塞，而是被告知这次由于竞争而导致的操作失败，但还可以再次尝试。 CAS的思路 CAS相关的指令是具备原子性的，”比较和交换“操作在执行期间不会被打断。 CAS有3个操作数：内存值V，预期值A、要修改的值B。当预期值A和当前的内存值V相同时，才将内存值修改为B。 CAS会提前假定当前内存值V应该等于值A，而值A往往是之前读取到当时的内存值V，如果发现当前的内存值V恰好是值A的话，那CAS就会把内存值V改成B。如果执行CAS时发现此时的内存值V不等于值A，则说明在刚才计算B的期间内，内存值已经被其他线程修改过了，那么本次CAS就不应该再修改了。 例子 1234567891011121314151617181920212223242526272829303132/** * 模拟CAS操作 等价代码 */public class SimulatedCAS implements Runnable &#123; private int value; public synchronized int compareAngSwap(int expectedValue, int newValue) &#123; int oldValue = value; if (oldValue == expectedValue) &#123; value = newValue; System.out.println(Thread.currentThread().getName() + \"执行成功！\"); &#125; return oldValue; &#125; public static void main(String[] args) throws InterruptedException &#123; SimulatedCAS simulatedCAS = new SimulatedCAS(); simulatedCAS.value = 100; Thread thread1 = new Thread(simulatedCAS); Thread thread2 = new Thread(simulatedCAS); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(simulatedCAS.value); &#125; @Override public void run() &#123; compareAngSwap(100, 150); &#125;&#125; 10.2 CAS的应用并发容器 ConcurrentHashMap putVal方法部分代码 123456789101112131415 final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125;...... &#125; casTabAt 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; U是Unsafe类型的，Unsafe类包含compareAndSwapInt、compareAndSwapLong、compareAndSwapObject等和CAS密切相关的native层的方法，其底层正是利用CPU对CAS指令的支持实现的。 ConcurrentLinkedQueue 非阻塞并发队列ConcurrentLinkedQueue的offer方法里也有CAS的身影，offer方法： 12345678910111213141516171819public boolean offer(E e) &#123; checkNotNull(e); final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); for (Node&lt;E&gt; t = tail, p = t;;) &#123; Node&lt;E&gt; q = p.next; if (q == null) &#123; if (p.casNext(null, newNode)) &#123; if (p != t) casTail(t, newNode); return true; &#125; &#125; else if (p == q) p = (t != (t = tail)) ? t : head; else p = (p != t &amp;&amp; t != (t = tail)) ? t : q; &#125;&#125; 数据库 在更新数据时，可以利用version字段在数据库中实现乐观锁和CAS操作，而在获取和修改数据时都不需要加悲观锁。 12345UPDATE studentSET name = ‘小王’, version = 2 WHERE id = 10 AND version = 1 先去比较version是不是最开始获取到的1，如果和初始值相同才去进行name字段的修改，同时也要把version的值加1。 原子类 在原子类中，如AtomicInteger，也使用了CAS。如AtomicInteger的getAndAdd方法。 1234567891011public final int addAndGet(int delta) &#123; return unsafe.getAndAddInt(this, valueOffset, delta) + delta;&#125;public final int getAndAddInt(Object o, long offset, int delta) &#123; int v; do &#123; v = getIntVolatile(o, offset); &#125; while (!compareAndSwapInt(o, offset, v, v + delta)); return v;&#125; var1 o object 将要修改的对象，传入的是this，也就是atomicInteger这个对象本身 var2 offset offset 偏移量，借助它就可以获取到oldvalue的数值 var5 v expectedValue 代表”期望值” var5+var4 v+delta newValue 希望修改为的新值，var4就是希望原子类所改变的数值，比如可以传入+1，也可以传入-1 Unsafe的getAndAddInt方法是通过循环+CAS的方式来实现的，在此过程中，它会通过compareAndSwapInt方法来尝试更新value的值，如果更新失败就重新获取，然后再次更新，直到更新成功。 10.3 CAS有什么缺点？ABA问题 CAS检查的并不是值有没有发生过变化，而是去比较这当前值和预期值是不是相等，如果变量的值从旧值A变成了新值B再变回旧值A，由于最开始是值A和现在的值A是相等的，所以CAS会认为变量的值在此期间没有发生过变化。所以，CAS并不能检测在此期间值是不是被修改过，它只能检查出现在的值和最初的值是不是一样。 在变量自身之外，再添加一个版本号，A-&gt;B-&gt;A，1A-&gt;2B-&gt;3A，可以通过版本号来判断值是否变化过。 atomic包中提供了AtomicStampedReference这个类，它是专门用来解决ABA问题，解决思路正是利用版本号，AtomicStampedReference会维护一种类似&lt;Object,int&gt;的数据结构，其中的int就是用于计数的，也就是版本号。 自旋时间过长 由于单次CAS不一定能执行成功，所以CAS往往是配合着循环来实现的，有的时候甚至是死循环，不停重试，直到竞争不激烈的时候，才能修改成功。 如果是高并发场景，有可能导致CAS一直操作不成功，循环的时间会越来越长。CPU资源一直在被消耗，会对性能产生很大的影响，高并发情况下，通常CAS的效率是不高的。 范围不能灵活控制 通常执行CAS的时候，是针对某一个，而不是多个共享变量的，多个变量之间是独立的，简单的把原子操作组合到一起，并不具备原子性。 有一个解决方案就是利用一个新的类，来整合刚才这一组共享变量，这个新的类中的多个成员变量就是刚才的那多个共享变量，然后再利用atomic包中的AtomicReference来把这个新对象整体进行CAS操作。 相比之下，如使用synchronized关键字时，如果想把更加的代码加锁，只需把更多的代码放到同步代码块里面就可以了。 11.死锁问题11.1 写一个必然死锁的例子什么是死锁？ 发生在并发中，两个或多个线程(或进程)被无限期的阻塞，相互等待对方手中资源。 例子 两个线程： 多个线程： 死锁的影响 数据库中： 在执行一个事务的时候可能需要获取多把锁，并一直持有这些锁直到事务完成。在某个事务中持有的锁可能在其他事务中也需要，因此在两个事务之间有可能会发生死锁的情况。当数据库检测到这一组事务发生了死锁，根据策略的不同，可能会选择放弃一个事务，被放弃的事务就会释放掉它所持有的锁，从而使其它事务继续进行。此时程序可以重新执行被强行终止的事务。 JVM中： JVM并不会自动进行处理，发生几率不高但危害大，在巨量的次数面前，整个系统发生问题的几率也会被放大。 发生死锁的例子 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 必然死锁的例子 */public class MustDeadLock implements Runnable &#123; public int flag; static Object object1 = new Object(); static Object object2 = new Object(); @Override public void run() &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"的flag为\" + flag); if (flag == 1)&#123; synchronized (object1)&#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (object2)&#123; System.out.println(\"线程\" + Thread.currentThread().getName()+\"获取到了两把锁！\"); &#125; &#125; &#125; if (flag == 2)&#123; synchronized (object2)&#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (object1)&#123; System.out.println(\"线程\" + Thread.currentThread().getName()+\"获取到了两把锁！\"); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; MustDeadLock mustDeadLock1 = new MustDeadLock(); MustDeadLock mustDeadLock2 = new MustDeadLock(); mustDeadLock1.flag = 1; mustDeadLock2.flag = 2; Thread thread1 = new Thread(mustDeadLock1,\"thread1\"); Thread thread2 = new Thread(mustDeadLock2,\"thread2\"); thread1.start(); thread2.start(); &#125;&#125;线程thread1的flag为1线程thread2的flag为2 当第1个线程运行的时候，它会发现自己的flag是1，所以它会尝试先获得object1这把锁，然后休眠500毫秒。 在线程1启动并休眠的期间，线程2同样会启动。由于线程2的flag是2，所以线程2首先会去获取object2这把锁，然后休眠500毫秒。 当线程1的500毫秒休眠时间结束，它会尝试去获取object2这把锁，此时object2这把锁正在被线程2持有，所以线程1无法获取到object2。 紧接着线程2也会苏醒过来，它将尝试获取object1这把锁，此时object1已被线程1持有。 线程1卡在获取object2这把锁的位置，而线程2卡在获取object1这把锁的位置。 11.2 发生死锁的4个必要条件 互斥条件 每个资源每次只能被一个线程(或进程)使用。 请求与保持条件 当一个线程因请求资源而阻塞时，则需对已获得的资源保持不放。 不剥夺条件 线程已获得的资源，在未使用完之前，不会被强行剥夺。 循环等待条件 只有若干个线程之间形成一种头尾相接的循环等待资源关系时，才有可能形成死锁。 11.3 如何定位死锁？ 命令：jstack 1234567891011121314151617181920212223242526272829D:\\IDEAProbject\\JavaStudyDemo\\Multithreading&gt;jps3044 Launcher4084 MustDeadLock11816 JpsD:\\IDEAProbject\\JavaStudyDemo\\Multithreading&gt;jstack 4084Found one Java-level deadlock:=============================\"thread2\": waiting to lock monitor 0x000000001c601e68 (object 0x0000000776319ce0, a java.lang.Object), which is held by \"thread1\"\"thread1\": waiting to lock monitor 0x000000001c6047a8 (object 0x0000000776319cf0, a java.lang.Object), which is held by \"thread2\"Java stack information for the threads listed above:===================================================\"thread2\": at com.example.MustDeadLock.run(MustDeadLock.java:34) - waiting to lock &lt;0x0000000776319ce0&gt; (a java.lang.Object) - locked &lt;0x0000000776319cf0&gt; (a java.lang.Object) at java.lang.Thread.run(Thread.java:748)\"thread1\": at com.example.MustDeadLock.run(MustDeadLock.java:22) - waiting to lock &lt;0x0000000776319cf0&gt; (a java.lang.Object) - locked &lt;0x0000000776319ce0&gt; (a java.lang.Object) at java.lang.Thread.run(Thread.java:748)Found 1 deadlock. 代码：ThreadMXBean 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 必然死锁的例子 */public class MustDeadLock implements Runnable &#123; public int flag; static Object object1 = new Object(); static Object object2 = new Object(); @Override public void run() &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"的flag为\" + flag); if (flag == 1) &#123; synchronized (object1) &#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (object2) &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"获取到了两把锁！\"); &#125; &#125; &#125; if (flag == 2) &#123; synchronized (object2) &#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (object1) &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"获取到了两把锁！\"); &#125; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; MustDeadLock mustDeadLock1 = new MustDeadLock(); MustDeadLock mustDeadLock2 = new MustDeadLock(); mustDeadLock1.flag = 1; mustDeadLock2.flag = 2; Thread thread1 = new Thread(mustDeadLock1, \"thread1\"); Thread thread2 = new Thread(mustDeadLock2, \"thread2\"); thread1.start(); thread2.start(); Thread.sleep(2000); ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); long[] deadlockedThreads = threadMXBean.findDeadlockedThreads(); if (deadlockedThreads != null &amp;&amp; deadlockedThreads.length &gt; 0) &#123; for (int i = 0; i &lt; deadlockedThreads.length; i++) &#123; ThreadInfo threadInfo = threadMXBean.getThreadInfo(deadlockedThreads[i]); System.out.println(threadInfo); &#125; &#125; &#125;&#125;线程thread1的flag为1线程thread2的flag为2\"thread2\" Id=21 BLOCKED on java.lang.Object@27d6c5e0 owned by \"thread1\" Id=20\"thread1\" Id=20 BLOCKED on java.lang.Object@4f3f5b24 owned by \"thread2\" Id=21 11.4 解决死锁问题的策略 避免策略 优化代码逻辑，从根本上消除发生死锁的可能性，如调整锁的获取顺序。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 调整锁的获取顺序来避免死锁问题 */public class TransferMoney implements Runnable &#123; static class Account &#123; int balance; public Account(int balance) &#123; this.balance = balance; &#125; &#125; int flag; static Account a = new Account(500); static Account b = new Account(500); @Override public void run() &#123; if (flag == 1) &#123; transferMoney(a, b, 200); &#125; if (flag == 0) &#123; transferMoney(b, a, 200); &#125; &#125; public static void transferMoney(Account from, Account to, int account) &#123; int fromHash = System.identityHashCode(from); int toHash = System.identityHashCode(to); // 模拟网络延迟 try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (fromHash &gt; toHash) &#123; // 先获取两把锁，然后开始转账 synchronized (to) &#123; synchronized (from) &#123; if (from.balance - account &lt; 0) &#123; System.out.println(\"余额不足，转账失败！\"); return; &#125; from.balance -= account; to.balance += account; System.out.println(\"成功转账\" + account + \"元！\"); &#125; &#125; &#125; if (toHash &gt; fromHash) &#123; // 先获取两把锁，然后开始转账 synchronized (from) &#123; synchronized (to) &#123; if (from.balance - account &lt; 0) &#123; System.out.println(\"余额不足，转账失败！\"); return; &#125; from.balance -= account; to.balance += account; System.out.println(\"成功转账\" + account + \"元！\"); &#125; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; TransferMoney r1 = new TransferMoney(); TransferMoney r2 = new TransferMoney(); r1.flag = 1; r2.flag = 0; Thread t1 = new Thread(r1); Thread t2 = new Thread(r2); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(\"a的余额\" + a.balance); System.out.println(\"b的余额\" + b.balance); &#125;&#125; 业务实际上不在乎获取锁的顺序，调整获取锁的顺序，使先获取的账户是”转入”或”转出”无关，而是使用HashCode的值来决定顺序，从而保证线程安全。但依然有极小的概率会发生HashCode相同的情况，在实际生产中，需要排序的往往是一个实体类，而一个实体类一般都会具有主键ID，主键ID具有唯一、不重复的特点，直接使用主键ID排序，按照主键ID的大小来决定获取锁的顺序，以确保避免死锁。 检测与恢复策略 先允许系统发生死锁，然后再解除。例如系统可以在每次调用锁的时候，都记录下来调用信息，形成一个”锁的调用链图”，然后隔一段时间就用死锁检测算法来检测一下，搜索这个图中是否存在环路，一旦发生死锁，就可以用死锁恢复机制，解开死锁，进行恢复。 线程终止 系统逐个去终止已经陷入死锁的线程，线程被终止，同时释放资源，死锁就会被解开。有各种各样的算法和策略，根据实际业务进行调整。 优先级 先终止优先级低的线程。 已占用资源、还需要的资源 如果某线程已经占有了一大堆资源，只需要最后一点点资源就可以顺利完成任务，那么系统会优先终止别的线程来优先促成该线程的完成。 已经运行时间 如果某线程已经运行很多天了，很快就要完成任务了，可以让那些刚刚开始运行的线程终止，并在之后把它们重新启动，这样成本更低。 资源抢占 不需要把整个线程终止，而是只需要把它已经获得的资源进行剥夺，如让线程回退几步、释放资源，这样就不需要终止掉整个线程，成本更低。但如果算法不好的话，我们抢占的那个线程可能一直是同一个线程，就会造成饥饿线程，即这个线程一直被剥夺它已经得到的资源，那么它就长期得不到运行。 鸵鸟策略 如果系统发生死锁的概率极低，并且一旦发生其后果不是特别严重，可以先选择忽略它，直到发生死锁后，再人工修复。 12.final关键字12.1 final的三种用法final修饰变量 final修饰的变量，一旦被赋值就不能被修改了。 目的：1.设计角度 2.线程安全 赋值时机： 成员变量，类中的非static修饰的属性 在变量的等号右边直接赋值 123public class FinalFieldAssignment1 &#123; private final int finalVar = 0;&#125; 在构造函数中赋值 123456class FinalFieldAssignment2 &#123; private final int finalVar; public FinalFieldAssignment2() &#123; finalVar = 0; &#125;&#125; 在类的构造代码块中赋值(不常用) 123456class FinalFieldAssignment3 &#123; private final int finalVar; &#123; finalVar = 0; &#125;&#125; 静态变量，类中的被static修饰的属性 在声明变量的等号右边直接赋值 123public class StaticFieldAssignment1 &#123; private static final int a = 0;&#125; 在一个静态的static初始代码块中赋值 123456class StaticFieldAssignment2 &#123; private static final int a; static &#123; a = 0; &#125;&#125; static的final变量不能在构造函数中进行赋值 局部变量，方法中的变量 使用前赋值即可 final修饰参数，意味着在方法内部无法对参数进行修改。 123456public class FinalPara &#123; public void withFinal(final int a) &#123; System.out.println(a);//可以读取final参数的值// a = 9; //编译错误，不允许修改final参数的值 &#125;&#125; final修饰方法 提高效率，早期的Java版本，会把final修饰的方法转为内嵌调用，消除方法调用的开销。 final修饰的方法不可以被重写。 final的private方法 12345678public class PrivateFinalMethod &#123; private final void privateEat() &#123; &#125;&#125;class SubClass2 extends PrivateFinalMethod &#123; private final void privateEat() &#123;//编译通过，但这并不是真正的重写 &#125;&#125; 类中的所有private方法都是隐式的指定为自动被final修饰的，由于这个方法是private类型的，所以对于子类而言，根本获取不到父类这个方法，更别说重写了。所以其实子类并没有真正意义上的去重写父类的privateEat方法，只是方法名碰巧一样而已。 final修饰类 final修饰的类不可被继承 类是final的，不代表里面的属性就会自动加上final。 final的类里面，所有的方法，不论是public、private还是其他权限修饰符修饰的，都会自动的、隐式的被指定为是final的。 12.2 为什么加了final却依然无法拥有”不变性”？如果对象在被创建之后，其状态就不能修改了，那么它就具备”不变性”。 final修饰对象时，只是引用不可变。当用final去修饰一个指向对象类型(而不是指向8种基本数据类型)的变量的时候，那么final起到的作用只是保证则个变量的引用不可变，而对象本身的内容依然是可变化的。 123456789101112131415161718192021222324class Test &#123; int p = 20; public static void main(String args[])&#123; final Test t = new Test(); t.p = 30; System.out.println(t.p); &#125;&#125;30class Test &#123; public static void main(String args[]) &#123; final int arr[] = &#123;1, 2, 3, 4, 5&#125;; // 注意，数组 arr 是 final 的 for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = arr[i]*10; System.out.println(arr[i]); &#125; &#125;&#125;1020304050 final修饰一个指向对象的变量的时候，对象本身的内容依然是可以变化的。 final和不可变的关系 final可以确保变量的引用保持不变，但是不变性意味着对象一旦创建完毕就不能改变其状态，它强调的是对象内容本身，而不是引用。 1234567891011public class ImmutableDemo &#123; private final Set&lt;String&gt; lessons = new HashSet&lt;&gt;(); public ImmutableDemo() &#123; lessons.add(\"第01讲：为何说只有 1 种实现线程的方法？\"); lessons.add(\"第02讲：如何正确停止线程？为什么 volatile 标记位的停止方法是错误的？\"); lessons.add(\"第03讲：线程是如何在 6 种状态之间转换的？\"); &#125; public boolean isLesson(String name) &#123; return lessons.contains(name); &#125;&#125; 包含对象类型的成员变量的类的对象，具备不可变性的例子：对于ImmutableDemo类而言，它只有这么一个成员变量，而这个成员变量一旦构造完毕后又不能改变。 12.3 为什么String被设计为是不可变的？12345String s = \"lagou\";s = \"la\";String lagou = \"lagou\";lagou = lagou.subString(0, 4); 只不是建了一个新的字符串而已，并把引用重新指向。 123456public final class String implements Java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; //...&#125; private final的char数组value，存储着字符串的每一位字符，value一旦被赋值，引用就不能修改了；并且在String的源码中，除构造函数之外，并没有任何其他方法会修改value数组里面的内容，而value的权限是private，外部的类也访问不到，所以value是不可变的。String类是被final修饰的，所以这个String类是不会被继承的。 String不变的好处 字符串常量池 用作HashMap的key 对于key来说，最重要的就是不可变，这样才能利用它去检索存储在HashMap里面的value。由于HashMap的工作原理是Hash，也就是散列，所以需要对象始终拥有相同的Hash值才能正常运行。 缓存HashCode 12/** Cache the hash code for the String */private int hash; 在String类中有一个hash属性，保存的是String对象的HashCode。因为String是不可变的，所以对象一旦被创建之后，HashCode的值也就不可能变化了，就可以把HashCode缓存起来。以后每次想用到HashCode的时候，不需要重新计算，直接返回缓存过的hash的值就可以了，所以使得字符串非常适合用作HashMap的key。 线程安全 线程安全，具备不变性的对象一定是线程安全的，避免了很多不必要的同步操作。 13.AQS13.1 为什么需要AQS? AQS在ReentrantLock、ReentrantReadWriteLock、Semaphore、CountDownLatch、ThreadPoolExcutor中都有运用(JDK1.8)，AQS是这些类的底层原理。 AQS是一个用于构建锁、同步器等线程协作工具类的框架，有了AQS之后，可以让更上层的开发极大的减少工作量，避免重复造轮子，同时也避免了上层因处理不当而导致的线程安全问题，因为AQS把这些事情都做好了。总之，有了AQS之后，构建线程协作工具类就容易多了。 13.2 AQS内部原理状态 1234/** * The synchronization state. */private volatile int state; state的含义并不是一成不变的，它会根据具体实现类的作用不同而表示不同的含义。 比如在信号量里，state表示的是剩余许可证的数量。当某一个线程衢州一个许可证之后，state会减1。 在CountDownLatch工具类里，state表示的是需要”倒数”的数量。每次调用CountDown方法时，state就会减1，直到减为0就代表这个”门闩”被放开。 在ReentrantLock中表示的是锁的占有情况。最开始是0，表示没有任何线程占有锁，如果state变成1，就代表这个锁已经被某一个线程所持有了。因为ReentrantLock是可重入的，同一个线程可以再次拥有这把锁就叫重入。如果这个锁被同一个线程多次获取，那么state就会逐渐的往上加，state的值表示重入的次数。在释放的时候也是逐步递减。 compareAndSetState： 1234protected final boolean compareAndSetState(int expect, int update) &#123; // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update);&#125; 利用了Unsafe里面的CAS操作，利用CPU指令的原子性保证了这个操作的原子性。 setState： 123protected final void setState(int newState) &#123; state = newState;&#125; 对于基本类型的变量进行直接赋值时，如果加了volatile就可以保证它的线程安全。 FIFO队列 先进先出队列，主要的作用是存储等待的线程。当多个线程去竞争同一把锁的时候，就需要用排队机制把那些没拿到锁的线程串在一起；而当前面的线程释放锁之后，这个管理器就会挑选一个合适的线程来尝试抢刚刚释放的那把锁。 队列内部是双向链表的形式： 在队列中，分别用head和tail来表示头节点和尾节点，两者在初始化的时候指向一个空节点。头节点可以理解为”当前持有锁的线程”，而在头节点之后的线程被阻塞了，它们会等待被唤醒，唤醒也是由AQS负责操作的。 获取/释放方法 获取方法 获取操作通常会依赖state变量的值，获取方法在不同类中代表不同的含义，但往往和state值相关，也经常会让线程进入阻塞状态。 如ReentrantLock中的lock方法，执行时如果发现state不等于0且当前线程不是持有锁的线程，那么就代表这个锁已经被其他线程所持有了，这个时候，当然获取不到锁，于是就让该线程进入阻塞状态。 Semaphore中的acquire，作用是获取许可证。如果state是正数，那么代表还有剩余的许可证，数量足够就可以获取成功；但如果state是0，则代表已经没有更多的空余许可证了，会进入阻塞状态。 CountDownLatch获取方法就是await方法，作用是”等待，直到倒数结束”。执行await的时候会判断state的值，如果state不等于0，线程就进入阻塞状态，直到其他线程执行倒数方法把state减为0，此时就代表这个门闩放开了，所以之前阻塞的线程就会被唤醒。 释放方法 释放方法通常是不会阻塞线程的。 比如在Semaphore信号量，释放就是release方法，release()方法的作用是去释放一个许可证，会让state加1；而在CountDownLatch里面，释放就是countDown方法，作用是倒数一个数，让state减1。 13.3 AQS在CountDownLatch中应用原理AQS用法 新建一个自己的线程协作工具类，在内部写一个Sync类，该类继承AbstractQueueSynchronizer，即AQS。 在Sync类中，根据是否是独占来重写对应的方法。独占，则重写tryAcquire和tryRelease等方法；非独占，则重写tryAcquireShared和tryReleaseShared等方法。 在自己的线程协作工具类中，实现获取/释放的相关方法，并在里面调用AQS对应的方法，独占调用acquire或release等方法；非独占调用acquireShared或releaseShared或acquireSharedInterruptibly。 AQS在CountDownLatch的应用 在CountDownLatch里面有一个子类Sync，这个类正是继承自AQS。 12345678910111213141516171819202122232425262728293031public class CountDownLatch &#123; /** * Synchronization control For CountDownLatch. * Uses AQS state to represent count. */ private static final class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 4982264981922014374L; Sync(int count) &#123; setState(count); &#125; int getCount() &#123; return getState(); &#125; protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1; &#125; protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125; &#125; private final Sync sync; //省略其他代码...&#125; 构造函数： 1234public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException(\"count &lt; 0\"); this.sync = new Sync(count);&#125; CountDown构造函数将传入的count最终传递到AQS内部的state变量，给state赋值，state就代表还需要倒数的次数。 getCount： 123public long getCount() &#123; return sync.getCount();&#125; 最终获取到的就是Sync中state的值。 countDown： 1234567891011public void countDown() &#123; sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; Sync中的tryReleaseShared()，doReleaseShared()对之前阻塞的线程进行唤醒。 await： 该方法时CountDownLatch的”获取”方法，调用await会把线程阻塞，直到倒数为0才能继续执行。 1234567891011public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125;public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125; Sync中的tryAcquireShared()，doAcquireSharedInterruptibly()会让线程进入阻塞状态。 总结 当线程调用CountDownLatch的await方法时，便会尝试获取”共享锁”，不过一开始通常获取不到锁，于是线程被阻塞。”共享锁”可以获取到的条件是”锁计数器”的值为0，而”锁计数器”的初始值为count，当每次调用CountDownLatch对象的countDown方法时，也可以把”锁计数器”-1。直到”锁计数器”为0，于是之前等待的线程就会继续运行了，并且此时如果再有线程想调用await方法时也会立刻放行，不会再去做任何阻塞操作了。","categories":[{"name":"多线程","slug":"多线程","permalink":"https://imokkkk.github.io/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"https://imokkkk.github.io/tags/多线程/"},{"name":"并发编程","slug":"并发编程","permalink":"https://imokkkk.github.io/tags/并发编程/"}]},{"title":"HashMap分析","slug":"HashMap分析","date":"2021-05-08T13:19:59.130Z","updated":"2020-07-05T14:21:39.646Z","comments":true,"path":"HashMap/","link":"","permalink":"https://imokkkk.github.io/HashMap/","excerpt":"1.概述HashMap位于java.util包中，HashMap基于Map接口实现，元素以键值对的方式存储，并且允许使用null键和null值，因为key不允许重复，因此只能有一个键为null,另外HashMap不能保证放入元素的顺序，它是无序的，和放入的顺序并不能相同。HashMap是线程不安全的。它的底层为哈希表结构（链表散列：数组+链表）实现，结合数组和链表的优点。JDK1.8之后，当链表长度超过 8 时，链表转换为红黑树。","text":"1.概述HashMap位于java.util包中，HashMap基于Map接口实现，元素以键值对的方式存储，并且允许使用null键和null值，因为key不允许重复，因此只能有一个键为null,另外HashMap不能保证放入元素的顺序，它是无序的，和放入的顺序并不能相同。HashMap是线程不安全的。它的底层为哈希表结构（链表散列：数组+链表）实现，结合数组和链表的优点。JDK1.8之后，当链表长度超过 8 时，链表转换为红黑树。 HashMap主要属性： 1234567891011121314151617181920212223public class HashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements Map&lt;K, V&gt;, Cloneable, Serializable &#123; private static final long serialVersionUID = 362498820763181265L; //HashMap的初始容量大小为16(1&lt;&lt;4)，指的是存储元素的数组大小，即桶的数量 //为啥是16呢? 因为在使用2的幂的数字的时候，Length-1的值是所有二进制位全为1，这种情况下，index的结果等同于HashCode后几位的值。只要输入的HashCode本身分布均匀，Hash算法的结果就是均匀的。这是为了实现均匀分布。 static final int DEFAULT_INITIAL_CAPACITY = 16; //HashMap的最大容量(1&lt;&lt;30) //使用位与运算速度更快 static final int MAXIMUM_CAPACITY = 1073741824; //默认负载因子0.75 //当HashMap中的数据量/HashMap的总容量=0.75或者指定值时，HashMap的总容量自动扩展一倍 //负载因子代表了hash表中元素的填满程度。加载因子越大，填满的元素越多，但是冲突的机会增大了，链表越来越长，查询速度会降低。反之，如果加载因子过小，冲突的机会减小了，但是hash表过于稀疏。冲突越大，查找的成本就越高。 static final float DEFAULT_LOAD_FACTOR = 0.75F; //由链表转换成树的阈值:8 //在存储数据时，当某一个桶中链表的长度&gt;=8时，链表结构会转换成红黑树结构(其实还要求桶的中数量&gt;=64) static final int TREEIFY_THRESHOLD = 8; //红黑树转为链表的阈值:6 //当在扩容(resize())时（此时HashMap的数据存储位置会重新计算），在重新计算存储位置后，当原有的红黑树内数量&lt;6时，则将红黑树转换成链表 static final int UNTREEIFY_THRESHOLD = 6; //最小树形化容量阈值:64 //当哈希表中的容量&gt;该值时，才允许链表转换成红黑树 static final int MIN_TREEIFY_CAPACITY = 64; ...... &#125; 2.NodeJDK 1.8采用的是Node数组，实质上还是Entry数组来存储key-value对，每一个键值对组成了一个Entry实体，Entry类实际上是一个单向的链表结构，它具有Next指针，可以连接下一个Entry实体，以此来解决Hash冲突的问题。一个桶中链表的长度&lt;8时:一个桶中链表的长度&gt;=8时:数组存储区间是连续的，占用内存严重，故空间复杂度很大。但数组的二分查找时间复杂度小，为O(1)；数组的特点是：寻址容易，插入和删除困难；链表存储区间离散，占用内存比较宽松，故空间复杂度很小，但时间复杂度很大，达O(n)。链表的特点是：寻址困难，插入和删除容易。HashMap的设计正是分别结合了数组和链表的优点。 123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + \"=\" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 3.hash算法首先获取对象的hashCode()值，然后将hashCode值右移16位，然后将右移后的值与原来的hashCode做异或(^)运算，返回结果。(其中h&gt;&gt;&gt;16，在JDK1.8中，优化了高位运算的算法，使用了零扩展，无论正数还是负数，都在高位插入0) 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 4.put()HashMap并没有直接提供putVal接口给用户调用，而是提供的put方法，而put方法就是通过putVal来插入元素的。 判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； 根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向6，如果table[i]不为空，转向3； 判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向4，这里的相同指的是hashCode以及equals； 判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向5； 遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； 插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125;//put函数的核心处理函数 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //HashMap是懒加载，所有的put先检查table数组是否已经初始化，没有初始化则进行扩容数组初始化table数组，保证table数组一定初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //通过hash算法(n - 1) &amp; hash找到数组下标得到数组元素，为空则新建 //(n-1)&amp;hash就等价于hash%n。&amp;运算的效率高于%运算。 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //找到数组元素，hash相等同时key相等，则直接覆盖，执行赋值操作 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // hash值不相等，即key不相等 //判断链表是否是红黑树 else if (p instanceof TreeNode) //放入树中 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //遍历当前的链表，一直遍历到链表末尾 for (int binCount = 0; ; ++binCount) &#123; //到达链表的尾部 if ((e = p.next) == null) &#123; //在尾部插入结点(JDK1.8之前采用头插法，JDK1.8之后采用尾插法，使用尾插，在扩容时会保持链表元素原本的顺序，就不会出现链表成环) p.next = newNode(hash, key, value, null); //当链表长度超过8(阈值)，就会将链表便转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //表示在桶中找到key值、hash值与插入元素相等的结点 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) //用新值替换旧值 e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //记录修改次数 ++modCount; //判断是否需要扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; HashMap的数据存储实现流程 根据key计算得到key.hash = (h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)； 根据key.hash计算得到桶数组的索引index = key.hash &amp; (table.length - 1)，这样就找到该key的存放位置了 ① 如果该位置没有数据，用该数据新生成一个节点保存新数据，返回null； ② 如果该位置有数据是一个红黑树，那么执行相应的插入 / 更新操作； ③ 如果该位置有数据是一个链表，分两种情况一是该链表没有这个节点，另一个是该链表上有这个节点，注意这里判断的依据是key.hash是否一样： 如果该链表没有这个节点，那么采用尾插法新增节点保存新数据，返回null；如果该链表已经有这个节点了，那么找到该节点并更新新数据，返回老数据。 注意： HashMap的put会返回key的上一次保存的数据，比如： 1234HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();System.out.println(map.put(\"a\", \"A\")); // 打印nullSystem.out.println(map.put(\"a\", \"AA\")); // 打印ASystem.out.println(map.put(\"a\", \"AB\")); // 打印AA 5.get()HashMap同样并没有直接提供getNode接口给用户调用，而是提供的get方法，而get方法就是通过getNode来取得元素的。 123456789101112131415161718192021222324252627282930public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //若table已经初始化，长度大于0，根据hash寻找table中的项也不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //永远检查第一个node，桶中第一项(数组元素)相等 //在Hashmap1.8中，无论是存元素还是取元素，都是优先判断bucket上第一个元素是否匹配，而在1.7中则是直接遍历查找 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) //一次就匹配到了，直接返回， //否则进行搜索 return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) //红黑树搜索/查找 return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; //链表搜索(查找) if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 基本流程: 根据key计算hash; 检查数组是否为空，为空返回null; 根据hash计算bucket位置，如果bucket第一个元素是目标元素，直接返回。否则执行4; 如果bucket上元素大于1并且是树结构，则执行树查找。否则执行5; 如果是链表结构，则遍历寻找目标 6.resize()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table;//定义了一个临时Node类型的数组 int oldCap = (oldTab == null) ? 0 : oldTab.length;//判断目前的table数组是否为空，记录下当前数组的大小 int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123;//如果oldCap不为空的话，就是hash桶数组不为空 //如果已达到最大容量不再扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) //通过位运算扩容到原来的两倍 newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold //用构造器初始化了阈值，将阈值直接赋值给容量 newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) //初始化新的Node类型数组 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; //将新数组的值复制给旧的hash桶数组 table = newTab; //当原来的table不为空，需要将数据迁移到新的数组里面去 if (oldTab != null) &#123; //开始对原table进行遍历 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; //取出这个数组第一个不为空的元素 if ((e = oldTab[j]) != null) &#123; //将旧的hash桶数组在j结点处设置为空，把空间释放掉，方便gc oldTab[j] = null; //如果e后面没有Node结点 if (e.next == null) //计算相应的hash值，把节点存储到新table的对应位置处 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode)///如果e是红黑树的类型，按照红黑树的节点移动 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next;//将Node结点的next赋值给next //如果结点e的hash值与原hash桶数组的长度作与运算为0 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; //如果结点e的hash值与原hash桶数组的长度作与运算不为0 else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 7.面试题 Object若不重写hashcode()的话，hashcode是如何计算出来的？Object的hashcode()方法是本地方法，该方法直接返回对象的内存地址，如果不重写hashcode()，则任何对象的hashcode都不相等。(然而hashmap想让部分值的hashcoe值相等，所以需要重写) 为什么重写equals()还要重写hashcode()？HashMap中比较key先求出key的hashcode()，比较其值是否相等，相等则比较equals()，若相等则认为它们是相等的，若equals()不相等则认为它们是不相等的。如果只重写equals()不重写hashcode()，就会导致相同的key值被认为不同(如果不重写hashcode()，则任何对象的hashcode都不相等)，就会在HashMap中存储相同的key值(map中key值不能相同)。①如果两个对象相同（equals()比较返回true)，那么它们的hashcode值一定相同；②如果两个对象的hashcode值相同，它们并不一定相同(equals()比较返回false)","categories":[{"name":"源码分析","slug":"源码分析","permalink":"https://imokkkk.github.io/categories/源码分析/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"https://imokkkk.github.io/tags/HashMap/"},{"name":"源码","slug":"源码","permalink":"https://imokkkk.github.io/tags/源码/"},{"name":"底层","slug":"底层","permalink":"https://imokkkk.github.io/tags/底层/"}]},{"title":"Git","slug":"Git","date":"2021-05-08T13:19:59.126Z","updated":"2020-08-05T07:34:41.314Z","comments":true,"path":"git_shell/","link":"","permalink":"https://imokkkk.github.io/git_shell/","excerpt":"Git1.创建版本库 git init 把这个目录变成Git可以管理的仓库 git add readme.md 把文件添加到仓库 git commit -m “wrote a readme file” -m后面输入的是本次提交的说明","text":"Git1.创建版本库 git init 把这个目录变成Git可以管理的仓库 git add readme.md 把文件添加到仓库 git commit -m “wrote a readme file” -m后面输入的是本次提交的说明 2.版本回退 修改readme.md内容git add readme.txtgit commit -m “append GPL”ps: git log可以查看版本历史记录 git reset –hard HEAD^ps: HEAD^表示上一个版本 HEAD^表示上上个版本 HEAD~100表示往上100个版本 命令行窗口还没有被关掉时，找到某个版本的commit id，也能进行回退。git reset –hard xxxxxxxx命令行窗口被被关掉时，使用git reflog查看记录的每一次命令 3.工作区和暂存区 工作区电脑里能看到的目录，比如gittest文件夹就是一个工作区 版本库工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库git add命令实际上就是把要提交的所有修改放到暂存区（Stage），然后，执行git commit就可以一次性把暂存区的所有修改提交到分支。git add . 添加所有数据git commit -m “understand how stage works” 4.管理修改 对readme.txt做一个修改git add readme.md再修改readme.mdgit commit -m “git tracks changes” 提交 git status然而，第二次修改并没有被提交，第一次修改 -&gt; git add -&gt; 第二次修改 -&gt; git commitGit管理的是修改，当你用git add命令后，在工作区的第一次修改被放入暂存区，准备提交，但是，在工作区的第二次修改并没有放入暂存区，所以，git commit只负责把暂存区的修改提交了，也就是第一次的修改被提交了，第二次的修改不会被提交。git diff HEAD – readme.md 查看工作区和版本库里面最新版本的区别 提交第二次修改第一次修改 -&gt; git add -&gt; 第二次修改 -&gt; git add -&gt; git commit 5.撤销修改 git checkout – readme.md 丢弃工作区的修改一种是readme.md自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是readme.md已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 git add后修改git reset HEAD readme.md 把暂存区的修改撤销掉git checkout – readme.md 丢弃工作区的修改 6.删除文件 先添加一个新文件test.txt到Git并且提交git add test.txtgit commit -m “add test.txt” 直接在文件管理器中把没用的文件删了，或者用rm命令删除rm test.txt从版本库中删除该文件，那就用命令git rm删掉，并且git commitgit rm test.txtgit commit -m “remove test.txt”删错了，因为版本库里还有，所以可以把误删的文件恢复到最新版本git checkout – test.txt 7.远程仓库 添加远程库在GitHub创建一个Git仓库git remote add origin git@github.com:xxxxxxxx 关联git push -u origin master 把本地库的所有内容推送到远程库上 从远程库克隆git clone git@github.com:xxxxxxxxx 8.分支管理 创建与合并分支git switch -c dev 创建dev分支，然后切换到dev分支git branch 查看当前分支修改readme.md git add readme.md git commit -m “branch test”git switch master 切换回master分支再查看readme.md文件，刚才添加的内容不见了！因为那个提交是在dev分支上，而master分支此刻的提交点并没有变。git merge dev 把dev分支的工作成果合并到master分支上git branch -d dev 删除dev分支 解决冲突git switch -c feature1 准备新的feature1分支修改readme.mdgit add readme.mdgit commit -m “AND simple” 在feature1分支上提交git switch master 切换到master分支在master分支上把readme.md进行不同修改git add readme.mdgit commit -m “&amp; simple” 在master分支上提交这种情况下，Git无法执行“快速合并”，只能试图把各自的修改合并起来，但这种合并就可能会有冲突。git status 查看冲突的文件手动修改文件解决冲突后再提交git add readme.txtgit commit -m “conflict fixed”git branch -d feature1 删除feature1分支准备合并dev分支，请注意--no-ff参数，表示禁用Fast forward Bug分支git stash 把当前工作现场“储藏”起来，等以后恢复现场后继续工作git switch -c issue-101修改Bug，然后提交git add readme.txtgit commit -m “fix bug 101”修复完成后，切换到master分支，并完成合并，最后删除issue-101分支git switch mastergit merge –no-ff -m “merged bug fix 101” issue-101git switch devgit stash pop 删除stash复制一个特定的提交到当前分支git loggit cherry-pick 4e4858db453de3da86ee2eafc2 多人协作git remote -v 查看远程库的详细信息多人协作的工作模式通常是这样： 首先，可以试图用git push origin推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin推送就能成功！如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream-to origin/。 忽略特殊文件在Git工作区的根目录下创建一个特殊的.gitignore文件，然后把要忽略的文件名填进去，Git就会自动忽略这些文件。不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。所有配置文件可以直接在线浏览：https://github.com/github/gitignore把.gitignore也提交到Git","categories":[{"name":"Git","slug":"Git","permalink":"https://imokkkk.github.io/categories/Git/"}],"tags":[{"name":"shell","slug":"shell","permalink":"https://imokkkk.github.io/tags/shell/"},{"name":"Git","slug":"Git","permalink":"https://imokkkk.github.io/tags/Git/"}]}],"categories":[{"name":"云原生","slug":"云原生","permalink":"https://imokkkk.github.io/categories/云原生/"},{"name":"操作系统","slug":"操作系统","permalink":"https://imokkkk.github.io/categories/操作系统/"},{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/categories/Spring/"},{"name":"数据库","slug":"数据库","permalink":"https://imokkkk.github.io/categories/数据库/"},{"name":"分布式","slug":"分布式","permalink":"https://imokkkk.github.io/categories/分布式/"},{"name":"JVM","slug":"JVM","permalink":"https://imokkkk.github.io/categories/JVM/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://imokkkk.github.io/categories/计算机网络/"},{"name":"多线程","slug":"多线程","permalink":"https://imokkkk.github.io/categories/多线程/"},{"name":"项目","slug":"项目","permalink":"https://imokkkk.github.io/categories/项目/"},{"name":"shell","slug":"shell","permalink":"https://imokkkk.github.io/categories/shell/"},{"name":"前端","slug":"前端","permalink":"https://imokkkk.github.io/categories/前端/"},{"name":"微服务","slug":"微服务","permalink":"https://imokkkk.github.io/categories/微服务/"},{"name":"Redis","slug":"Redis","permalink":"https://imokkkk.github.io/categories/Redis/"},{"name":"源码分析","slug":"源码分析","permalink":"https://imokkkk.github.io/categories/源码分析/"},{"name":"Git","slug":"Git","permalink":"https://imokkkk.github.io/categories/Git/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://imokkkk.github.io/tags/Prometheus/"},{"name":"Grafana","slug":"Grafana","permalink":"https://imokkkk.github.io/tags/Grafana/"},{"name":"监控","slug":"监控","permalink":"https://imokkkk.github.io/tags/监控/"},{"name":"Netty","slug":"Netty","permalink":"https://imokkkk.github.io/tags/Netty/"},{"name":"Kafka","slug":"Kafka","permalink":"https://imokkkk.github.io/tags/Kafka/"},{"name":"NIO","slug":"NIO","permalink":"https://imokkkk.github.io/tags/NIO/"},{"name":"Spring","slug":"Spring","permalink":"https://imokkkk.github.io/tags/Spring/"},{"name":"Docker","slug":"Docker","permalink":"https://imokkkk.github.io/tags/Docker/"},{"name":"MySQL","slug":"MySQL","permalink":"https://imokkkk.github.io/tags/MySQL/"},{"name":"分库分表","slug":"分库分表","permalink":"https://imokkkk.github.io/tags/分库分表/"},{"name":"ShardingSphere","slug":"ShardingSphere","permalink":"https://imokkkk.github.io/tags/ShardingSphere/"},{"name":"分布式","slug":"分布式","permalink":"https://imokkkk.github.io/tags/分布式/"},{"name":"事务","slug":"事务","permalink":"https://imokkkk.github.io/tags/事务/"},{"name":"源码","slug":"源码","permalink":"https://imokkkk.github.io/tags/源码/"},{"name":"JVM","slug":"JVM","permalink":"https://imokkkk.github.io/tags/JVM/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://imokkkk.github.io/tags/Kubernetes/"},{"name":"云原生","slug":"云原生","permalink":"https://imokkkk.github.io/tags/云原生/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://imokkkk.github.io/tags/计算机网络/"},{"name":"HTTP","slug":"HTTP","permalink":"https://imokkkk.github.io/tags/HTTP/"},{"name":"TCP","slug":"TCP","permalink":"https://imokkkk.github.io/tags/TCP/"},{"name":"UDP","slug":"UDP","permalink":"https://imokkkk.github.io/tags/UDP/"},{"name":"数据库","slug":"数据库","permalink":"https://imokkkk.github.io/tags/数据库/"},{"name":"代理模式","slug":"代理模式","permalink":"https://imokkkk.github.io/tags/代理模式/"},{"name":"javase","slug":"javase","permalink":"https://imokkkk.github.io/tags/javase/"},{"name":"多线程","slug":"多线程","permalink":"https://imokkkk.github.io/tags/多线程/"},{"name":"并发编程","slug":"并发编程","permalink":"https://imokkkk.github.io/tags/并发编程/"},{"name":"tool","slug":"tool","permalink":"https://imokkkk.github.io/tags/tool/"},{"name":"项目部署","slug":"项目部署","permalink":"https://imokkkk.github.io/tags/项目部署/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://imokkkk.github.io/tags/Spring-Boot/"},{"name":"微信公众号","slug":"微信公众号","permalink":"https://imokkkk.github.io/tags/微信公众号/"},{"name":"面试","slug":"面试","permalink":"https://imokkkk.github.io/tags/面试/"},{"name":"Vue","slug":"Vue","permalink":"https://imokkkk.github.io/tags/Vue/"},{"name":"算法","slug":"算法","permalink":"https://imokkkk.github.io/tags/算法/"},{"name":"AOP","slug":"AOP","permalink":"https://imokkkk.github.io/tags/AOP/"},{"name":"面向切面编程","slug":"面向切面编程","permalink":"https://imokkkk.github.io/tags/面向切面编程/"},{"name":"微服务","slug":"微服务","permalink":"https://imokkkk.github.io/tags/微服务/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://imokkkk.github.io/tags/Spring-Cloud/"},{"name":"Redis","slug":"Redis","permalink":"https://imokkkk.github.io/tags/Redis/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://imokkkk.github.io/tags/垃圾回收/"},{"name":"GC","slug":"GC","permalink":"https://imokkkk.github.io/tags/GC/"},{"name":"HashMap","slug":"HashMap","permalink":"https://imokkkk.github.io/tags/HashMap/"},{"name":"底层","slug":"底层","permalink":"https://imokkkk.github.io/tags/底层/"},{"name":"shell","slug":"shell","permalink":"https://imokkkk.github.io/tags/shell/"},{"name":"Git","slug":"Git","permalink":"https://imokkkk.github.io/tags/Git/"}]}